{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import os.path\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import lovely_tensors as lt # can be removed\n",
    "\n",
    "from l2o.others import w, detach_var, rsetattr, rgetattr, count_parameters, print_grads, \\\n",
    "    load_l2o_opter_ckpt, load_baseline_opter_ckpt, load_ckpt, dict_to_str, get_baseline_ckpt_dir\n",
    "from l2o.training import fit_normal, find_best_lr_normal\n",
    "from l2o.regularization import (\n",
    "    regularize_updates_translation_constraints,\n",
    "    regularize_updates_scale_constraints,\n",
    "    regularize_updates_rescale_constraints,\n",
    "    regularize_updates_constraints,\n",
    "    regularize_translation_conservation_law_breaking,\n",
    "    regularize_rescale_conservation_law_breaking,\n",
    ")\n",
    "from l2o.analysis import (\n",
    "    get_baseline_opter_param_updates,\n",
    "    collect_rescale_sym_deviations,\n",
    "    collect_translation_sym_deviations,\n",
    "    collect_scale_sym_deviations,\n",
    "    calc_sai,\n",
    ")\n",
    "from l2o.data import MNIST, CIFAR10\n",
    "from l2o.optimizer import Optimizer\n",
    "from l2o.optimizee import (\n",
    "    MNISTSigmoid,\n",
    "    MNISTReLU,\n",
    "    MNISTNet,\n",
    "    MNISTNet2Layer,\n",
    "    MNISTNetBig,\n",
    "    MNISTRelu,\n",
    "    MNISTLeakyRelu,\n",
    "    MNISTSimoidBatchNorm,\n",
    "    MNISTReluBatchNorm,\n",
    "    MNISTConv,\n",
    "    MNISTReluBig,\n",
    "    MNISTReluBig2Layer,\n",
    "    MNISTMixtureOfActivations,\n",
    "    MNISTNetBig2Layer,\n",
    ")\n",
    "from l2o.meta_module import *\n",
    "# from meta_test import meta_test, meta_test_baselines\n",
    "\n",
    "lt.monkey_patch() # can be removed\n",
    "sns.set(color_codes=True)\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_fit(\n",
    "    opter,\n",
    "    opter_optim,\n",
    "    data_cls,\n",
    "    optee_cls,\n",
    "    unroll,\n",
    "    n_iters,\n",
    "    optee_updates_lr,\n",
    "    data_config=None,\n",
    "    train_opter=True,\n",
    "    log_unroll_losses=False,\n",
    "    opter_updates_reg_func=None,\n",
    "    opter_updates_reg_func_config=None,\n",
    "    reg_mul=1.0,\n",
    "    optee_config=None,\n",
    "    eval_iter_freq=10,\n",
    "    ckpt_iter_freq=None,\n",
    "    ckpt_prefix=\"\",\n",
    "    ckpt_dir=\"\",\n",
    "):\n",
    "    if train_opter:\n",
    "        opter.train()\n",
    "        opter_optim.zero_grad()\n",
    "    else:\n",
    "        opter.eval()\n",
    "        unroll = 1\n",
    "\n",
    "    train_data = data_cls(\n",
    "        training=True, **data_config if data_config is not None else {}\n",
    "    )\n",
    "    test_data = data_cls(\n",
    "        training=False, **data_config if data_config is not None else {}\n",
    "    )\n",
    "    optee = w(optee_cls(**optee_config if optee_config is not None else {}))\n",
    "    optee_n_params = sum(\n",
    "        [int(np.prod(p.size())) for _, p in optee.all_named_parameters() if p.requires_grad]\n",
    "    )\n",
    "\n",
    "    ### save initial optee's parameters for regularization\n",
    "    optee_init_params = dict()\n",
    "    for name, p in optee.all_named_parameters():\n",
    "        optee_init_params[name] = p.data.detach().clone()\n",
    "\n",
    "    ### initialize hidden and cell states\n",
    "    hidden_states = [\n",
    "        w(Variable(torch.zeros(optee_n_params, opter.hidden_sz))) for _ in range(2)\n",
    "    ]\n",
    "    cell_states = [\n",
    "        w(Variable(torch.zeros(optee_n_params, opter.hidden_sz))) for _ in range(2)\n",
    "    ]\n",
    "\n",
    "    metrics = {m: [] for m in [\"train_loss\", \"train_acc\", \"test_loss\", \"test_acc\"]}\n",
    "    unroll_losses = None\n",
    "    reg_losses = None\n",
    "    updates_for_ckpt = dict()\n",
    "    prev_params = dict()\n",
    "\n",
    "    ### run optee's training loop\n",
    "    for iteration in range(1, n_iters + 1):\n",
    "        ### train optee\n",
    "        optee.train()\n",
    "        train_loss, train_acc = optee(train_data, return_acc=True)  # a single minibatch\n",
    "        train_loss.backward(retain_graph=train_opter)\n",
    "\n",
    "        ### track for training opter\n",
    "        unroll_losses = (\n",
    "            train_loss if unroll_losses is None else unroll_losses + train_loss\n",
    "        )\n",
    "\n",
    "        ### optimizer: gradients -> updates\n",
    "        result_params = dict()\n",
    "        updates_for_reg = dict()\n",
    "        hidden_states2 = [\n",
    "            w(Variable(torch.zeros(optee_n_params, opter.hidden_sz))) for _ in range(2)\n",
    "        ]\n",
    "        cell_states2 = [\n",
    "            w(Variable(torch.zeros(optee_n_params, opter.hidden_sz))) for _ in range(2)\n",
    "        ]\n",
    "        offset = 0\n",
    "        for name, p in optee.all_named_parameters():\n",
    "            if p.requires_grad == False:  # batchnorm stats\n",
    "                result_params[name] = p\n",
    "                continue\n",
    "\n",
    "            # We do this so the gradients are disconnected from the graph but we still get\n",
    "            # gradients from the rest\n",
    "            cur_sz = int(np.prod(p.size()))\n",
    "            gradients = detach_var(p.grad.view(cur_sz, 1))\n",
    "            \n",
    "            ### prepare additional input for optimizer\n",
    "            if iteration == 1:\n",
    "                additional_inp = w(torch.zeros(cur_sz, 1))\n",
    "            else:\n",
    "                additional_inp = w(calc_sai(\n",
    "                    vec_t0=prev_params[name].detach().view(-1),\n",
    "                    vec_t1=p.detach().view(-1),\n",
    "                    time_delta=1,\n",
    "                    normalize=True\n",
    "                ).expand(cur_sz, 1))\n",
    "            \n",
    "            updates, new_hidden, new_cell = opter(\n",
    "                optee_grads=gradients,\n",
    "                hidden=[h[offset : offset + cur_sz] for h in hidden_states],\n",
    "                cell=[c[offset : offset + cur_sz] for c in cell_states],\n",
    "                additional_inp=additional_inp,\n",
    "            )\n",
    "\n",
    "            ### track updates for checkpointing\n",
    "            if ckpt_iter_freq and (iteration % ckpt_iter_freq == 0 or iteration == 1):\n",
    "                updates_for_ckpt[name] = updates.view(*p.size()).detach()\n",
    "\n",
    "            ### track updates for regularization\n",
    "            if train_opter and opter_updates_reg_func is not None:\n",
    "                updates_for_reg[name] = updates.view(*p.size())\n",
    "\n",
    "            ### update hidden and cell states\n",
    "            for i in range(len(new_hidden)):\n",
    "                hidden_states2[i][offset : offset + cur_sz] = new_hidden[i]\n",
    "                cell_states2[i][offset : offset + cur_sz] = new_cell[i]\n",
    "\n",
    "            ### update optee's params\n",
    "            result_params[name] = p + optee_updates_lr * updates.view(*p.size())\n",
    "            result_params[name].retain_grad()\n",
    "            offset += cur_sz\n",
    "\n",
    "        ### add regularization loss for opter\n",
    "        if train_opter and opter_updates_reg_func is not None:\n",
    "            if \"conservation_law_breaking\" in opter_updates_reg_func.__name__:  # TODO: quick hack\n",
    "                reg_loss = torch.abs(\n",
    "                    reg_mul\n",
    "                    * opter_updates_reg_func(\n",
    "                        optee=optee,\n",
    "                        params_t0=optee_init_params,\n",
    "                        **opter_updates_reg_func_config\n",
    "                        if opter_updates_reg_func_config is not None\n",
    "                        else {},\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                reg_loss = torch.abs(\n",
    "                    reg_mul\n",
    "                    * opter_updates_reg_func(\n",
    "                        updates=updates_for_reg,\n",
    "                        optee=optee,\n",
    "                        lr=optee_updates_lr,\n",
    "                        **opter_updates_reg_func_config\n",
    "                        if opter_updates_reg_func_config is not None\n",
    "                        else {},\n",
    "                    )\n",
    "                )\n",
    "            reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "            updates_for_reg = dict()\n",
    "            # add to metrics\n",
    "            if \"train_reg_loss\" not in metrics:\n",
    "                metrics[\"train_reg_loss\"] = []\n",
    "            metrics[\"train_reg_loss\"].append(reg_loss.item())\n",
    "\n",
    "        ### track metrics\n",
    "        metrics[\"train_loss\"].append(train_loss.item())\n",
    "        metrics[\"train_acc\"].append(train_acc.item())\n",
    "\n",
    "        ### eval\n",
    "        if eval_iter_freq is not None and (iteration % eval_iter_freq == 0 or iteration == 1):\n",
    "            optee.eval()\n",
    "            test_loss, test_acc = optee(test_data, return_acc=True)\n",
    "            metrics[\"test_loss\"].append(test_loss.item())\n",
    "            metrics[\"test_acc\"].append(test_acc.item())\n",
    "            optee.train()\n",
    "\n",
    "        ### checkpoint\n",
    "        if ckpt_iter_freq and (iteration % ckpt_iter_freq == 0 or iteration == 1):\n",
    "            ckpt = {\n",
    "                \"optimizee\": optee.state_dict(),\n",
    "                \"optimizee_grads\": {k: v.grad for k, v in optee.all_named_parameters()},\n",
    "                \"optimizee_updates\": updates_for_ckpt,\n",
    "                \"optimizer\": opter.state_dict(),\n",
    "                \"hidden_states\": hidden_states,\n",
    "                \"cell_states\": cell_states,\n",
    "                \"metrics\": metrics,\n",
    "            }\n",
    "            if not ckpt_dir.startswith(os.environ[\"CKPT_PATH\"]):\n",
    "                print(f\"[WARNING] ckpt_dir {ckpt_dir} does not start with CKPT_PATH, prepending CKPT_PATH to it\")\n",
    "                ckpt_dir = os.path.join(os.environ[\"CKPT_PATH\"], ckpt_dir)\n",
    "            torch.save(ckpt, os.path.join(ckpt_dir, f\"{ckpt_prefix}{iteration}.pt\"))\n",
    "            updates_for_ckpt = dict()\n",
    "\n",
    "        ### save current optee params\n",
    "        prev_params = deepcopy({n: p.detach() for n, p in optee.all_named_parameters()})\n",
    "\n",
    "        ### update - continue unrolling or step w/ opter\n",
    "        if iteration % unroll == 0:\n",
    "            ### step w/ the optimizer\n",
    "            if train_opter:\n",
    "                opter_optim.zero_grad()\n",
    "                if log_unroll_losses:\n",
    "                    unroll_losses = torch.log(unroll_losses)\n",
    "                total_loss = (\n",
    "                    unroll_losses + reg_losses\n",
    "                    if reg_losses is not None\n",
    "                    else unroll_losses\n",
    "                )\n",
    "                total_loss.backward()\n",
    "                opter_optim.step()\n",
    "\n",
    "            ### reinitialize - start next unroll\n",
    "            optee = w(optee_cls(**optee_config if optee_config is not None else {}))\n",
    "            optee.load_state_dict(result_params)\n",
    "            optee.zero_grad()\n",
    "            hidden_states = [detach_var(v) for v in hidden_states2]\n",
    "            cell_states = [detach_var(v) for v in cell_states2]\n",
    "            unroll_losses = None\n",
    "            reg_losses = None\n",
    "        else:\n",
    "            ### update the optimizee and optimizer's states\n",
    "            for name, p in optee.all_named_parameters():\n",
    "                if p.requires_grad:  # leave the batchnorm stats\n",
    "                    rsetattr(optee, name, result_params[name])\n",
    "            hidden_states = hidden_states2\n",
    "            cell_states = cell_states2\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_optimizer(\n",
    "    data_cls,\n",
    "    optee_cls,\n",
    "    data_config=None,\n",
    "    optee_config=None,\n",
    "    opter_cls=Optimizer,\n",
    "    opter_config=None,\n",
    "    unroll=20,\n",
    "    n_epochs=20,\n",
    "    n_optim_runs_per_epoch=20,\n",
    "    n_iters=100,\n",
    "    n_tests=100,\n",
    "    opter_lr=0.01,\n",
    "    log_unroll_losses=False,\n",
    "    opter_updates_reg_func=None,\n",
    "    opter_updates_reg_func_config=None,\n",
    "    reg_mul=1.0,\n",
    "    optee_updates_lr=1.0,\n",
    "    eval_iter_freq=10,\n",
    "    ckpt_iter_freq=None,\n",
    "    ckpt_epoch_freq=None,\n",
    "    ckpt_prefix=\"\",\n",
    "    ckpt_dir=\"\",\n",
    "    load_ckpt=None,\n",
    "    start_from_epoch=0,\n",
    "    verbose=1,\n",
    "):\n",
    "    if ckpt_iter_freq is not None:\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "    opter = w(opter_cls(**opter_config if opter_config is not None else {}))\n",
    "    meta_opt = optim.Adam(opter.parameters(), lr=opter_lr)\n",
    "\n",
    "    best_opter = None\n",
    "    best_loss = np.inf\n",
    "    all_metrics = list()\n",
    "    \n",
    "    ### load checkpoint\n",
    "    if load_ckpt is not None:\n",
    "        print(f\"... loading checkpoint from {load_ckpt} ...\")\n",
    "        ckpt = torch.load(load_ckpt)\n",
    "        best_opter = ckpt[\"best_opter\"]\n",
    "        best_loss = ckpt[\"best_loss\"]\n",
    "        all_metrics = ckpt[\"metrics\"]\n",
    "        opter.load_state_dict(ckpt[\"opter\"])\n",
    "        meta_opt = optim.Adam(opter.parameters(), lr=opter_lr)\n",
    "        meta_opt.load_state_dict(ckpt[\"meta_opter\"])\n",
    "        meta_opt.zero_grad()\n",
    "        opter.train()\n",
    "\n",
    "    ### meta-training epochs\n",
    "    for epoch_i in range(start_from_epoch, n_epochs):\n",
    "        start_time = time.time()\n",
    "        if verbose > 0:\n",
    "            print(f\"[{epoch_i + 1}/{n_epochs}]\")\n",
    "        all_metrics.append({k: dict() for k in [\"meta_training\", \"meta_testing\"]})\n",
    "\n",
    "        ### meta-train\n",
    "        for run_i in range(n_optim_runs_per_epoch):\n",
    "            curr_ckpt_iter_freq = None\n",
    "            if epoch_i % ckpt_epoch_freq == 0 and run_i == n_optim_runs_per_epoch - 1:\n",
    "                curr_ckpt_iter_freq = ckpt_iter_freq\n",
    "            optim_run_metrics = do_fit(\n",
    "                opter=opter,\n",
    "                opter_optim=meta_opt,\n",
    "                data_cls=data_cls,\n",
    "                data_config=data_config,\n",
    "                optee_cls=optee_cls,\n",
    "                optee_config=optee_config,\n",
    "                unroll=unroll,\n",
    "                n_iters=n_iters,\n",
    "                optee_updates_lr=optee_updates_lr,\n",
    "                train_opter=True,\n",
    "                log_unroll_losses=log_unroll_losses,\n",
    "                opter_updates_reg_func=opter_updates_reg_func,\n",
    "                opter_updates_reg_func_config=opter_updates_reg_func_config,\n",
    "                reg_mul=reg_mul,\n",
    "                eval_iter_freq=eval_iter_freq,\n",
    "                ckpt_iter_freq=curr_ckpt_iter_freq,\n",
    "                ckpt_prefix=f\"{ckpt_prefix}{epoch_i}e_\",\n",
    "                ckpt_dir=ckpt_dir,\n",
    "            )\n",
    "            log_msg = f\"  [{run_i + 1}/{n_optim_runs_per_epoch}]\"\n",
    "            for k, v in optim_run_metrics.items():\n",
    "                if k not in all_metrics[-1][\"meta_training\"]:\n",
    "                    all_metrics[-1][\"meta_training\"][k] = np.array(v)\n",
    "                else:\n",
    "                    all_metrics[-1][\"meta_training\"][k] += np.array(v)\n",
    "                if \"acc\" in k:\n",
    "                    log_msg += f\"  {k}_last={v[-1]:.3f}\"\n",
    "                else:\n",
    "                    log_msg += f\"  {k}_sum={np.sum(v):.3f}  {k}_last={v[-1]:.3f}\"\n",
    "            if verbose > 1:\n",
    "                print(log_msg)\n",
    "\n",
    "        ### average metrics and log\n",
    "        for k, v in all_metrics[-1][\"meta_training\"].items():\n",
    "            v = v / n_optim_runs_per_epoch\n",
    "            all_metrics[-1][\"meta_training\"][k] = {\"sum\": np.sum(v), \"last\": v[-1]}\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(\n",
    "                f\"[{epoch_i + 1}/{n_epochs}] Meta-training metrics:\"\n",
    "                f\"\\n{json.dumps(all_metrics[-1]['meta_training'], indent=4, sort_keys=False)}\"\n",
    "            )\n",
    "\n",
    "        ### meta-test\n",
    "        if n_tests > 0:\n",
    "            for _ in range(n_tests):\n",
    "                optim_run_metrics = do_fit(\n",
    "                    opter=opter,\n",
    "                    opter_optim=meta_opt,\n",
    "                    data_cls=data_cls,\n",
    "                    data_config=data_config,\n",
    "                    optee_cls=optee_cls,\n",
    "                    optee_config=optee_config,\n",
    "                    unroll=unroll,\n",
    "                    n_iters=n_iters,\n",
    "                    optee_updates_lr=optee_updates_lr,\n",
    "                    train_opter=False,\n",
    "                    eval_iter_freq=eval_iter_freq,\n",
    "                    ckpt_iter_freq=None,\n",
    "                )\n",
    "                for k, v in optim_run_metrics.items():\n",
    "                    if k not in all_metrics[-1][\"meta_testing\"]:\n",
    "                        all_metrics[-1][\"meta_testing\"][k] = np.array(v)\n",
    "                    else:\n",
    "                        all_metrics[-1][\"meta_testing\"][k] += np.array(v)\n",
    "\n",
    "            ### average metrics and log\n",
    "            for k, v in all_metrics[-1][\"meta_testing\"].items():\n",
    "                v = v / n_tests\n",
    "                all_metrics[-1][\"meta_testing\"][k] = {\"sum\": np.sum(v), \"last\": v[-1]}\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(\n",
    "                    f\"[{epoch_i + 1}/{n_epochs}] Meta-testing metrics:\"\n",
    "                    f\"\\n{json.dumps(all_metrics[-1]['meta_testing'], indent=4, sort_keys=False)}\"\n",
    "                )\n",
    "\n",
    "            if all_metrics[-1][\"meta_testing\"][\"train_loss\"][\"sum\"] < best_loss:\n",
    "                if verbose > 0:\n",
    "                    print(\n",
    "                        f\"[{epoch_i + 1}/{n_epochs}] New best loss\"\n",
    "                        f\"\\n\\t previous:\\t {best_loss}\"\n",
    "                        f\"\\n\\t current:\\t {all_metrics[-1]['meta_testing']['train_loss']['sum']} (at last iter: {all_metrics[-1]['meta_testing']['train_loss']['last']})\"\n",
    "                    )\n",
    "                best_loss = all_metrics[-1][\"meta_testing\"][\"train_loss\"][\"sum\"]\n",
    "                best_opter = copy.deepcopy(opter.state_dict())\n",
    "        else:\n",
    "            ### no meta-testing, so just save the best model based on meta-training\n",
    "            if all_metrics[-1][\"meta_training\"][\"train_loss\"][\"sum\"] < best_loss:\n",
    "                if verbose > 0:\n",
    "                    print(\n",
    "                        f\"[{epoch_i + 1}/{n_epochs}] New best loss\"\n",
    "                        f\"\\n\\t previous:\\t {best_loss}\"\n",
    "                        f\"\\n\\t current:\\t {all_metrics[-1]['meta_training']['train_loss']['sum']:.3f} (at last iter: {all_metrics[-1]['meta_training']['train_loss']['last']:.3f})\"\n",
    "                    )\n",
    "                best_loss = all_metrics[-1][\"meta_training\"][\"train_loss\"][\"sum\"]\n",
    "                best_opter = copy.deepcopy(opter.state_dict())\n",
    "\n",
    "        ### save ckpt\n",
    "        if epoch_i % ckpt_epoch_freq == 0:\n",
    "            ckpt = {\n",
    "                \"best_opter\": best_opter if best_opter else None,\n",
    "                \"best_loss\": best_loss,\n",
    "                \"opter\": opter.state_dict(),\n",
    "                \"meta_opter\": meta_opt.state_dict(),\n",
    "                \"metrics\": all_metrics,\n",
    "            }\n",
    "            torch.save(ckpt, os.path.join(ckpt_dir, f\"{epoch_i}.pt\"))\n",
    "\n",
    "        end_time = time.time()\n",
    "        if verbose > 0:\n",
    "            print(f\"[{epoch_i + 1}/{n_epochs}] Epoch took {end_time - start_time:.2f}s\")\n",
    "\n",
    "    return best_loss, all_metrics, best_opter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_test(opter, optees, config, save_ckpts_for_all_test_runs=False, seed=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    opter : l2o.optimizer.Optimizer\n",
    "        The L2O optimizer model to meta-test.\n",
    "    optees : list of tuples\n",
    "        List of tuples of the form (optimizee_cls, optimizee_config) to meta-test on.\n",
    "    config : dict\n",
    "        The config dictionary for the meta-testing run.\n",
    "    save_ckpts_for_all_test_runs : bool, optional\n",
    "        Whether to save checkpoints for all test runs, by default False.\n",
    "    seed : int, optional\n",
    "        The random seed to use for the meta-testing run, by default 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Dictionary of results.\n",
    "    \"\"\"\n",
    "    ### meta-test the L2O optimizer model on various optimizees\n",
    "    results = dict()\n",
    "    for optee_cls, optee_config in optees:\n",
    "        meta_task_name = (\n",
    "            f\"{optee_cls.__name__}_{dict_to_str(optee_config)}\"\n",
    "            + f\"_{config['meta_testing']['data_cls'].__name__}_{dict_to_str(config['meta_testing']['data_config'])}\"\n",
    "        )\n",
    "        print(f\"Meta-testing on {meta_task_name}\")\n",
    "\n",
    "        ### set config\n",
    "        run_config = deepcopy(config)\n",
    "        run_config[\"meta_testing\"][\"optee_cls\"] = optee_cls\n",
    "        run_config[\"meta_testing\"][\"optee_config\"] = optee_config\n",
    "        if save_ckpts_for_all_test_runs is False:\n",
    "            ckpt_prefix = \"\"\n",
    "\n",
    "        ### run the meta-testing run with l2o optimizer\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        metrics = []\n",
    "        for eval_test_i in range(run_config[\"eval_n_tests\"]):\n",
    "            ckpt_prefix = f\"run{eval_test_i}_\" # add prefix to checkpoints\n",
    "            if not save_ckpts_for_all_test_runs and eval_test_i > 0:\n",
    "                run_config[\"meta_testing\"][\"ckpt_iter_freq\"] = None # only save checkpoints for the first run\n",
    "            metrics.append(\n",
    "                do_fit(\n",
    "                    opter=opter,\n",
    "                    **run_config[\"meta_testing\"],\n",
    "                    ckpt_prefix=ckpt_prefix,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        ### aggregate metrics by metric name and save\n",
    "        metrics = {k: np.array([m[k] for m in metrics]) for k in metrics[0].keys()}\n",
    "        metrics_path = os.path.join(\n",
    "            os.environ[\"CKPT_PATH\"], run_config[\"ckpt_base_dir\"], f\"metrics_{meta_task_name}.npy\"\n",
    "        )\n",
    "        np.save(metrics_path, metrics)\n",
    "        print(f\"  Metrics saved to {metrics_path}\")\n",
    "\n",
    "        ### save the config for this run\n",
    "        config_path = os.path.join(\n",
    "            os.environ[\"CKPT_PATH\"], run_config[\"ckpt_base_dir\"], f\"config_{meta_task_name}.json\"\n",
    "        )\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(run_config, f, indent=4, default=str)\n",
    "        \n",
    "        ### save also the whole run information as .pt (config and metrics)\n",
    "        run_info = {\n",
    "            \"config\": run_config,\n",
    "            \"metrics\": metrics,\n",
    "        }\n",
    "        run_info_path = os.path.join(\n",
    "            os.environ[\"CKPT_PATH\"], run_config[\"ckpt_base_dir\"], f\"run_{meta_task_name}.pt\"\n",
    "        )\n",
    "        torch.save(run_info, run_info_path)\n",
    "        \n",
    "        results[meta_task_name] = run_info\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_test_baselines(baseline_opters, optees, config, use_existing_baselines=True, save_ckpts_for_all_test_runs=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    baseline_opters : list of tuples\n",
    "        List of tuples of the form (optimizer_name, optimizer_cls, optimizer_config).\n",
    "    optees : list of tuples\n",
    "        List of tuples of the form (optimizee_cls, optimizee_config).\n",
    "    config : dict\n",
    "        The config dictionary for the training run.\n",
    "    use_existing_baselines : bool, optional\n",
    "        Whether to use existing baselines if they exist, by default True.\n",
    "    save_ckpts_for_all_test_runs : bool, optional\n",
    "        Whether to save checkpoints for all test runs, by default False.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Dictionary of results.\n",
    "    \"\"\"\n",
    "    results = dict()\n",
    "    for optee_cls, optee_config in optees:\n",
    "        ### train optees with baseline optimizers (or load previous)\n",
    "        for opter_name, baseline_opter_cls, opter_config in baseline_opters:\n",
    "            print(f\"Training {optee_cls.__name__}_{dict_to_str(optee_config)} with {opter_name} optimizer\")\n",
    "            torch.manual_seed(0)\n",
    "            np.random.seed(0)\n",
    "\n",
    "            ### set config\n",
    "            run_config = deepcopy(config)\n",
    "            run_config[\"meta_testing\"][\"optee_cls\"] = optee_cls\n",
    "            run_config[\"meta_testing\"][\"optee_config\"] = optee_config\n",
    "\n",
    "            ### prepare checkpointing\n",
    "            baseline_file_nickname = get_baseline_ckpt_dir(\n",
    "                opter_cls=baseline_opter_cls,\n",
    "                opter_config=opter_config,\n",
    "                optee_cls=run_config[\"meta_testing\"][\"optee_cls\"],\n",
    "                optee_config=run_config[\"meta_testing\"][\"optee_config\"],\n",
    "                data_cls=run_config[\"meta_testing\"][\"data_cls\"],\n",
    "                data_config=run_config[\"meta_testing\"][\"data_config\"],\n",
    "            )\n",
    "            baseline_opter_dir = os.path.join(\n",
    "                os.environ[\"CKPT_PATH\"], run_config[\"ckpt_baselines_dir\"], baseline_file_nickname\n",
    "            )\n",
    "            os.makedirs(baseline_opter_dir, exist_ok=True)\n",
    "            metrics_path = os.path.join(baseline_opter_dir, \"metrics.npy\")\n",
    "\n",
    "            ### load previous if exists\n",
    "            if use_existing_baselines and os.path.exists(metrics_path) and os.path.isdir(os.path.join(baseline_opter_dir, \"ckpt\")):\n",
    "                print(\n",
    "                    f\"  Existing metrics and checkpoints for {opter_name} exist, skipping...\"\n",
    "                    f\"\\n  metrics_file: {metrics_path}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            ### not reusing existing, train from scratch\n",
    "            opter_config_for_run = deepcopy(opter_config)\n",
    "\n",
    "            ### find best lr\n",
    "            if \"lr\" in opter_config_for_run and callable(opter_config_for_run[\"lr\"]):\n",
    "                print(f\"  Finding best lr for {opter_name} optimizer\")\n",
    "                best_lr = opter_config_for_run[\"lr\"](\n",
    "                    data_cls=run_config[\"meta_testing\"][\"data_cls\"],\n",
    "                    optee_cls=run_config[\"meta_testing\"][\"optee_cls\"],\n",
    "                    opter_cls=baseline_opter_cls,\n",
    "                    n_tests=3,\n",
    "                    n_iters=run_config[\"meta_testing\"][\"n_iters\"] // 2,\n",
    "                    optee_config=run_config[\"meta_testing\"][\"optee_config\"],\n",
    "                    opter_config=opter_config_for_run,\n",
    "                    consider_metric=\"train_loss\",\n",
    "                )\n",
    "                opter_config_for_run[\"lr\"] = best_lr\n",
    "                print(f\"  Best lr for {opter_name} optimizer: {best_lr}\")\n",
    "\n",
    "            ### dump config\n",
    "            run_config[\"meta_testing\"][\"baseline_opter_config\"] = opter_config_for_run\n",
    "            with open(os.path.join(baseline_opter_dir, \"config.json\"), \"w\") as f:\n",
    "                json.dump(run_config, f, indent=4, default=str)\n",
    "            torch.save(run_config, os.path.join(baseline_opter_dir, \"config.pt\"))\n",
    "\n",
    "            ### train\n",
    "            baseline_ckpt_dir = os.path.join(baseline_opter_dir, \"ckpt\")\n",
    "            baseline_metrics = fit_normal(\n",
    "                data_cls=run_config[\"meta_testing\"][\"data_cls\"],\n",
    "                data_config=run_config[\"meta_testing\"][\"data_config\"],\n",
    "                optee_cls=run_config[\"meta_testing\"][\"optee_cls\"],\n",
    "                optee_config=run_config[\"meta_testing\"][\"optee_config\"],\n",
    "                opter_cls=baseline_opter_cls,\n",
    "                opter_config=opter_config_for_run,\n",
    "                n_iters=run_config[\"meta_testing\"][\"n_iters\"],\n",
    "                n_tests=run_config[\"eval_n_tests\"],\n",
    "                ckpt_iter_freq=run_config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "                ckpt_dir=baseline_ckpt_dir,\n",
    "                save_ckpts_for_all_test_runs=save_ckpts_for_all_test_runs,\n",
    "            )\n",
    "\n",
    "            ### save metrics to disk\n",
    "            np.save(metrics_path, baseline_metrics)\n",
    "            print(f\"  Metrics of {opter_name} saved to {metrics_path}\")\n",
    "\n",
    "            ### save all info to disk as .pt (config and metrics)\n",
    "            run_info = {\n",
    "                \"config\": run_config,\n",
    "                \"metrics\": baseline_metrics,\n",
    "            }\n",
    "            run_info_path = os.path.join(baseline_opter_dir, \"run.pt\")\n",
    "            torch.save(run_info, run_info_path)\n",
    "\n",
    "            results[f\"{opter_name}_{optee_cls.__name__}_{dict_to_str(optee_config)}\"] = run_info\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize L2O Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained L2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load previous checkpoint (and skip meta-training of a new l2O optimizer)\n",
    "opter, config, ckpt = load_ckpt(\n",
    "    dir_path=os.path.join(os.environ[\"CKPT_PATH\"], \"10-05-2023_13-26-17_MNISTReluBatchNorm_Optimizer\")\n",
    ")\n",
    "print(json.dumps(config, indent=4, default=str))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-train a new L2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a new config\n",
    "config = { # global config\n",
    "    \"opter_cls\": Optimizer,\n",
    "    \"opter_config\": {\n",
    "        \"preproc\": True,\n",
    "        \"additional_inp_dim\": 1,\n",
    "        \"manual_init_output_params\": False,\n",
    "    },\n",
    "    \"eval_n_tests\": 5,\n",
    "    \"ckpt_base_dir\": datetime.now().strftime('%d-%m-%Y_%H-%M-%S'),\n",
    "    \"ckpt_baselines_dir\": \"baselines\",\n",
    "}\n",
    "\n",
    "config[\"meta_training\"] = { # training the optimizer\n",
    "    \"opter_cls\": config[\"opter_cls\"],\n",
    "    \"opter_config\": config[\"opter_config\"],\n",
    "    \"data_cls\": MNIST,\n",
    "    \"data_config\": {\n",
    "        \"batch_size\": 128,\n",
    "        # \"only_classes\": [0, 1],\n",
    "    },\n",
    "    \"optee_cls\": MNISTNet,\n",
    "    \"optee_config\": {},\n",
    "    \"n_epochs\": 50,\n",
    "    \"n_optim_runs_per_epoch\": 20,\n",
    "    \"n_iters\": 200,\n",
    "    \"unroll\": 20,\n",
    "    \"n_tests\": 0,\n",
    "    \"optee_updates_lr\": 0.1,\n",
    "    \"opter_lr\": 0.001,\n",
    "    \"log_unroll_losses\": True,\n",
    "    \"opter_updates_reg_func\": None,\n",
    "    \"opter_updates_reg_func_config\": {},\n",
    "    \"reg_mul\": 0,\n",
    "    \"eval_iter_freq\": 10,\n",
    "    \"ckpt_iter_freq\": 5,\n",
    "    \"ckpt_epoch_freq\": 5,\n",
    "    \"ckpt_dir\": None, # will be set later\n",
    "    \"load_ckpt\": None,\n",
    "    \"start_from_epoch\": 0,\n",
    "    \"verbose\": 2,\n",
    "}\n",
    "\n",
    "config[\"meta_testing\"] = { # testing the optimizer\n",
    "    \"data_cls\": config[\"meta_training\"][\"data_cls\"],\n",
    "    \"data_config\": config[\"meta_training\"][\"data_config\"],\n",
    "    \"optee_cls\": config[\"meta_training\"][\"optee_cls\"],\n",
    "    \"optee_config\": config[\"meta_training\"][\"optee_config\"],\n",
    "    \"unroll\": 1,\n",
    "    \"n_iters\": 1000,\n",
    "    \"optee_updates_lr\": config[\"meta_training\"][\"optee_updates_lr\"],\n",
    "    \"train_opter\": False,\n",
    "    \"opter_optim\": None,\n",
    "    \"ckpt_iter_freq\": 5,\n",
    "    \"ckpt_dir\": None, # will be set later\n",
    "}\n",
    "\n",
    "### additional config\n",
    "config[\"ckpt_base_dir\"] = f\"{config['ckpt_base_dir']}_{config['meta_training']['optee_cls'].__name__}_{config['opter_cls'].__name__}\"\n",
    "config[\"meta_training\"][\"ckpt_dir\"] = os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_base_dir\"], \"meta_training\")\n",
    "config[\"meta_testing\"][\"ckpt_dir\"] = os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_base_dir\"], \"meta_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create directories\n",
    "os.makedirs(os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_base_dir\"]), exist_ok=False)\n",
    "os.makedirs(config[\"meta_training\"][\"ckpt_dir\"], exist_ok=True)\n",
    "os.makedirs(config[\"meta_testing\"][\"ckpt_dir\"], exist_ok=True)\n",
    "\n",
    "### dump config\n",
    "with open(os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_base_dir\"], \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4, default=str)\n",
    "\n",
    "print(f\"Base directory: {config['ckpt_base_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### meta-train a new L2O optimizer model\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "best_training_loss, metrics, opter_state_dict = fit_optimizer(\n",
    "    **config[\"meta_training\"],\n",
    ")\n",
    "opter = w(\n",
    "    config[\"opter_cls\"](\n",
    "        **config[\"opter_config\"] if config[\"opter_config\"] is not None else {}\n",
    "    )\n",
    ")\n",
    "opter.load_state_dict(opter_state_dict)\n",
    "print(best_training_loss)\n",
    "\n",
    "### save the final model\n",
    "torch.save({\n",
    "    \"state_dict\": opter_state_dict,\n",
    "    \"config\": config,\n",
    "    \"loss\": best_training_loss,\n",
    "    \"metrics\": metrics,\n",
    "}, os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_base_dir\"], f\"l2o_optimizer.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optees_to_test = [\n",
    "    (MNISTRelu, {}),\n",
    "    (MNISTConv, {}),\n",
    "    (MNISTSigmoid, {\"layer_sizes\": [100, 100]}),\n",
    "    (MNISTReLU, {\"layer_sizes\": [100, 100]}),\n",
    "    (MNISTNet2Layer, {}),\n",
    "    (MNISTNetBig, {}),\n",
    "    (MNISTReluBatchNorm, {\"affine\": True, \"track_running_stats\": True}),\n",
    "    (MNISTLeakyRelu, {}),\n",
    "    (MNISTNet, {}),\n",
    "]\n",
    "baselines_to_test_against = [\n",
    "    (\"Adam\", optim.Adam, {\"lr\": find_best_lr_normal}),\n",
    "    (\"SGD\", optim.SGD, {\"lr\": find_best_lr_normal, \"momentum\": 0.9}),\n",
    "]\n",
    "use_existing_baselines = True # loads existing if exists, otherwise trains from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-test L2O optimizer and baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = meta_test(\n",
    "    opter=opter,\n",
    "    optees=optees_to_test,\n",
    "    config=config,\n",
    "    save_ckpts_for_all_test_runs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_baselines = meta_test_baselines(\n",
    "    baseline_opters=baselines_to_test_against,\n",
    "    optees=optees_to_test,\n",
    "    config=config,\n",
    "    use_existing_baselines=use_existing_baselines,\n",
    "    save_ckpts_for_all_test_runs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load all l2o metrics from disk\n",
    "l2o_metrics = {}\n",
    "\n",
    "for metrics_file in [f_name for f_name in os.listdir(os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_base_dir\"])) if f_name.startswith(\"metrics_\")]:\n",
    "    print(f\"Loading {metrics_file}\")\n",
    "    metrics_name = metrics_file[8:-4] # remove the \"metrics_\" prefix and \".npy\" suffix\n",
    "    l2o_metrics[metrics_name] = np.load(os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_base_dir\"], metrics_file), allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load baseline metrics from disk\n",
    "baseline_metrics = dict()\n",
    "\n",
    "for optee_cls, optee_config in optees_to_test:\n",
    "    run_nickname = f\"{optee_cls.__name__}_{dict_to_str(optee_config)}_{config['meta_testing']['data_cls'].__name__}_{dict_to_str(config['meta_testing']['data_config'])}\"\n",
    "    baseline_metrics[run_nickname] = dict()\n",
    "    \n",
    "    ### load metrics for all considered baselines\n",
    "    for (opter_name, baseline_opter_cls, baseline_opter_config) in baselines_to_test_against:\n",
    "        baseline_opter_config_copy = deepcopy(baseline_opter_config)\n",
    "        \n",
    "        if \"lr\" in baseline_opter_config and callable(baseline_opter_config[\"lr\"]):\n",
    "            baseline_opter_config_copy[\"lr\"] = baseline_opter_config_copy[\"lr\"].__name__ # replace function with its name\n",
    "\n",
    "        baseline_dir_name = f\"{opter_name}_{dict_to_str(baseline_opter_config_copy)}\" \\\n",
    "            + f\"_{optee_cls.__name__}_{dict_to_str(optee_config)}\" \\\n",
    "            + f\"_{config['meta_testing']['data_cls'].__name__}_{dict_to_str(config['meta_testing']['data_config'])}\"\n",
    "        metrics_path = os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_baselines_dir\"], baseline_dir_name, \"metrics.npy\")\n",
    "        \n",
    "        ### load\n",
    "        print(f\"Loading {metrics_path}\")\n",
    "        baseline_metrics[run_nickname][opter_name] = np.load(metrics_path, allow_pickle=True).item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot meta-testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_max_iters = 500\n",
    "log_losses = True\n",
    "fig_save_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_base_dir\"])\n",
    "# fig_save_dir = None\n",
    "\n",
    "for optee_nickname, metrics in l2o_metrics.items():\n",
    "    if optee_nickname not in baseline_metrics:\n",
    "        continue\n",
    "    curr_baseline_metrics = baseline_metrics[optee_nickname]\n",
    "\n",
    "    ### plot comparison\n",
    "    fig = plt.figure(figsize=(26, 16))\n",
    "    fig.suptitle(f\"Meta-testing on {optee_nickname}\", fontsize=16, fontweight=\"bold\", y=0.92)\n",
    "    \n",
    "    for m_i, metric in enumerate([\"train_loss\", \"test_loss\", \"train_acc\", \"test_acc\"]):\n",
    "        ax = fig.add_subplot(2, 2, m_i + 1)\n",
    "        \n",
    "        ### baseline optimizers\n",
    "        for opter_name, opter_metrics in curr_baseline_metrics.items():\n",
    "            if \"test\" in metric:\n",
    "                x = np.arange(config[\"meta_training\"][\"eval_iter_freq\"], show_max_iters + 1, config[\"meta_training\"][\"eval_iter_freq\"])\n",
    "                y = np.mean(opter_metrics[metric][:,:show_max_iters // 10], axis=0)\n",
    "                y_min = np.min(opter_metrics[metric][:,:show_max_iters // 10], axis=0)\n",
    "                y_max = np.max(opter_metrics[metric][:,:show_max_iters // 10], axis=0)\n",
    "            else:\n",
    "                x = range(opter_metrics[metric][:,:show_max_iters].shape[1])\n",
    "                y = np.mean(opter_metrics[metric][:,:show_max_iters], axis=0)\n",
    "                y_min = np.min(opter_metrics[metric][:,:show_max_iters], axis=0)\n",
    "                y_max = np.max(opter_metrics[metric][:,:show_max_iters], axis=0)\n",
    "            sns.lineplot(\n",
    "                x=x,\n",
    "                y=y,\n",
    "                label=opter_name,\n",
    "                linestyle=\"--\",\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.fill_between(\n",
    "                x=x,\n",
    "                y1=y_min,\n",
    "                y2=y_max,\n",
    "                alpha=0.1,\n",
    "            )\n",
    "\n",
    "            if log_losses:\n",
    "                # set y to log scale\n",
    "                if \"loss\" in metric:\n",
    "                    ax.set_yscale(\"log\")\n",
    "        \n",
    "        ### L2O optimizer\n",
    "        if \"test\" in metric:\n",
    "            x = np.arange(config[\"meta_training\"][\"eval_iter_freq\"], show_max_iters + 1, config[\"meta_training\"][\"eval_iter_freq\"])\n",
    "            y = np.mean(metrics[metric][:,:show_max_iters // config[\"meta_training\"][\"eval_iter_freq\"]], axis=0)\n",
    "            y_min = np.min(metrics[metric][:,:show_max_iters // config[\"meta_training\"][\"eval_iter_freq\"]], axis=0)\n",
    "            y_max = np.max(metrics[metric][:,:show_max_iters // config[\"meta_training\"][\"eval_iter_freq\"]], axis=0)\n",
    "        else:\n",
    "            x = range(metrics[metric][:,:show_max_iters].shape[1])\n",
    "            y = np.mean(metrics[metric][:,:show_max_iters], axis=0)\n",
    "            y_min = np.min(metrics[metric][:,:show_max_iters], axis=0)\n",
    "            y_max = np.max(metrics[metric][:,:show_max_iters], axis=0)\n",
    "        sns.lineplot(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            label=\"L2O\",\n",
    "            color=\"orange\",\n",
    "            linewidth=2,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            x=x,\n",
    "            y1=y_min,\n",
    "            y2=y_max,\n",
    "            alpha=0.2,\n",
    "            color=\"orange\",\n",
    "        )\n",
    "        \n",
    "        ### plot settings\n",
    "        ax.set_title(metric, fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_ylabel(metric)\n",
    "        if \"acc\" in metric:\n",
    "            ax.set_ylim(0.6, 1.0)\n",
    "        ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ### save the figure\n",
    "    if fig_save_dir is not None:\n",
    "        prefix = \"log_losses\" if log_losses else \"losses\"\n",
    "        fig.savefig(os.path.join(fig_save_dir, f\"{prefix}_{optee_nickname}_{show_max_iters}.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5692ede66a2eeda96ca4e496ad881a063b66ee8e9ec6003b28974c60439bc6fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
