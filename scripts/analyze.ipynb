{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import lovely_tensors as lt # can be removed\n",
    "\n",
    "from l2o.others import w, detach_var, rsetattr, rgetattr, count_parameters, print_grads, \\\n",
    "    load_l2o_opter_ckpt, load_baseline_opter_ckpt, load_ckpt, get_baseline_ckpt_dir, dict_to_str\n",
    "from l2o.visualization import get_model_dot\n",
    "from l2o.training import do_fit, fit_normal, fit_optimizer, find_best_lr_normal\n",
    "from l2o.regularization import (\n",
    "    regularize_updates_translation_constraints,\n",
    "    regularize_updates_scale_constraints,\n",
    "    regularize_updates_rescale_constraints,\n",
    "    regularize_updates_constraints,\n",
    "    regularize_translation_conservation_law_breaking,\n",
    "    regularize_rescale_conservation_law_breaking,\n",
    ")\n",
    "from l2o.analysis import (\n",
    "    get_rescale_sym_constraint_deviation,\n",
    "    get_translation_sym_constraint_deviations,\n",
    "    get_scale_sym_constraint_deviation,\n",
    "    get_baseline_opter_param_updates,\n",
    "    collect_rescale_sym_deviations,\n",
    "    collect_translation_sym_deviations,\n",
    "    collect_scale_sym_deviations,\n",
    "    collect_conservation_law_deviations,\n",
    "    calc_sai,\n",
    ")\n",
    "from l2o.tail_index_utils import (\n",
    "    alpha_estimator,\n",
    ")\n",
    "from l2o.data import MNIST, CIFAR10\n",
    "from l2o.optimizer import Optimizer\n",
    "from l2o.optimizee import (\n",
    "    MNISTSigmoid,\n",
    "    MNISTReLU,\n",
    "    MNISTNet,\n",
    "    MNISTNet2Layer,\n",
    "    MNISTNetBig,\n",
    "    MNISTRelu,\n",
    "    MNISTLeakyRelu,\n",
    "    MNISTSimoidBatchNorm,\n",
    "    MNISTReluBatchNorm,\n",
    "    MNISTConv,\n",
    "    MNISTReluBig,\n",
    "    MNISTReluBig2Layer,\n",
    "    MNISTMixtureOfActivations,\n",
    "    MNISTNetBig2Layer,\n",
    ")\n",
    "from l2o.meta_module import *\n",
    "from meta_test import meta_test, meta_test_baselines\n",
    "\n",
    "lt.monkey_patch() # can be removed\n",
    "sns.set(color_codes=True)\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### publication figure settings:\n",
    "plt.rc(\"font\", family=\"serif\")\n",
    "\n",
    "plt.rc(\"legend\", fontsize=12)\n",
    "plt.rc(\"xtick\", labelsize=12)\n",
    "plt.rc(\"ytick\", labelsize=12)\n",
    "plt.rc(\"axes\", labelsize=13)\n",
    "plt.rc(\"axes\", titlesize=13)\n",
    "plt.rc(\"axes\", linewidth=0.5)\n",
    "plt.rc(\"axes\", labelpad=10)\n",
    "\n",
    "plt.rc(\"lines\", linewidth=1.)\n",
    "\n",
    "plt.rc(\"figure\", dpi=300)\n",
    "plt.rc(\"figure\", figsize=(6, 4))\n",
    "\n",
    "plt.rc(\"savefig\", dpi=300)\n",
    "plt.rc(\"savefig\", format=\"pdf\")\n",
    "plt.rc(\"savefig\", bbox=\"tight\")\n",
    "plt.rc(\"savefig\", pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "l2os = {\n",
    "    ### MNISTReluBatchNorm ###\n",
    "    # r\"L2O, $\\beta$=0\": \"26-04-2023_01-22-31_MNISTReluBatchNorm_Optimizer\", # DEPRECATED\n",
    "    # r\"L2O, $\\beta$=0\": \"12-05-2023_00-38-54_MNISTReluBatchNorm_Optimizer\", # lr: 1e-3\n",
    "    # r\"L2O, $\\beta$=0.01\": \"03-05-2023_22-42-56_MNISTReluBatchNorm_Optimizer\", # scale\n",
    "    # r\"L2O, $\\beta$=0.1\": \"02-05-2023_16-51-19_MNISTReluBatchNorm_Optimizer\", # scale\n",
    "    # r\"L2O, $\\beta$=1.\": \"03-05-2023_22-42-36_MNISTReluBatchNorm_Optimizer\", # scale\n",
    "    \n",
    "    # \"L2O - unroll 10\": \"10-05-2023_13-26-17_MNISTReluBatchNorm_Optimizer\", # already collected\n",
    "    # \"L2O - unroll 20\": \"12-05-2023_00-38-54_MNISTReluBatchNorm_Optimizer\", # lr: 1e-3, already collected\n",
    "    # \"L2O - unroll 30\": \"11-05-2023_01-27-43_MNISTReluBatchNorm_Optimizer\", # already collected\n",
    "\n",
    "    # \"L2O - randomized restarts\": \"20-06-2023_00-06-50_MNISTReluBatchNorm_Optimizer\",\n",
    "    # \"L2O - baseline\": \"12-05-2023_00-38-54_MNISTReluBatchNorm_Optimizer\",\n",
    "\n",
    "    # \"L2O - w/ SAI input\": \"21-06-2023_12-34-25_MNISTReluBatchNorm_Optimizer\",\n",
    "    # \"L2O - w/ SAI input & random restarts\": \"22-06-2023_11-52-12_MNISTReluBatchNorm_Optimizer\",\n",
    "    ##########################\n",
    "\n",
    "\n",
    "    ### MNISTLeakyRelu ###\n",
    "    # r\"L2O, $\\beta$=0\": \"19-02-2023_18-20-21_MNISTLeakyRelu_Optimizer\", # DEPRECATED\n",
    "    # r\"L2O, $\\beta$=0\": \"12-05-2023_00-35-50_MNISTLeakyRelu_Optimizer\", # lr: 1e-3\n",
    "    # r\"L2O, $\\beta$=0.01\": \"30-04-2023_13-08-09_MNISTLeakyRelu_Optimizer\", # rescale\n",
    "    # r\"L2O, $\\beta$=0.1\": \"06-05-2023_01-54-04_MNISTLeakyRelu_Optimizer\", # rescale\n",
    "    # r\"L2O, $\\beta$=0.5\": \"05-05-2023_01-42-29_MNISTLeakyRelu_Optimizer\", # rescale\n",
    "    \n",
    "    # \"L2O - unroll 10\": \"09-05-2023_20-50-27_MNISTLeakyRelu_Optimizer\", # already collected\n",
    "    # \"L2O - unroll 20\": \"12-05-2023_00-35-50_MNISTLeakyRelu_Optimizer\", # lr: 1e-3, already collected\n",
    "    # \"L2O - unroll 30\": \"10-05-2023_11-11-39_MNISTLeakyRelu_Optimizer\", # already collected\n",
    "    \n",
    "    # \"L2O - randomized restarts\": \"19-06-2023_20-42-55_MNISTLeakyRelu_Optimizer\",\n",
    "    # \"L2O - baseline\": \"12-05-2023_00-35-50_MNISTLeakyRelu_Optimizer\",\n",
    "\n",
    "    # \"L2O - w/ SAI input\": \"21-06-2023_12-26-53_MNISTLeakyRelu_Optimizer\",\n",
    "    # \"L2O - w/ log10(SAI) input\": \"21-06-2023_12-28-08_MNISTLeakyRelu_Optimizer\",\n",
    "    # \"L2O - w/ SAI input & random restarts\": \"22-06-2023_11-51-28_MNISTLeakyRelu_Optimizer\",\n",
    "    # \"L2O - w/ time input & random restarts\": \"23-07-2023_14-07-30_MNISTLeakyRelu_Optimizer\",\n",
    "\n",
    "    # \"L2O, shared state\": \"23-07-2023_23-32-08_MNISTLeakyRelu_Optimizer\", # shared state (256)\n",
    "    ######################\n",
    "\n",
    "\n",
    "    ### MNISTNet ###\n",
    "    # r\"L2O, $\\beta$=0\": \"05-03-2023_01-33-57_MNISTNet_Optimizer\", # DEPRECATED\n",
    "    # r\"L2O, $\\beta$=0\": \"07-05-2023_20-52-18_MNISTNet_Optimizer\", # lr: 1e-3\n",
    "    # r\"L2O, $\\beta$=0.01\": \"29-04-2023_01-35-00_MNISTNet_Optimizer\", # translation\n",
    "    # r\"L2O, $\\beta$=0.1\": \"28-04-2023_13-45-29_MNISTNet_Optimizer\", # translation\n",
    "    # r\"L2O, $\\beta$=0.5\": \"30-04-2023_12-41-11_MNISTNet_Optimizer\", # translation\n",
    "    \n",
    "    # \"L2O - unroll 10\": \"09-05-2023_20-41-10_MNISTNet_Optimizer\", # already collected\n",
    "    # \"L2O - unroll 20\": \"07-05-2023_20-52-18_MNISTNet_Optimizer\", # lr: 1e-3, already collected\n",
    "    # \"L2O - unroll 30\": \"09-05-2023_20-42-08_MNISTNet_Optimizer\", # already collected\n",
    "    \n",
    "    # \"L2O - randomized restarts\": \"18-06-2023_13-29-45_MNISTNet_Optimizer\",\n",
    "    # \"L2O - baseline\": \"05-03-2023_01-33-57_MNISTNet_Optimizer\",\n",
    "\n",
    "    # \"L2O - w/ SAI input\": \"20-06-2023_21-57-17_MNISTNet_Optimizer\",\n",
    "    # \"L2O - w/ log10(SAI) input\": \"20-06-2023_22-00-58_MNISTNet_Optimizer\",\n",
    "    # \"L2O - w/ SAI input (shared)\": \"21-06-2023_16-34-47_MNISTNet_Optimizer\",\n",
    "    # \"L2O - w/ SAI input (init ones)\": \"21-06-2023_18-44-45_MNISTNet_Optimizer\",\n",
    "    # \"L2O - w/ SAI input & random restarts\": \"22-06-2023_11-50-14_MNISTNet_Optimizer\",\n",
    "    # \"L2O - w/ time input & random restarts\": \"23-07-2023_23-37-16_MNISTNet_Optimizer\",\n",
    "    # r\"L2O - hidden_sz=30 & w/ sym. reg. enc & $\\beta$=0.1\": \"11-08-2023_21-13-36_MNISTNet_Optimizer\", # translation\n",
    "\n",
    "    # \"L2O, shared state\": \"23-07-2023_14-37-19_MNISTNet_Optimizer\", # shared state (152)\n",
    "    #################\n",
    "\n",
    "\n",
    "    ### Meta-training for generalization ###\n",
    "    # \"L2O - multi-task\": \"23-06-2023_21-49-22_MNISTMixtureOfActivationsFeatureDim_Optimizer\",\n",
    "    # \"L2O - multi-task\": \"06-06-2023_15-48-07_MNISTMixtureOfActivations_Optimizer\",\n",
    "    # \"L2O - fine-tuning\": \"06-06-2023_16-16-33_MNISTRelu_Optimizer\",\n",
    "    # \"L2O - baseline\": \"07-05-2023_20-52-18_MNISTNet_Optimizer\",\n",
    "    ########################################\n",
    "}\n",
    "\n",
    "baselines = {\n",
    "    ### MNISTReluBatchNorm\n",
    "    # \"Adam\": \"Adam_{lr=find_best_lr_normal}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=128}\",\n",
    "    # \"SGD\": \"SGD_{lr=find_best_lr_normal_momentum=0.9}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=128}\",\n",
    "\n",
    "    ### MNISTLeakyRelu\n",
    "    # \"Adam\": \"Adam_{lr=find_best_lr_normal}_MNISTLeakyRelu_{}_MNIST_{batch_size=128}\",\n",
    "    # \"SGD\": \"SGD_{lr=find_best_lr_normal_momentum=0.9}_MNISTLeakyRelu_{}_MNIST_{batch_size=128}\",\n",
    "\n",
    "    ### MNISTNet\n",
    "    # \"Adam\": \"Adam_{lr=find_best_lr_normal}_MNISTNet_{}_MNIST_{batch_size=128}\",\n",
    "    # \"SGD\": \"SGD_{lr=find_best_lr_normal_momentum=0.9}_MNISTNet_{}_MNIST_{batch_size=128}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load l2os from disk\n",
    "for l2o_name, l2o_dir in l2os.items():\n",
    "    ### load final l2o checkpoint\n",
    "    ckpt = torch.load(os.path.join(os.environ[\"CKPT_PATH\"], l2o_dir, \"l2o_optimizer.pt\"), map_location=\"cpu\")\n",
    "\n",
    "    ### load all metrics\n",
    "    l2o_metrics = {}\n",
    "    for metrics_file in [f_name for f_name in os.listdir(os.path.join(os.environ[\"CKPT_PATH\"], ckpt[\"config\"][\"ckpt_base_dir\"])) if f_name.startswith(\"metrics_\")]:\n",
    "        metrics_name = metrics_file[8:-4] # remove the \"metrics_\" prefix and \".npy\" suffix\n",
    "        l2o_metrics[metrics_name] = np.load(os.path.join(os.environ[\"CKPT_PATH\"], ckpt[\"config\"][\"ckpt_base_dir\"], metrics_file), allow_pickle=True).item()\n",
    "    l2os[l2o_name] = {\n",
    "        \"ckpt\": ckpt,\n",
    "        \"config\": ckpt[\"config\"],\n",
    "        \"metrics\": l2o_metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load baselines from disk\n",
    "def load_baselines(baselines_dict):\n",
    "    baselines_root_dir = os.path.join(os.environ[\"CKPT_PATH\"], \"baselines\")\n",
    "    for baseline_name, baseline_dir in baselines_dict.items():\n",
    "        ### load config\n",
    "        config = torch.load(os.path.join(baselines_root_dir, baseline_dir, \"config.pt\"), map_location=\"cpu\")\n",
    "        if \"baseline_opter_cls\" not in config:\n",
    "            if \"sgd\" in baseline_name.lower():\n",
    "                config[\"meta_testing\"][\"baseline_opter_cls\"] = optim.SGD\n",
    "            elif \"adam\" in baseline_name.lower():\n",
    "                config[\"meta_testing\"][\"baseline_opter_cls\"] = optim.Adam\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        ### load metrics\n",
    "        metrics = np.load(os.path.join(baselines_root_dir, baseline_dir, \"metrics.npy\"), allow_pickle=True).item()\n",
    "        baselines_dict[baseline_name] = {\n",
    "            \"baseline_dir\": os.path.join(baselines_root_dir, baseline_dir),\n",
    "            \"config\": config,\n",
    "            # \"baseline_config\": baseline_config,\n",
    "            \"metrics\": metrics,\n",
    "        }\n",
    "    return baselines_dict\n",
    "\n",
    "baselines = load_baselines(baselines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(\n",
    "    plot_baselines,\n",
    "    plot_l2os,\n",
    "    run_nickname,\n",
    "    show_max_iters,\n",
    "    metric,\n",
    "    log_loss=False,\n",
    "    save_fig_to_path=None,\n",
    "    with_err_bars=False,\n",
    "    conv_window=None,\n",
    "):\n",
    "    ### plot comparison\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ### baseline optimizers\n",
    "    for baseline_name, baseline_dict in plot_baselines.items():\n",
    "        opter_metrics = baseline_dict[\"metrics\"]\n",
    "        config = baseline_dict[\"config\"]\n",
    "        if \"test\" in metric:\n",
    "            x = np.arange(config[\"meta_training\"][\"eval_iter_freq\"], show_max_iters + 1, config[\"meta_training\"][\"eval_iter_freq\"])\n",
    "            y = np.mean(opter_metrics[metric][:,:show_max_iters // 10], axis=0)\n",
    "        else:\n",
    "            x = range(opter_metrics[metric][:,:show_max_iters].shape[1])\n",
    "            y = np.mean(opter_metrics[metric][:,:show_max_iters], axis=0)\n",
    "        if conv_window and conv_window > 1:\n",
    "            y_removed_start = y[:conv_window - 1]\n",
    "            y = np.convolve(y, np.ones(conv_window), \"valid\") / conv_window\n",
    "            y = np.concatenate([y_removed_start, y])\n",
    "        sns.lineplot(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            label=baseline_name,\n",
    "            linestyle=\"--\",\n",
    "            ax=ax,\n",
    "        )\n",
    "        \n",
    "        if with_err_bars:\n",
    "            if \"test\" in metric:\n",
    "                # x = np.arange(config[\"meta_training\"][\"eval_iter_freq\"], show_max_iters + 1, config[\"meta_training\"][\"eval_iter_freq\"])\n",
    "                y_std = np.std(opter_metrics[metric][:,:show_max_iters // 10], axis=0)\n",
    "            else:\n",
    "                # x = range(opter_metrics[metric][:,:show_max_iters].shape[1])\n",
    "                y_std = np.std(opter_metrics[metric][:,:show_max_iters], axis=0)\n",
    "            if conv_window and conv_window > 1:\n",
    "                y_removed_start = y_std[:conv_window - 1]\n",
    "                y_std = np.convolve(y_std, np.ones(conv_window), \"valid\") / conv_window\n",
    "                y_std = np.concatenate([y_removed_start, y_std])\n",
    "            ax.fill_between(\n",
    "                x,\n",
    "                y - y_std,\n",
    "                y + y_std,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "            \n",
    "\n",
    "    ### L2O optimizers\n",
    "    for l2o_name, l2o_dict in plot_l2os.items():\n",
    "        metrics = l2o_dict[\"metrics\"][run_nickname]\n",
    "        config = l2o_dict[\"config\"]\n",
    "\n",
    "        if \"test\" in metric:\n",
    "            x = np.arange(config[\"meta_training\"][\"eval_iter_freq\"], show_max_iters + 1, config[\"meta_training\"][\"eval_iter_freq\"])\n",
    "            y = np.mean(metrics[metric][:,:show_max_iters // config[\"meta_training\"][\"eval_iter_freq\"]], axis=0)\n",
    "        else:\n",
    "            x = range(metrics[metric][:,:show_max_iters].shape[1])\n",
    "            y = np.mean(metrics[metric][:,:show_max_iters], axis=0)\n",
    "        if conv_window and conv_window > 1:\n",
    "            y_removed_start = y[:conv_window - 1]\n",
    "            y = np.convolve(y, np.ones(conv_window), \"valid\") / conv_window\n",
    "            y = np.concatenate([y_removed_start, y])\n",
    "        sns.lineplot(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            label=fr\"{l2o_name}\",\n",
    "            # label=fr\"{l2o_name}, $\\beta$={config['meta_training']['reg_mul']}\",\n",
    "            # linewidth=1.,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        if with_err_bars:\n",
    "            if \"test\" in metric:\n",
    "                y_std = np.std(metrics[metric][:,:show_max_iters // config[\"meta_training\"][\"eval_iter_freq\"]], axis=0)\n",
    "            else:\n",
    "                y_std = np.std(metrics[metric][:,:show_max_iters], axis=0)\n",
    "            if conv_window and conv_window > 1:\n",
    "                y_removed_start = y_std[:conv_window - 1]\n",
    "                y_std = np.convolve(y_std, np.ones(conv_window), \"valid\") / conv_window\n",
    "                y_std = np.concatenate([y_removed_start, y_std])\n",
    "\n",
    "            ax.fill_between(\n",
    "                x,\n",
    "                y - y_std,\n",
    "                y + y_std,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "    ### plot settings\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    if metric == \"train_loss\":\n",
    "        metric_as_label = \"Train Loss\"\n",
    "    elif metric == \"test_loss\":\n",
    "        metric_as_label = \"Test Loss\"\n",
    "    elif metric == \"train_acc\":\n",
    "        metric_as_label = \"Train Accuracy\"\n",
    "    elif metric == \"test_acc\":\n",
    "        metric_as_label = \"Test Accuracy\"\n",
    "    else:\n",
    "        metric_as_label = metric\n",
    "    ax.set_ylabel(metric_as_label)\n",
    "\n",
    "    # set y to log scale\n",
    "    if log_loss and \"loss\" in metric:\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "    if \"acc\" in metric:\n",
    "        ax.set_ylim(0.6, 1.0)\n",
    "    elif log_loss is not True:\n",
    "        ax.set_ylim(0.0, None)\n",
    "\n",
    "    # ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.32), ncol=2)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=3)\n",
    "    legend = ax.get_legend()\n",
    "    for legend_handle in legend.legendHandles:\n",
    "        legend_handle.set_linewidth(3.0)\n",
    "\n",
    "    # x-ticks\n",
    "    x_ticks = ax.get_xticks()\n",
    "    x_ticks = np.linspace(0, show_max_iters, 3)\n",
    "    ax.set_xticks(x_ticks)\n",
    "\n",
    "    ### y-ticks\n",
    "    if log_loss is not True:\n",
    "        y_max = ax.get_ylim()[1]\n",
    "        y_max = np.ceil(y_max / 0.5) * 0.5 # round\n",
    "        y_ticks = np.linspace(0, y_max, 3)\n",
    "        ax.set_yticks(y_ticks)\n",
    "    else:\n",
    "        # y_ticks = ax.get_yticks()\n",
    "        # y_ticks = np.linspace(y_ticks[1], y_ticks[-2], 2)\n",
    "        # ax.set_yticks(y_ticks)\n",
    "        ax.set_yticks([1e0, 1e-1])\n",
    "        # ax.set_ylim(0.2e-1, None)\n",
    "        # ax.set_ylim(1.6e-2, 1e1)\n",
    "    \n",
    "    ### add zoom region inset axes (zoom on the first 100 iterations)\n",
    "    # axins = ax.inset_axes([0.55, 0.56, 0.35, 0.44])\n",
    "    # for l_i, line in enumerate(ax.lines):\n",
    "    #     xy_data = line.get_xydata()\n",
    "    #     axins.plot(xy_data[:,0], xy_data[:,1], linewidth=1., color=line.get_color(), linestyle=line.get_linestyle())\n",
    "\n",
    "    # axins.set_xlim(0, 80)\n",
    "    # axins.set_ylim(0.35, 2.5)\n",
    "    # # axins.set_yscale(\"log\")\n",
    "    # axins.set_xticks([0, 40, 80])\n",
    "    # # axins.set_yticks([1.])\n",
    "    # axins.set_yticklabels([1.0], fontsize=7)\n",
    "    # axins.set_xticklabels([0, 40, 80], fontsize=7.5, position=(0., .08))\n",
    "    # ax.indicate_inset_zoom(axins)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ### save the figure\n",
    "    if save_fig_to_path is not None:\n",
    "        fig.savefig(save_fig_to_path, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "show_max_iters = 500\n",
    "log_loss = True\n",
    "metric = \"train_loss\" # [\"train_loss\", \"test_loss\", \"train_acc\", \"test_acc\"]\n",
    "conv_window = 5\n",
    "with_err_bars = True\n",
    "\n",
    "### run specification\n",
    "optee_cls = MNISTLeakyRelu\n",
    "optee_config = {}\n",
    "# optee_config = {\"affine\": True, \"track_running_stats\": True}\n",
    "# optee_config = {\"layer_sizes\": [100,100]}\n",
    "data_cls = MNIST\n",
    "data_config = {\"batch_size\": 128}\n",
    "optee_nickname = f\"{optee_cls.__name__}_{dict_to_str(optee_config)}\"\n",
    "run_nickname = f\"{optee_nickname}_{data_cls.__name__}_{dict_to_str(data_config)}\"\n",
    "\n",
    "### load corresponding baselines\n",
    "plot_baselines = {\n",
    "    \"Adam\": \"Adam_{lr=find_best_lr_normal}_\" + run_nickname,\n",
    "    \"SGD\": \"SGD_{lr=find_best_lr_normal_momentum=0.9}_\" + run_nickname,\n",
    "}\n",
    "plot_baselines = load_baselines(plot_baselines)\n",
    "\n",
    "### where to save the figure\n",
    "fig_dir = \"../results/publication/meta_training_for_generalization/MNISTNet_meta_training\"\n",
    "fig_name = f\"{metric}_comparison_{optee_nickname}_{show_max_iters}.pdf\"\n",
    "if log_loss is True:\n",
    "    fig_name = f\"log_{fig_name}\"\n",
    "save_fig_to_path = os.path.join(fig_dir, fig_name)\n",
    "save_fig_to_path = None # don't save\n",
    "\n",
    "print(f\"Final destination: {save_fig_to_path if save_fig_to_path is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(\n",
    "    plot_l2os=l2os,\n",
    "    plot_baselines=plot_baselines,\n",
    "    run_nickname=run_nickname,\n",
    "    show_max_iters=show_max_iters,\n",
    "    metric=metric,\n",
    "    log_loss=log_loss,\n",
    "    save_fig_to_path=save_fig_to_path,\n",
    "    with_err_bars=with_err_bars,\n",
    "    conv_window=conv_window,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance comparison between L2Os\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### settings\n",
    "ckpt_root_dir = \"./ckpt\"\n",
    "l2o_opter_ckpt_dirs_all = {\n",
    "    \"MNISTNet\": [\n",
    "        \"05-03-2023_01-33-57_MNISTNet_Optimizer\", # reg_mul=0\n",
    "        \"29-04-2023_01-35-00_MNISTNet_Optimizer\", # reg_mul=0.01\n",
    "        \"28-04-2023_13-45-29_MNISTNet_Optimizer\", # reg_mul=0.1\n",
    "    ],\n",
    "    \"MNISTLeakyRelu_normalized_params\": [\n",
    "        \"19-02-2023_18-20-21_MNISTLeakyRelu_Optimizer\", # reg_mul=0\n",
    "        \"29-04-2023_01-38-19_MNISTLeakyRelu_Optimizer\", # reg_mul=0.01\n",
    "        \"28-04-2023_02-11-33_MNISTLeakyRelu_Optimizer\", # reg_mul=0.1\n",
    "        \"29-04-2023_23-46-08_MNISTLeakyRelu_Optimizer\", # reg_mul=0.5\n",
    "    ],\n",
    "    \"MNISTLeakyRelu\": [\n",
    "        \"19-02-2023_18-20-21_MNISTLeakyRelu_Optimizer\", # reg_mul=0\n",
    "        # \"...\", # reg_mul=0.01\n",
    "        \"27-04-2023_16-39-10_MNISTLeakyRelu_Optimizer\", # reg_mul=0.1\n",
    "    ],\n",
    "    \"MNISTReluBatchNorm\": [\n",
    "        \"26-04-2023_01-22-31_MNISTReluBatchNorm_Optimizer\", # reg_mul=0\n",
    "        \"26-04-2023_01-31-59_MNISTReluBatchNorm_Optimizer\", # reg_mul=0.01\n",
    "        \"27-04-2023_01-37-23_MNISTReluBatchNorm_Optimizer\", # reg_mul=0.1\n",
    "    ],\n",
    "    \"MNISTReluBatchNorm_normalized_params\": [\n",
    "        \"26-04-2023_01-22-31_MNISTReluBatchNorm_Optimizer\", # reg_mul=0\n",
    "        \"29-04-2023_23-51-43_MNISTReluBatchNorm_Optimizer\" # reg_mul=0.01\n",
    "        \"28-04-2023_11-48-30_MNISTReluBatchNorm_Optimizer\", # reg_mul=0.1\n",
    "    ]\n",
    "}\n",
    "for k in l2o_opter_ckpt_dirs_all:\n",
    "    l2o_opter_ckpt_dirs_all[k] = [os.path.join(ckpt_root_dir, d) for d in l2o_opter_ckpt_dirs_all[k]]\n",
    "\n",
    "baselines_dir = \"./ckpt/baselines\"\n",
    "baselines_to_test_against = [\n",
    "    (\"Adam\", optim.Adam, {\"lr\": find_best_lr_normal}),\n",
    "    (\"SGD\", optim.SGD, {\"lr\": find_best_lr_normal, \"momentum\": 0.9}),\n",
    "]\n",
    "\n",
    "data_cls = MNIST\n",
    "data_config = {\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "opter_key = \"MNISTNet\"\n",
    "optee_cls = MNISTReluBig2Layer\n",
    "optee_config = {}\n",
    "# optee_config = {\"affine\": True, \"track_running_stats\": True}\n",
    "# optee_config = {\"layer_sizes\": [100,100]}\n",
    "\n",
    "show_max_iters = 200\n",
    "log_loss = True\n",
    "metric = \"train_loss\" # [\"train_loss\", \"test_loss\", \"train_acc\", \"test_acc\"]\n",
    "\n",
    "l2o_opter_ckpt_dirs = l2o_opter_ckpt_dirs_all[opter_key]\n",
    "optee_nickname = f\"{optee_cls.__name__}_{optee_config}\"\n",
    "run_nickname = f\"{optee_nickname}_{data_cls.__name__}_{data_config}\"\n",
    "\n",
    "fig_name = f\"{metric}_comparison_{optee_nickname}_meta_trained_on_{opter_key}_{show_max_iters}.pdf\"\n",
    "if log_loss is True:\n",
    "    fig_name = f\"log_{fig_name}\"\n",
    "fig_dir = \"../results/publication/reg_comparison\"\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "fig_dir = None\n",
    "\n",
    "print(f\"Final destination: {os.path.join(fig_dir, fig_name) if fig_dir is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load baseline metrics from disk\n",
    "baseline_metrics = dict()\n",
    "\n",
    "### load metrics for all considered baselines\n",
    "for (opter_name, baseline_opter_cls, baseline_opter_config) in baselines_to_test_against:\n",
    "    baseline_opter_config_copy = deepcopy(baseline_opter_config)\n",
    "    \n",
    "    if \"lr\" in baseline_opter_config and callable(baseline_opter_config[\"lr\"]):\n",
    "        baseline_opter_config_copy[\"lr\"] = baseline_opter_config_copy[\"lr\"].__name__ # replace function with its name\n",
    "\n",
    "    baseline_dir_name = f\"{opter_name}_{baseline_opter_config_copy}\" \\\n",
    "        + f\"_{optee_cls.__name__}_{optee_config}\" \\\n",
    "        + f\"_{data_cls.__name__}_{data_config}\"\n",
    "    metrics_path = os.path.join(baselines_dir, baseline_dir_name, \"metrics.npy\")\n",
    "    \n",
    "    ### load\n",
    "    print(f\"Loading {metrics_path}\")\n",
    "    baseline_metrics[opter_name] = np.load(metrics_path, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load all l2o opters from disk (results of meta-testing + configs)\n",
    "l2o_opters = []\n",
    "\n",
    "for l2o_opter_ckpt_dir in l2o_opter_ckpt_dirs:\n",
    "    ### load previous checkpoint (and skip meta-training of a new l2O optimizer)\n",
    "    print(f\"Loading {l2o_opter_ckpt_dir}\")\n",
    "    _, config, _ = load_ckpt(dir_path=l2o_opter_ckpt_dir)\n",
    "    assert l2o_opter_ckpt_dir == config[\"ckpt_base_dir\"]\n",
    "    l2o_opter_dict = {\n",
    "        \"l2o_opter_ckpt_dir\": l2o_opter_ckpt_dir,\n",
    "        \"config\": config,\n",
    "        \"metrics\": dict(),\n",
    "    }\n",
    "\n",
    "    for metrics_file in [f_name for f_name in os.listdir(config[\"ckpt_base_dir\"]) if f_name.startswith(\"metrics_\")]:\n",
    "        metrics_name = metrics_file[8:-4] # remove the \"metrics_\" prefix and \".npy\" suffix\n",
    "        l2o_opter_dict[\"metrics\"][metrics_name] = np.load(os.path.join(config[\"ckpt_base_dir\"], metrics_file), allow_pickle=True).item()\n",
    "    \n",
    "    l2o_opters.append(l2o_opter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot comparison\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "### baseline optimizers\n",
    "for opter_name, opter_metrics in baseline_metrics.items():\n",
    "    if \"test\" in metric:\n",
    "        x = np.arange(config[\"meta_training\"][\"eval_iter_freq\"], show_max_iters + 1, config[\"meta_training\"][\"eval_iter_freq\"])\n",
    "        y = np.mean(opter_metrics[metric][:,:show_max_iters // 10], axis=0)\n",
    "        y_min = np.min(opter_metrics[metric][:,:show_max_iters // 10], axis=0)\n",
    "        y_max = np.max(opter_metrics[metric][:,:show_max_iters // 10], axis=0)\n",
    "    else:\n",
    "        x = range(opter_metrics[metric][:,:show_max_iters].shape[1])\n",
    "        y = np.mean(opter_metrics[metric][:,:show_max_iters], axis=0)\n",
    "        y_min = np.min(opter_metrics[metric][:,:show_max_iters], axis=0)\n",
    "        y_max = np.max(opter_metrics[metric][:,:show_max_iters], axis=0)\n",
    "    sns.lineplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        label=opter_name,\n",
    "        linestyle=\"--\",\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "### L2O optimizers\n",
    "for l2o_opter_dict in l2o_opters:\n",
    "    metrics = l2o_opter_dict[\"metrics\"][run_nickname]\n",
    "    config = l2o_opter_dict[\"config\"]\n",
    "\n",
    "    if \"test\" in metric:\n",
    "        x = np.arange(config[\"meta_training\"][\"eval_iter_freq\"], show_max_iters + 1, config[\"meta_training\"][\"eval_iter_freq\"])\n",
    "        y = np.mean(metrics[metric][:,:show_max_iters // config[\"meta_training\"][\"eval_iter_freq\"]], axis=0)\n",
    "        y_min = np.min(metrics[metric][:,:show_max_iters // config[\"meta_training\"][\"eval_iter_freq\"]], axis=0)\n",
    "        y_max = np.max(metrics[metric][:,:show_max_iters // config[\"meta_training\"][\"eval_iter_freq\"]], axis=0)\n",
    "    else:\n",
    "        x = range(metrics[metric][:,:show_max_iters].shape[1])\n",
    "        y = np.mean(metrics[metric][:,:show_max_iters], axis=0)\n",
    "        y_min = np.min(metrics[metric][:,:show_max_iters], axis=0)\n",
    "        y_max = np.max(metrics[metric][:,:show_max_iters], axis=0)\n",
    "\n",
    "    reg_func_name = config['meta_training']['opter_updates_reg_func'].__name__.replace(\"regularize_updates_\", \"\") if config['meta_training']['opter_updates_reg_func'] is not None else \"None\"\n",
    "    sns.lineplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        label=fr\"L2O, $\\alpha$={config['meta_training']['reg_mul']}\",\n",
    "        linewidth=1,\n",
    "        ax=ax,\n",
    "    )\n",
    "    \n",
    "\n",
    "### plot settings\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "if metric == \"train_loss\":\n",
    "    metric_as_label = \"Train Loss\"\n",
    "elif metric == \"test_loss\":\n",
    "    metric_as_label = \"Test Loss\"\n",
    "elif metric == \"train_acc\":\n",
    "    metric_as_label = \"Train Accuracy\"\n",
    "elif metric == \"test_acc\":\n",
    "    metric_as_label = \"Test Accuracy\"\n",
    "else:\n",
    "    metric_as_label = metric\n",
    "ax.set_ylabel(metric_as_label)\n",
    "\n",
    "# set y to log scale\n",
    "if log_loss and \"loss\" in metric:\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "if \"acc\" in metric:\n",
    "    ax.set_ylim(0.6, 1.0)\n",
    "elif log_loss is not True:\n",
    "    ax.set_ylim(0.0, None)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=3)\n",
    "# legend = ax.get_legend()\n",
    "# for legend_handle in legend.legendHandles:\n",
    "#     legend_handle.set_linewidth(3.0)\n",
    "\n",
    "\n",
    "# x-ticks\n",
    "x_ticks = ax.get_xticks()\n",
    "x_ticks = np.linspace(0, 200, 3)\n",
    "ax.set_xticks(x_ticks)\n",
    "\n",
    "### y-ticks\n",
    "if log_loss is not True:\n",
    "    y_max = ax.get_ylim()[1]\n",
    "    y_max = np.ceil(y_max / 0.5) * 0.5 # round\n",
    "    y_ticks = np.linspace(0, y_max, 3)\n",
    "    ax.set_yticks(y_ticks)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### save the figure\n",
    "if fig_dir is not None:\n",
    "    fig.savefig(os.path.join(fig_dir, fig_name), bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params, Grads, Updates\n",
    "Plot the norm and mean abs value of parameters, gradients, and parameter updates across training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "ckpt_iter_freq = 10\n",
    "max_iters = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### baseline optimizers\n",
    "for baseline_name in baselines.keys():\n",
    "    baselines[baseline_name][\"params_grads\"] = {k: {} for k in [\"param_norms\", \"param_abs_means\", \"grad_norms\", \"grad_abs_means\", \"updates_norms\", \"updates_abs_means\"]}\n",
    "    opter_metrics = baselines[baseline_name][\"metrics\"]\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "\n",
    "    assert ckpt_iter_freq % config[\"meta_testing\"][\"ckpt_iter_freq\"] == 0\n",
    "\n",
    "    ### collect for all test runs\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        test_run_params_grads = {k: {} for k in [\"param_norms\", \"param_abs_means\", \"grad_norms\", \"grad_abs_means\", \"updates_norms\", \"updates_abs_means\"]}\n",
    "\n",
    "        for iter_i in range(ckpt_iter_freq, max_iters + 1, ckpt_iter_freq):\n",
    "            ckpt_dir = os.path.join(os.environ[\"CKPT_PATH\"], \"baselines\", baselines[baseline_name][\"baseline_dir\"], \"ckpt\")\n",
    "            optee, opter, optee_grads, loss_history = load_baseline_opter_ckpt(\n",
    "                path=os.path.join(ckpt_dir, f\"run{test_run_i}_{iter_i}.pt\"),\n",
    "                optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "                opter_cls=config[\"meta_testing\"][\"baseline_opter_cls\"],\n",
    "                optee_config=config[\"meta_testing\"][\"optee_config\"],\n",
    "                opter_config=config[\"meta_testing\"][\"baseline_opter_config\"]\n",
    "            )\n",
    "            optee_updates = get_baseline_opter_param_updates(optee, opter)\n",
    "        \n",
    "            for n, p in optee.all_named_parameters():\n",
    "                if not p.requires_grad:\n",
    "                    continue\n",
    "\n",
    "                for k in test_run_params_grads:\n",
    "                    if n not in test_run_params_grads[k]:\n",
    "                        test_run_params_grads[k][n] = []\n",
    "\n",
    "                test_run_params_grads[\"param_norms\"][n].append(p.norm().item())\n",
    "                test_run_params_grads[\"param_abs_means\"][n].append(p.abs().mean().item())\n",
    "                test_run_params_grads[\"grad_norms\"][n].append(p.grad.norm().item())\n",
    "                test_run_params_grads[\"grad_abs_means\"][n].append(p.grad.abs().mean().item())\n",
    "                test_run_params_grads[\"updates_norms\"][n].append(optee_updates[n].norm().item())\n",
    "                test_run_params_grads[\"updates_abs_means\"][n].append(optee_updates[n].abs().mean().item())\n",
    "\n",
    "        ### add to all test runs\n",
    "        for k in test_run_params_grads:\n",
    "            for n in test_run_params_grads[k]:\n",
    "                if n not in baselines[baseline_name][\"params_grads\"][k]:\n",
    "                    baselines[baseline_name][\"params_grads\"][k][n] = []\n",
    "                baselines[baseline_name][\"params_grads\"][k][n].append(test_run_params_grads[k][n])\n",
    "\n",
    "    ### convert to np arrays\n",
    "    for k in baselines[baseline_name][\"params_grads\"]:\n",
    "        for n in baselines[baseline_name][\"params_grads\"][k]:\n",
    "            baselines[baseline_name][\"params_grads\"][k][n] = np.array(baselines[baseline_name][\"params_grads\"][k][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### l2o optimizers\n",
    "for l2o_name in l2os:\n",
    "    config = l2os[l2o_name][\"config\"]\n",
    "    assert ckpt_iter_freq % config[\"meta_testing\"][\"ckpt_iter_freq\"] == 0\n",
    "    if \"params_grads\" in l2os[l2o_name] and len(l2os[l2o_name][\"params_grads\"][\"param_norms\"]) > 0:\n",
    "        print(f\"Skipping {l2o_name}\")\n",
    "        continue\n",
    "    l2os[l2o_name][\"params_grads\"] = {k: {} for k in [\"param_norms\", \"param_abs_means\", \"grad_norms\", \"grad_abs_means\", \"updates_norms\", \"updates_abs_means\"]}\n",
    "    \n",
    "    ### collect for all test runs\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        test_run_params_grads = {k: {} for k in [\"param_norms\", \"param_abs_means\", \"grad_norms\", \"grad_abs_means\", \"updates_norms\", \"updates_abs_means\"]}\n",
    "\n",
    "        for iter_i in range(ckpt_iter_freq, max_iters + 1, ckpt_iter_freq):\n",
    "            ### load L2O optimizer\n",
    "            optee, opter, optee_grads, optee_updates, loss_history = load_l2o_opter_ckpt(\n",
    "                path=os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"], f\"run{test_run_i}_{iter_i}.pt\"),\n",
    "                optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "                opter_cls=config[\"opter_cls\"],\n",
    "                optee_config=config[\"meta_testing\"][\"optee_config\"],\n",
    "                opter_config=config[\"opter_config\"],\n",
    "            )\n",
    "            # scale updates by optee update lr\n",
    "            optee_updates = {n: p * config[\"meta_testing\"][\"optee_updates_lr\"] for n, p in optee_updates.items()}\n",
    "\n",
    "            for n, p in optee.all_named_parameters():\n",
    "                if not p.requires_grad:\n",
    "                    continue\n",
    "\n",
    "                for k in test_run_params_grads:\n",
    "                    if n not in test_run_params_grads[k]:\n",
    "                        test_run_params_grads[k][n] = []\n",
    "                test_run_params_grads[\"param_norms\"][n].append(p.norm().item())\n",
    "                test_run_params_grads[\"param_abs_means\"][n].append(p.abs().mean().item())\n",
    "                test_run_params_grads[\"grad_norms\"][n].append(p.grad.norm().item())\n",
    "                test_run_params_grads[\"grad_abs_means\"][n].append(p.grad.abs().mean().item())\n",
    "                test_run_params_grads[\"updates_norms\"][n].append(optee_updates[n].norm().item())\n",
    "                test_run_params_grads[\"updates_abs_means\"][n].append(optee_updates[n].abs().mean().item())\n",
    "        \n",
    "        ### add to all test runs\n",
    "        for k in test_run_params_grads:\n",
    "            for n in test_run_params_grads[k]:\n",
    "                if n not in l2os[l2o_name][\"params_grads\"][k]:\n",
    "                    l2os[l2o_name][\"params_grads\"][k][n] = []\n",
    "                l2os[l2o_name][\"params_grads\"][k][n].append(test_run_params_grads[k][n])\n",
    "        \n",
    "    ### convert to np arrays\n",
    "    for k in l2os[l2o_name][\"params_grads\"]:\n",
    "        for n in l2os[l2o_name][\"params_grads\"][k]:\n",
    "            l2os[l2o_name][\"params_grads\"][k][n] = np.array(l2os[l2o_name][\"params_grads\"][k][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config for plotting\n",
    "params_grads_metrics = [\"param_norms\", \"param_abs_means\", \"grad_norms\", \"grad_abs_means\", \"updates_norms\", \"updates_abs_means\"]\n",
    "save_to_dir = \"../results/sym_breaking_regularization/MNISTReluBatchNorm_meta_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, box_pts=10):\n",
    "    box = np.ones(box_pts) / box_pts\n",
    "    y_smooth = np.convolve(y, box, mode=\"same\")\n",
    "    return y_smooth\n",
    "\n",
    "for k in params_grads_metrics:\n",
    "    fig = plt.figure(figsize=(22, 20))\n",
    "    x_ticks = np.arange(ckpt_iter_freq, max_iters + 1, ckpt_iter_freq)\n",
    "\n",
    "    for i, n in enumerate(baselines[\"SGD\"][\"params_grads\"][k]):\n",
    "        ax = fig.add_subplot(3, 2, i + 1)\n",
    "\n",
    "        for baseline_name, baseline_dict in baselines.items():\n",
    "            if n not in baseline_dict[\"params_grads\"][k]:\n",
    "                continue\n",
    "            # sns.lineplot(x=x_ticks, y=smooth(baseline_dict[\"params_grads\"][k][n][:max_iters]), alpha=0.8, linewidth=1.5, linestyle=\"--\", ax=ax, label=f\"{baseline_name}\")\n",
    "            y_mean = np.mean(baseline_dict[\"params_grads\"][k][n][:, :max_iters // ckpt_iter_freq], axis=0)\n",
    "            sns.lineplot(x=x_ticks, y=y_mean, alpha=0.8, linewidth=1.5, linestyle=\"--\", ax=ax, label=f\"{baseline_name}\")\n",
    "\n",
    "            ### add error bars\n",
    "            y_std = np.std(baseline_dict[\"params_grads\"][k][n][:, :max_iters // ckpt_iter_freq], axis=0)\n",
    "            ax.fill_between(x=x_ticks, y1=y_mean - y_std, y2=y_mean + y_std, alpha=0.2)\n",
    "\n",
    "        for l2o_name, l2o_dict in l2os.items():\n",
    "            if n not in l2o_dict[\"params_grads\"][k]:\n",
    "                continue\n",
    "            # sns.lineplot(x=x_ticks, y=smooth(l2o_dict[\"params_grads\"][k][n][:max_iters]), alpha=0.8, linewidth=1.5, ax=ax, label=f\"{l2o_name}\")\n",
    "            y_mean = np.mean(l2o_dict[\"params_grads\"][k][n][:, :max_iters // ckpt_iter_freq], axis=0)\n",
    "            sns.lineplot(x=x_ticks, y=y_mean, alpha=0.8, linewidth=1.5, ax=ax, label=f\"{l2o_name}\")\n",
    "\n",
    "            ### add error bars\n",
    "            y_std = np.std(l2o_dict[\"params_grads\"][k][n][:, :max_iters // ckpt_iter_freq], axis=0)\n",
    "            ax.fill_between(x=x_ticks, y1=y_mean - y_std, y2=y_mean + y_std, alpha=0.2)\n",
    "\n",
    "        ax.set_xlabel(\"Iteration\")\n",
    "        ax.set_title(k.replace(\"_\", \" \").title() + \": \" + n, fontsize=13, fontweight=\"bold\")\n",
    "        ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if save_to_dir is not None:\n",
    "        ### save the figure\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(os.path.join(save_to_dir, f\"{k}.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conservation Law Breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = \"meta_testing\"\n",
    "max_iters = 500\n",
    "reg_func = regularize_translation_conservation_law_breaking\n",
    "\n",
    "### collect deviations\n",
    "for l2o_name in l2os:\n",
    "    config = l2os[l2o_name][\"config\"]\n",
    "    l2os[l2o_name][reg_func.__name__] = np.array(collect_conservation_law_deviations(\n",
    "        func=reg_func,\n",
    "        opter_cls=Optimizer,\n",
    "        opter_config=config[\"opter_config\"],\n",
    "        optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "        optee_config=config[\"meta_testing\"][\"optee_config\"],\n",
    "        ckpt_iter_freq=config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "        n_iters=config[\"meta_testing\"][\"n_iters\"],\n",
    "        ckpt_path_prefix=os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"], \"\"),\n",
    "        is_l2o=True,\n",
    "        max_iters=max_iters,\n",
    "    ))\n",
    "\n",
    "### Baseline optimizers\n",
    "for baseline_name in baselines:\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "    ### collect deviations\n",
    "    baselines[baseline_name][reg_func.__name__] = np.array(collect_conservation_law_deviations(\n",
    "        func=reg_func,\n",
    "        opter_cls=config[\"meta_testing\"][\"baseline_opter_config\"],\n",
    "        opter_config=config[\"meta_testing\"][\"baseline_opter_config\"],\n",
    "        optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "        optee_config=baselines[baseline_name][\"config\"][\"meta_testing\"][\"optee_config\"],\n",
    "        ckpt_iter_freq=baselines[baseline_name][\"config\"][\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "        n_iters=baselines[baseline_name][\"config\"][\"meta_testing\"][\"n_iters\"],\n",
    "        ckpt_path_prefix=os.path.join(os.environ[\"CKPT_PATH\"], baselines[baseline_name][\"baseline_dir\"], \"ckpt/\"),\n",
    "        is_l2o=False,\n",
    "        max_iters=max_iters,\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking Geometric Constraints on Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = \"meta_testing\"\n",
    "max_iters = None\n",
    "collect_func = collect_translation_sym_deviations\n",
    "\n",
    "### L2O optimizers\n",
    "for l2o_name in l2os:\n",
    "    config = l2os[l2o_name][\"config\"]\n",
    "    l2os[l2o_name][collect_func.__name__ + \"_grads\"] = []\n",
    "    l2os[l2o_name][collect_func.__name__ + \"_updates\"] = []\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        grad_deviations, param_update_deviations = collect_func(\n",
    "            ckpt_iter_freq=config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "            n_iters=config[\"meta_testing\"][\"n_iters\"],\n",
    "            optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "            opter_cls=config[\"opter_cls\"],\n",
    "            optee_config=config[\"meta_testing\"][\"optee_config\"],\n",
    "            opter_config=config[\"opter_config\"],\n",
    "            phase=\"meta_testing\",\n",
    "            ckpt_path_prefix=os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"], f\"run{test_run_i}_\"),\n",
    "            max_iters=max_iters,\n",
    "        )\n",
    "        if np.ndim(grad_deviations) == 2:\n",
    "            ### sum the deviations (weight and bias)\n",
    "            grad_deviations = grad_deviations.sum(-1)\n",
    "            param_update_deviations = param_update_deviations.sum(-1)\n",
    "        l2os[l2o_name][collect_func.__name__ + \"_grads\"].append(grad_deviations)\n",
    "        l2os[l2o_name][collect_func.__name__ + \"_updates\"].append(param_update_deviations)\n",
    "\n",
    "### Baseline optimizers\n",
    "for baseline_name in baselines:\n",
    "    if \"baseline_opter_cls\" in baselines[baseline_name][\"config\"]:\n",
    "        baseline_opter_cls = baselines[baseline_name][\"config\"][\"baseline_opter_cls\"]\n",
    "    elif \"sgd\" in baseline_name.lower():\n",
    "        baseline_opter_cls = optim.SGD\n",
    "    elif \"adam\" in baseline_name.lower():\n",
    "        baseline_opter_cls = optim.Adam\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "    baseline_opter_config = baselines[baseline_name][\"config\"][\"meta_testing\"][\"baseline_opter_config\"]\n",
    "    \n",
    "    ### collect deviations\n",
    "    baselines[baseline_name][collect_func.__name__ + \"_grads\"] = []\n",
    "    baselines[baseline_name][collect_func.__name__ + \"_updates\"] = []\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        grad_deviations, param_update_deviations = collect_func(\n",
    "            ckpt_iter_freq=config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "            n_iters=config[\"meta_testing\"][\"n_iters\"],\n",
    "            optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "            optee_config=config[\"meta_testing\"][\"optee_config\"],\n",
    "            opter_cls=baseline_opter_cls,\n",
    "            opter_config=baseline_opter_config,\n",
    "            phase=\"meta_testing\",\n",
    "            ckpt_path_prefix=os.path.join(baselines[baseline_name][\"baseline_dir\"], f\"ckpt/run{test_run_i}_\"),\n",
    "            max_iters=max_iters,\n",
    "        )\n",
    "        if np.ndim(grad_deviations) == 2:\n",
    "            ### sum the deviations (weight and bias)\n",
    "            grad_deviations = grad_deviations.sum(-1)\n",
    "            param_update_deviations =  param_update_deviations.sum(-1)\n",
    "        baselines[baseline_name][collect_func.__name__ + \"_grads\"].append(grad_deviations)\n",
    "        baselines[baseline_name][collect_func.__name__ + \"_updates\"].append(param_update_deviations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting - Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deviations(dict_key_to_plot, log=False, abs_values=False, max_iters=None, save_fig_to_path=None):\n",
    "    ### plot comparison\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    x_ticks = np.arange(\n",
    "        0,\n",
    "        min(max_iters + 1, config[\"meta_testing\"][\"n_iters\"] + 1) if max_iters != None else config[\"meta_testing\"][\"n_iters\"] + 1,\n",
    "        config[\"meta_testing\"][\"ckpt_iter_freq\"]\n",
    "    )\n",
    "\n",
    "    ### L2O optimizers\n",
    "    for l2o_name, l2o_dict in l2os.items():\n",
    "        to_plot = np.array(\n",
    "            l2o_dict[dict_key_to_plot] if not abs_values else np.abs(l2o_dict[dict_key_to_plot])\n",
    "        )[:, :len(x_ticks)]\n",
    "        sns.lineplot(\n",
    "            x=x_ticks,\n",
    "            y=to_plot.mean(0),\n",
    "            label=l2o_name,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=to_plot.mean(0) - to_plot.std(0),\n",
    "            y2=to_plot.mean(0) + to_plot.std(0),\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "    ### baseline optimizers\n",
    "    for baseline_name, baseline_dict in baselines.items():\n",
    "        to_plot = np.array(\n",
    "            baseline_dict[dict_key_to_plot] if not abs_values else np.abs(baseline_dict[dict_key_to_plot])\n",
    "        )[:, :len(x_ticks)]\n",
    "        sns.lineplot(\n",
    "            x=x_ticks,\n",
    "            y=to_plot.mean(0),\n",
    "            label=baseline_name,\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.5,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=to_plot.mean(0) - to_plot.std(0),\n",
    "            y2=to_plot.mean(0) + to_plot.std(0),\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "    ### plot settings\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Deviation\" if not abs_values else \"Absolute Deviation\")\n",
    "\n",
    "    # set y to log scale\n",
    "    if log:\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "    # ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=2)\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    legend = ax.get_legend()\n",
    "    for legend_handle in legend.legendHandles:\n",
    "        legend_handle.set_linewidth(3.0)\n",
    "\n",
    "    # x-ticks\n",
    "    x_ticks = ax.get_xticks()\n",
    "    x_ticks = np.linspace(0, max_iters, 3)\n",
    "    ax.set_xticks(x_ticks)\n",
    "    \n",
    "    # y-ticks\n",
    "    # y_max = ax.get_ylim()[1]\n",
    "    # y_max = np.ceil(y_max / 100) * 100 # round\n",
    "    # y_ticks = np.linspace(0, y_max, 4)\n",
    "    # ax.set_yticks(y_ticks)\n",
    "    # y_ticks = ax.get_yticks()\n",
    "    # ax.set_yticks(y_ticks[::3])\n",
    "    # ax.set_ylim(0, 90)\n",
    "    # ax.set_yticks([0, 45, 90])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ### save the figure\n",
    "    if save_fig_to_path is not None:\n",
    "        fig.savefig(save_fig_to_path, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_plot = collect_func.__name__ + \"_updates\"\n",
    "# key_to_plot = reg_func.__name__\n",
    "save_fig_to_path = os.path.join(\n",
    "    \"..\",\n",
    "    \"results\",\n",
    "    \"publication\",\n",
    "    \"constraint_deviations\",\n",
    "    \"translation_sym_meta_testing_MNISTNet_{}_200.pdf\"\n",
    ")\n",
    "save_fig_to_path = None\n",
    "\n",
    "plot_deviations(\n",
    "    key_to_plot,\n",
    "    log=False,\n",
    "    abs_values=True,\n",
    "    max_iters=200,\n",
    "    save_fig_to_path=save_fig_to_path,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect for L2Os\n",
    "for opter_name in l2os:\n",
    "    print(f\"Running {opter_name}...\")\n",
    "    config = l2os[opter_name][\"config\"]\n",
    "    ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"])\n",
    "\n",
    "    run_history_all_test_runs = []\n",
    "    # for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "    for test_run_i in range(1):\n",
    "        print(f\"  Test run {test_run_i}...\")\n",
    "        # ckpt_prefix = f\"run{test_run_i}_\"\n",
    "        ckpt_prefix = f\"\"\n",
    "        \n",
    "        history = {k: [] for k in [\"hidden_rnn_1\", \"hidden_rnn_2\", \"cell_rnn_1\", \"cell_rnn_2\", \"updates\"]}\n",
    "        for iter_i in range(\n",
    "            config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "            config[\"meta_testing\"][\"n_iters\"] + 1,\n",
    "            config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "        ):\n",
    "            ### load checkpoint\n",
    "            ckpt_path = os.path.join(ckpts_dir, f\"{ckpt_prefix}{iter_i}.pt\")\n",
    "            ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "            \n",
    "            hidden_states = ckpt[\"hidden_states\"] # (2, num_params, hidden_size)\n",
    "            cell_states = ckpt[\"cell_states\"] # (2, num_params, hidden_size)\n",
    "            optee_updates = ckpt[\"optimizee_updates\"]\n",
    "\n",
    "            history[\"hidden_rnn_1\"].append(hidden_states[0].mean(dim=0).tolist())\n",
    "            history[\"hidden_rnn_2\"].append(hidden_states[1].mean(dim=0).tolist())\n",
    "\n",
    "            history[\"cell_rnn_1\"].append(cell_states[0].mean(dim=0).tolist())\n",
    "            history[\"cell_rnn_2\"].append(cell_states[1].mean(dim=0).tolist())\n",
    "        \n",
    "        for k in history:\n",
    "            history[k] = np.array(history[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "to_plot = \"hidden_rnn_1\"\n",
    "nperseg = 20\n",
    "noverlap = 19\n",
    "nfft = 20\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "for channel_idx in range(history[to_plot].shape[-1]):\n",
    "    ax = fig.add_subplot(5, 4, channel_idx + 1)\n",
    "    f, t, Sxx = signal.spectrogram(\n",
    "        x=history[to_plot][:, channel_idx],\n",
    "        fs=1.0,\n",
    "        window=\"hann\",\n",
    "        nperseg=nperseg,\n",
    "        noverlap=noverlap,\n",
    "        nfft=nfft,\n",
    "    )\n",
    "    ax.pcolormesh(t, f, Sxx, shading=\"gouraud\")\n",
    "    ax.set_ylabel(\"Frequency [Hz]\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_title(f\"Channel {channel_idx}\")\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavy-tail gradient/update noise\n",
    "- From the [paper](http://proceedings.mlr.press/v97/simsekli19a/simsekli19a.pdf) *A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks. U. Simsekli, L. Sagun, M. Gurbuzbalaban.In Proceedings of the 36th International Conference on Machine Learning, (ICML) 2019.*\n",
    "- [GitHub repository](https://github.com/umutsimsekli/sgd_tail_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(optee, data_loader, opter=None, hidden_states=None, cell_states=None, optee_updates_lr=None):\n",
    "    ### calculate gradient (and update) noise over the the given data loader\n",
    "    optee.eval()\n",
    "    tmp_optee_optim = optim.SGD(optee.parameters(), lr=0.0) # just for zeroing out the gradients\n",
    "    grads = []\n",
    "    param_updates = []\n",
    "    losses = []\n",
    "    accs = []\n",
    "    n_minibatches = 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        n_minibatches += 1\n",
    "        tmp_optee_optim.zero_grad()\n",
    "        \n",
    "        x, y = x.view(-1, 784).cuda(), y.cuda()\n",
    "        loss, acc = optee(inp=x, out=y, return_acc=True)\n",
    "        loss.backward()\n",
    "\n",
    "        ### collect gradients\n",
    "        grads.append(torch.cat([p.grad.detach().view(-1) for n, p in optee.all_named_parameters() if p.requires_grad]).cpu())\n",
    "\n",
    "        ### collect updates\n",
    "        if opter is not None:\n",
    "            if isinstance(opter, Optimizer):\n",
    "                ### L2O\n",
    "                curr_l2o_updates = []\n",
    "                offset = 0\n",
    "                for name, p in optee.all_named_parameters():\n",
    "                    if p.requires_grad == False: # batchnorm stats\n",
    "                        continue\n",
    "\n",
    "                    cur_sz = int(np.prod(p.size()))\n",
    "                    gradients = p.grad.detach().view(cur_sz, 1)\n",
    "                    updates, _, _ = opter(\n",
    "                        optee_grads=gradients,\n",
    "                        hidden=[h[offset : offset + cur_sz] for h in hidden_states],\n",
    "                        cell=[c[offset : offset + cur_sz] for c in cell_states],\n",
    "                        additional_inp=None,\n",
    "                    )\n",
    "                    offset += cur_sz\n",
    "                    curr_l2o_updates.append(optee_updates_lr * updates.detach().view(-1))\n",
    "                param_updates.append(torch.cat(curr_l2o_updates).cpu())\n",
    "            else:\n",
    "                ### baseline optimizer\n",
    "                param_updates.append(\n",
    "                    torch.cat([\n",
    "                        p.detach().view(-1).cpu() for p in get_baseline_opter_param_updates(optee, opter, verbose=False).values()\n",
    "                    ], dim=0)\n",
    "                )\n",
    "\n",
    "        ### track history\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc.item())\n",
    "\n",
    "    optee_total_params = len(grads[0])\n",
    "\n",
    "    ### gradients\n",
    "    grads = torch.stack(grads, dim=0) # (n_minibatches, optee_total_params)\n",
    "    mean_grad = grads.mean(dim=0) # (optee_total_params,)\n",
    "    grads_noise_norm = (grads - mean_grad).norm(dim=1) # (n_minibatches,)\n",
    "    # get the tail index alpha\n",
    "    N = optee_total_params * n_minibatches\n",
    "    for i in range(1, 1 + int(np.sqrt(N))):\n",
    "        if N % i == 0:\n",
    "            m = i\n",
    "    grads_alpha = alpha_estimator(m, (grads - mean_grad).view(-1, 1))\n",
    "\n",
    "    ### l2o updates\n",
    "    updates_noise_norm, updates_alpha = None, None\n",
    "    if opter is not None:\n",
    "        param_updates = torch.stack(param_updates, dim=0) # (n_minibatches, optee_total_params)\n",
    "        mean_l2o_updates = param_updates.mean(dim=0) # (optee_total_params,)\n",
    "        updates_noise_norm = (param_updates - mean_l2o_updates).norm(dim=1) # (n_minibatches,)\n",
    "        # get the tail index alpha\n",
    "        N = optee_total_params * n_minibatches\n",
    "        for i in range(1, 1 + int(np.sqrt(N))):\n",
    "            if N % i == 0:\n",
    "                m = i\n",
    "        updates_alpha = alpha_estimator(m, (param_updates - mean_l2o_updates).view(-1, 1))\n",
    "\n",
    "    return (\n",
    "        losses,\n",
    "        accs,\n",
    "        grads_noise_norm,\n",
    "        grads_alpha,\n",
    "        updates_noise_norm,\n",
    "        updates_alpha,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_grad_update_noise_for_l2o(ckpt_path, run_history, config, train_data, test_data):\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    optee = config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"]).cuda()\n",
    "    optee.load_state_dict(ckpt[\"optimizee\"])\n",
    "\n",
    "    ### init l2o optimizer to collect noise in its updates\n",
    "    opter = config[\"opter_cls\"](**config[\"opter_config\"]).cuda()\n",
    "    opter.load_state_dict(ckpt[\"optimizer\"])\n",
    "\n",
    "    ### collect history\n",
    "    for phase, data_loader in ((\"train\", train_data.loader), (\"test\", test_data.loader)):\n",
    "        losses, accs, grads_noise_norm, grads_alpha, updates_noise_norm, updates_alpha = eval_metrics(\n",
    "            optee=optee,\n",
    "            data_loader=data_loader,\n",
    "            opter=opter,\n",
    "            hidden_states=ckpt[\"hidden_states\"],\n",
    "            cell_states=ckpt[\"cell_states\"],\n",
    "            optee_updates_lr=config[\"meta_testing\"][\"optee_updates_lr\"],\n",
    "        )\n",
    "        run_history[phase][\"loss\"].append(np.mean(losses))\n",
    "        run_history[phase][\"acc\"].append(np.mean(accs))\n",
    "        run_history[phase][\"grads_noise_norm\"].append(grads_noise_norm)\n",
    "        run_history[phase][\"grads_alpha\"].append(grads_alpha.item())\n",
    "        run_history[phase][\"updates_noise_norm\"].append(updates_noise_norm)\n",
    "        run_history[phase][\"updates_alpha\"].append(updates_alpha.item())\n",
    "    \n",
    "    return run_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing = True\n",
    "max_n_tests = 3\n",
    "ckpt_iter_freq = 10\n",
    "max_iters = 1000\n",
    "\n",
    "### collect for L2Os\n",
    "for opter_name in l2os:\n",
    "    print(f\"Running {opter_name}...\")\n",
    "    config = l2os[opter_name][\"config\"]\n",
    "    config[\"meta_testing\"][\"n_iters\"] = 1000\n",
    "    ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"])\n",
    "    save_run_history_path = os.path.join(\n",
    "        os.environ[\"CKPT_PATH\"],\n",
    "        config[\"ckpt_base_dir\"],\n",
    "        f\"grads_updates_noise_heavy_tail_alpha_estimates\" +\n",
    "            f\"_{config['meta_testing']['optee_cls'].__name__}_{dict_to_str(config['meta_testing']['optee_config'])}\" +\n",
    "            f\"_{config['meta_testing']['data_cls'].__name__}_{dict_to_str(config['meta_testing']['data_config'])}\" +\n",
    "            f\"_{config['eval_n_tests']}_tests.pt\"\n",
    "    )\n",
    "\n",
    "    if load_existing and os.path.exists(save_run_history_path):\n",
    "        print(f\"  Loading existing run history from {save_run_history_path}\")\n",
    "        run_history = torch.load(save_run_history_path)\n",
    "        l2os[opter_name][\"run_history\"] = run_history\n",
    "        continue\n",
    "\n",
    "    run_history_all_test_runs = []\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        if max_n_tests and test_run_i >= max_n_tests:\n",
    "            break\n",
    "        print(f\"  Test run {test_run_i}...\")\n",
    "        ckpt_prefix = f\"run{test_run_i}_\"\n",
    "    \n",
    "        ### collect\n",
    "        run_history = {k: {k: [] for k in (\"loss\", \"acc\", \"grads_noise_norm\", \"grads_alpha\", \"updates_noise_norm\", \"updates_alpha\")}\n",
    "            for k in (\"train\", \"test\")}\n",
    "        train_data = MNIST(training=True, batch_size=128)\n",
    "        test_data = MNIST(training=False, batch_size=128)\n",
    "\n",
    "        ### load checkpoints within this test run\n",
    "        for iter_i in [1, *range(\n",
    "            ckpt_iter_freq,\n",
    "            min(max_iters, config[\"meta_testing\"][\"n_iters\"]) + 1,\n",
    "            ckpt_iter_freq,\n",
    "        )]:\n",
    "        # for iter_i in [5, *range(\n",
    "        #     ckpt_iter_freq,\n",
    "        #     min(max_iters, config[\"meta_testing\"][\"n_iters\"]) + 1,\n",
    "        #     ckpt_iter_freq,\n",
    "        # )]:\n",
    "            print(f\"  [{iter_i}/{min(max_iters, config['meta_testing']['n_iters'])}]\")\n",
    "\n",
    "            ### load checkpoint\n",
    "            ckpt_path = os.path.join(ckpts_dir, f\"{ckpt_prefix}{iter_i}.pt\")\n",
    "            run_history = collect_grad_update_noise_for_l2o(\n",
    "                ckpt_path=ckpt_path,\n",
    "                run_history=run_history,\n",
    "                config=config,\n",
    "                train_data=train_data,\n",
    "                test_data=test_data\n",
    "            )\n",
    "\n",
    "        ### save current test run\n",
    "        run_history_all_test_runs.append(run_history)\n",
    "\n",
    "    ### save\n",
    "    l2os[opter_name][\"run_history\"] = run_history_all_test_runs\n",
    "    torch.save(run_history_all_test_runs, save_run_history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_grad_update_noise_for_normal(ckpt_path, run_history, config, train_data, test_data):\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    \n",
    "    ### init optee\n",
    "    optee = config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"]).cuda()\n",
    "    optee.load_state_dict(ckpt[\"optimizee\"])\n",
    "\n",
    "    ### init opter\n",
    "    opter = config[\"meta_testing\"][\"baseline_opter_cls\"](optee.parameters(), **config[\"meta_testing\"][\"baseline_opter_config\"])\n",
    "    opter.load_state_dict(ckpt[\"optimizer\"])\n",
    "\n",
    "    ### collect\n",
    "    for phase, data_loader in ((\"train\", train_data.loader), (\"test\", test_data.loader)):\n",
    "        losses, accs, grads_noise_norm, grads_alpha, updates_noise_norm, updates_alpha = eval_metrics(\n",
    "            optee=optee,\n",
    "            data_loader=data_loader,\n",
    "            opter=opter,\n",
    "            hidden_states=None,\n",
    "            cell_states=None\n",
    "        )\n",
    "        run_history[phase][\"loss\"].append(np.mean(losses))\n",
    "        run_history[phase][\"acc\"].append(np.mean(accs))\n",
    "        run_history[phase][\"grads_noise_norm\"].append(grads_noise_norm)\n",
    "        run_history[phase][\"grads_alpha\"].append(grads_alpha.item())\n",
    "        run_history[phase][\"updates_noise_norm\"].append(updates_noise_norm)\n",
    "        run_history[phase][\"updates_alpha\"].append(updates_alpha.item())\n",
    "\n",
    "    return run_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing = True\n",
    "max_n_tests = 3\n",
    "ckpt_iter_freq = 10\n",
    "max_iters = 1000\n",
    "\n",
    "### collect for baselines\n",
    "for baseline_name in baselines:\n",
    "    print(f\"Running {baseline_name}...\")\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "    save_run_history_path = os.path.join(\n",
    "        baselines[baseline_name][\"baseline_dir\"],\n",
    "        f\"grad_noise_run_history_{config['eval_n_tests']}_tests.pt\"\n",
    "    )\n",
    "\n",
    "    if load_existing and os.path.exists(save_run_history_path):\n",
    "        print(f\"  Loading existing run history from {save_run_history_path}\")\n",
    "        run_history = torch.load(save_run_history_path)\n",
    "        baselines[baseline_name][\"run_history\"] = run_history\n",
    "        continue\n",
    "    \n",
    "    run_history_all_test_runs = []\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        if max_n_tests and test_run_i >= max_n_tests:\n",
    "            break\n",
    "        print(f\"  Test run {test_run_i}...\")\n",
    "        ckpt_prefix = f\"run{test_run_i}_\"\n",
    "        ### collect\n",
    "        run_history = {k: {k: [] for k in (\"loss\", \"acc\", \"grads_noise_norm\", \"grads_alpha\", \"updates_noise_norm\", \"updates_alpha\")}\n",
    "            for k in (\"train\", \"test\")}\n",
    "        train_data = MNIST(training=True, batch_size=128)\n",
    "        test_data = MNIST(training=False, batch_size=128)\n",
    "\n",
    "        ### load checkpoints within this test run\n",
    "        for iter_i in [1, *range(\n",
    "            ckpt_iter_freq,\n",
    "            min(max_iters, config[\"meta_testing\"][\"n_iters\"]) + 1,\n",
    "            ckpt_iter_freq,\n",
    "        )]:\n",
    "            print(f\"  [{iter_i}/{min(max_iters, config['meta_testing']['n_iters'])}]\")\n",
    "            \n",
    "            ### load checkpoint\n",
    "            ckpt_path = os.path.join(baselines[baseline_name][\"baseline_dir\"], \"ckpt\", f\"{ckpt_prefix}{iter_i}.pt\")\n",
    "            run_history = collect_grad_update_noise_for_normal(\n",
    "                ckpt_path=ckpt_path,\n",
    "                run_history=run_history,\n",
    "                config=config,\n",
    "                train_data=train_data,\n",
    "                test_data=test_data\n",
    "            )\n",
    "        \n",
    "        ### save current test run\n",
    "        run_history_all_test_runs.append(run_history)\n",
    "\n",
    "    ### save\n",
    "    baselines[baseline_name][\"run_history\"] = run_history_all_test_runs\n",
    "    torch.save(run_history_all_test_runs, save_run_history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocess\n",
    "### turn l2os[opter_name][\"run_history\"][<number-of-test>][\"train\"][<metric>] into l2os[opter_name][\"run_history\"][\"train\"][<metric>][<number-of-test>]\n",
    "for opter_name in l2os:\n",
    "    new_train_run_history = dict()\n",
    "    new_test_run_history = dict()\n",
    "    for test_run_i in range(min(max_n_tests, l2os[opter_name][\"config\"][\"eval_n_tests\"])):\n",
    "        for metric_name in l2os[opter_name][\"run_history\"][test_run_i][\"train\"]:\n",
    "            if test_run_i == 0:\n",
    "                new_train_run_history[metric_name] = [l2os[opter_name][\"run_history\"][test_run_i][\"train\"][metric_name]]\n",
    "                new_test_run_history[metric_name] = [l2os[opter_name][\"run_history\"][test_run_i][\"test\"][metric_name]]\n",
    "            else:\n",
    "                new_train_run_history[metric_name].append(l2os[opter_name][\"run_history\"][test_run_i][\"train\"][metric_name])\n",
    "                new_test_run_history[metric_name].append(l2os[opter_name][\"run_history\"][test_run_i][\"test\"][metric_name])\n",
    "    l2os[opter_name][\"run_history\"] = dict()\n",
    "    l2os[opter_name][\"run_history\"][\"train\"] = new_train_run_history\n",
    "    l2os[opter_name][\"run_history\"][\"test\"] = new_test_run_history\n",
    "\n",
    "for baseline_name in baselines:\n",
    "    new_train_run_history = dict()\n",
    "    new_test_run_history = dict()\n",
    "    for test_run_i in range(min(max_n_tests, baselines[baseline_name][\"config\"][\"eval_n_tests\"])):\n",
    "        for metric_name in baselines[baseline_name][\"run_history\"][test_run_i][\"train\"]:\n",
    "            if test_run_i == 0:\n",
    "                new_train_run_history[metric_name] = [baselines[baseline_name][\"run_history\"][test_run_i][\"train\"][metric_name]]\n",
    "                new_test_run_history[metric_name] = [baselines[baseline_name][\"run_history\"][test_run_i][\"test\"][metric_name]]\n",
    "            else:\n",
    "                new_train_run_history[metric_name].append(baselines[baseline_name][\"run_history\"][test_run_i][\"train\"][metric_name])\n",
    "                new_test_run_history[metric_name].append(baselines[baseline_name][\"run_history\"][test_run_i][\"test\"][metric_name])\n",
    "    baselines[baseline_name][\"run_history\"] = dict()\n",
    "    baselines[baseline_name][\"run_history\"][\"train\"] = new_train_run_history\n",
    "    baselines[baseline_name][\"run_history\"][\"test\"] = new_test_run_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config for plotting\n",
    "show_max_iters = max_iters\n",
    "# show_max_iters = 200\n",
    "plot_l2o_grad_alpha = True\n",
    "plot_l2o_update_alpha = True\n",
    "plot_baseline_grad_alpha = True\n",
    "plot_baseline_update_alpha = True\n",
    "optee_name = \"MNISTNet\"\n",
    "phase = \"train\"\n",
    "save_fig_to_path = os.path.join(\n",
    "    \"../results/heavy_tail_grad_update_noise/publication\",\n",
    "    f\"update_noise_alpha_estimates_{optee_name}_{phase}_{show_max_iters}.pdf\"\n",
    ")\n",
    "save_fig_to_path = None\n",
    "print(f\"Saving figure to {save_fig_to_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot\n",
    "x_ticks = range(\n",
    "    ckpt_iter_freq,\n",
    "    show_max_iters + ckpt_iter_freq + 1 \\\n",
    "        if show_max_iters is None or config[\"meta_testing\"][\"n_iters\"] <= show_max_iters \\\n",
    "        else show_max_iters + 1,\n",
    "    ckpt_iter_freq\n",
    ")\n",
    "\n",
    "### plot alpha estimates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for baseline_name in baselines:\n",
    "    if plot_baseline_update_alpha:\n",
    "        ### alpha estimates for param updates\n",
    "        updates_alpha = np.array(baselines[baseline_name][\"run_history\"][phase][\"updates_alpha\"])[:, :len(x_ticks)]\n",
    "        sns.lineplot(x=x_ticks, y=updates_alpha.mean(0), label=f\"{baseline_name} - updates\", linestyle=\"--\", ax=ax)\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=updates_alpha.mean(0) - updates_alpha.std(0),\n",
    "            y2=updates_alpha.mean(0) + updates_alpha.std(0),\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "    if plot_baseline_grad_alpha:\n",
    "        ### alpha estimates for gradients\n",
    "        grads_alpha = np.array(baselines[baseline_name][\"run_history\"][phase][\"grads_alpha\"])[:, :len(x_ticks)] # (n_tests, n_iters)\n",
    "        sns.lineplot(x=x_ticks, y=grads_alpha.mean(0), label=f\"{baseline_name} - gradients\", linestyle=\"--\", ax=ax)\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=grads_alpha.mean(0) - grads_alpha.std(0),\n",
    "            y2=grads_alpha.mean(0) + grads_alpha.std(0),\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "for opter_name in l2os:\n",
    "    if plot_l2o_update_alpha:\n",
    "        ### alpha estimates for param updates\n",
    "        updates_alpha = np.array(l2os[opter_name][\"run_history\"][phase][\"updates_alpha\"])[:, :len(x_ticks)] # (n_tests, n_iters)\n",
    "        sns.lineplot(x=x_ticks, y=updates_alpha.mean(0), label=f\"{opter_name} - updates\", ax=ax)\n",
    "        # sns.lineplot(x=x_ticks, y=updates_alpha.mean(0), label=f\"L2O - updates\", ax=ax)\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=updates_alpha.mean(0) - updates_alpha.std(0),\n",
    "            y2=updates_alpha.mean(0) + updates_alpha.std(0),\n",
    "            alpha=0.2\n",
    "        )\n",
    "    \n",
    "    if plot_l2o_grad_alpha:\n",
    "        ### alpha estimates for gradients\n",
    "        grads_alpha = np.array(l2os[opter_name][\"run_history\"][phase][\"grads_alpha\"])[:, :len(x_ticks)] # (n_tests, n_iters)\n",
    "        sns.lineplot(x=x_ticks, y=grads_alpha.mean(0), label=f\"{opter_name} - gradients\", ax=ax)\n",
    "        # sns.lineplot(x=x_ticks, y=grads_alpha.mean(0), label=f\"L2O - gradients\", ax=ax)\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=grads_alpha.mean(0) - grads_alpha.std(0),\n",
    "            y2=grads_alpha.mean(0) + grads_alpha.std(0),\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "# ax.set_title(f\"Alpha estimates ({phase})\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Alpha estimate\")\n",
    "\n",
    "\n",
    "### manual\n",
    "ax.set_ylim(0.4, 1.06)\n",
    "# yticks = ax.get_yticks()\n",
    "# ax.set_yticks(yticks[::3])\n",
    "# ax.set_yticks([0.5, 0.65, 0.8, 1])\n",
    "# ax.set_xticks(np.arange(0, 1001 if show_max_iters is None else show_max_iters + 1, 500))\n",
    "\n",
    "### automatic (3 x-ticks)\n",
    "x_ticks = ax.get_xticks()\n",
    "ax.set_xticks([0, show_max_iters // 2, show_max_iters])\n",
    "\n",
    "# legend\n",
    "ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.25), ncol=3)\n",
    "legend = ax.get_legend()\n",
    "for legend_handle in legend.legendHandles:\n",
    "    legend_handle.set_linewidth(3.0)\n",
    "for legend_text in legend.get_texts():\n",
    "    legend_text.set_fontsize(9)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save fig\n",
    "if save_fig_to_path:\n",
    "    fig.savefig(save_fig_to_path, bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating results from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, input_dim=28*28 , width=50, depth=3, num_classes=10):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        layers = self.get_layers()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.width, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            *layers,\n",
    "            nn.Linear(self.width, self.num_classes, bias=False),\n",
    "        )\n",
    "\n",
    "    def get_layers(self):\n",
    "        layers = []\n",
    "        for i in range(self.depth - 2):\n",
    "            layers.append(nn.Linear(self.width, self.width, bias=False))\n",
    "            layers.append(nn.ReLU())\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), self.input_dim)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    from torchvision import datasets, transforms\n",
    "    data_class = 'MNIST'\n",
    "    num_classes = 10\n",
    "    stats = {\n",
    "        'mean': [0.1307], \n",
    "        'std': [0.3081]\n",
    "        }\n",
    "\n",
    "    # input transformation w/o preprocessing for now\n",
    "\n",
    "    trans = [\n",
    "        transforms.ToTensor(),\n",
    "        lambda t: t.type(torch.get_default_dtype()),\n",
    "        transforms.Normalize(**stats)\n",
    "        ]\n",
    "        \n",
    "    # get tr and te data with the same normalization\n",
    "    tr_data = getattr(datasets, data_class)(\n",
    "        root=os.environ['DATA_PATH'], \n",
    "        train=True, \n",
    "        download=False,\n",
    "        transform=transforms.Compose(trans)\n",
    "        )\n",
    "\n",
    "    te_data = getattr(datasets, data_class)(\n",
    "        root=os.environ['DATA_PATH'], \n",
    "        train=False, \n",
    "        download=False,\n",
    "        transform=transforms.Compose(trans)\n",
    "        )\n",
    "\n",
    "    # get tr_loader for train/eval and te_loader for eval\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=tr_data,\n",
    "        batch_size=100, \n",
    "        shuffle=False,\n",
    "        )\n",
    "\n",
    "    train_loader_eval = torch.utils.data.DataLoader(\n",
    "        dataset=tr_data,\n",
    "        batch_size=100, \n",
    "        shuffle=False,\n",
    "        )\n",
    "\n",
    "    test_loader_eval = torch.utils.data.DataLoader(\n",
    "        dataset=te_data,\n",
    "        batch_size=100, \n",
    "        shuffle=False,\n",
    "        )\n",
    "\n",
    "    return train_loader, test_loader_eval, train_loader_eval, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run separately\n",
    "run_history = {k: [] for k in (\"train_loss\", \"train_acc\", \"noise_norm\", \"alpha\")}\n",
    "# train_data = MNIST(training=True, batch_size=100)\n",
    "# test_data = MNIST(training=False, batch_size=100)\n",
    "train_loader, test_loader_eval, train_loader_eval, num_classes = get_data()\n",
    "\n",
    "# optee = MNISTRelu().cuda()\n",
    "optee = FullyConnected(width=20, depth=1).cuda()\n",
    "optee_optim = optim.SGD(optee.parameters(), lr=0.1)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def eval():\n",
    "    optee.eval()\n",
    "    grads = []\n",
    "    losses = []\n",
    "    accs = []\n",
    "    n_minibatches = 0\n",
    "    # for data in (train_data,):\n",
    "    for i, (x, y) in enumerate(train_loader_eval):\n",
    "        n_minibatches += 1\n",
    "        \n",
    "        x, y = x.view(-1, 784).cuda(), y.cuda()\n",
    "        # loss, acc = optee(x, out=y, return_acc=True)\n",
    "        y_hat = optee(x)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "\n",
    "        ### collect gradients\n",
    "        # grads.append(torch.cat([p.grad.detach().view(-1) for n, p in optee.all_named_parameters() if p.requires_grad]).cpu())\n",
    "        grads.append(torch.cat([p.grad.detach().view(-1) for n, p in optee.named_parameters() if p.requires_grad]).cpu())\n",
    "\n",
    "        ### track history\n",
    "        losses.append(loss.item())\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        accs.append(acc.item())\n",
    "        \n",
    "        optee_optim.zero_grad()\n",
    "\n",
    "    optee_total_params = len(grads[0])\n",
    "    grads = torch.stack(grads, dim=0) # (n_minibatches, optee_total_params)\n",
    "    mean_grad = grads.mean(dim=0) # (optee_total_params,)\n",
    "    noise_norm = (grads - mean_grad).norm(dim=1) # (n_minibatches,)\n",
    "\n",
    "    ### get the tail index alpha\n",
    "    N = optee_total_params * n_minibatches\n",
    "    for i in range(1, 1 + int(np.sqrt(N))):\n",
    "        if N % i == 0:\n",
    "            m = i\n",
    "    alpha = alpha_estimator(m, (grads - mean_grad).view(-1, 1))\n",
    "\n",
    "    ### collect history\n",
    "    run_history[\"train_loss\"].append(np.mean(losses))\n",
    "    run_history[\"train_acc\"].append(np.mean(accs))\n",
    "    run_history[\"noise_norm\"].append(noise_norm)\n",
    "    run_history[\"alpha\"].append(alpha.item())\n",
    "\n",
    "def cyclic_loader(loader):\n",
    "    while True:\n",
    "        for x in loader:\n",
    "            yield x\n",
    "\n",
    "cyclic_train_loader = cyclic_loader(train_loader)\n",
    "\n",
    "for i, (x, y) in enumerate(cyclic_train_loader):\n",
    "    if i % 100 == 0:\n",
    "        eval()\n",
    "        print(i, run_history[\"train_loss\"][-1], run_history[\"train_acc\"][-1], run_history[\"alpha\"][-1])\n",
    "    \n",
    "    optee.train()\n",
    "    x, y = x.view(-1, 784).cuda(), y.cuda()\n",
    "    # loss, acc = optee(inp=x, out=y, return_acc=True)\n",
    "    y_hat = optee(x)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    loss.backward()\n",
    "    optee_optim.step()\n",
    "    optee_optim.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(run_history[\"train_loss\"])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(run_history[\"train_acc\"])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(run_history[\"alpha\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavy-tail distribution of parameters\n",
    "- From the [paper](http://proceedings.mlr.press/v139/gurbuzbalaban21a/gurbuzbalaban21a.pdf) *The heavy-tail phenomenon in SGD by Gurbuzbalaban, M., Simsekli, U., & Zhu, L.. In International Conference on Machine Learning (ICML), 2021.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling_ms(N):\n",
    "    ms = [2]\n",
    "    select_ms_closest_to_curr_idx = 0\n",
    "    select_ms_closest_to = [5, 10, 20, 50, 100, 500, 1000]\n",
    "    for i in range(3, 1 + int(np.sqrt(N))):\n",
    "        if N % i == 0 \\\n",
    "            and i > ms[-1] \\\n",
    "            and ms[-1] < select_ms_closest_to[select_ms_closest_to_curr_idx] \\\n",
    "            and i >= select_ms_closest_to[select_ms_closest_to_curr_idx]:\n",
    "            ms.append(i)\n",
    "            select_ms_closest_to_curr_idx += 1\n",
    "            if select_ms_closest_to_curr_idx >= len(select_ms_closest_to):\n",
    "                break\n",
    "    return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_parameter_heavy_tail_alpha_estimates(\n",
    "    config,\n",
    "    ckpts_dir,\n",
    "    iters_window,\n",
    "    ckpt_iter_freq,\n",
    "    iter_print_freq=50,\n",
    "):\n",
    "    ### collect\n",
    "    parameter_history_metrics = {k: [] for k in (\"loss\", \"acc\", \"params_alpha_total\")}\n",
    "    for k in (\"params_noise_norm_per_iter\", \"params_alpha_per_iter\"):\n",
    "        parameter_history_metrics[k] = dict()\n",
    "    \n",
    "    _tmp_optee = config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"])\n",
    "    params = {iter_i: {n: [] for n, p in _tmp_optee.named_parameters() if p.requires_grad}\n",
    "        for iter_i in range(iters_window[0], iters_window[1], ckpt_iter_freq)}\n",
    "    del _tmp_optee\n",
    "\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        print(f\"  Test run {test_run_i}...\")\n",
    "        for iter_i in range(\n",
    "            iters_window[0],\n",
    "            iters_window[1],\n",
    "            ckpt_iter_freq\n",
    "        ):\n",
    "            if iter_i % iter_print_freq == 0:\n",
    "                print(f\"  [COLLECTING-PARAMS-{test_run_i}][{iter_i}/{iters_window[1]}]\")\n",
    "\n",
    "            ### load checkpoint\n",
    "            ckpt_path = os.path.join(ckpts_dir, f\"run{test_run_i}_{iter_i}.pt\")\n",
    "            ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "            ### load optee and collect parameters\n",
    "            for param_name in ckpt[\"optimizee\"]:\n",
    "                if \"mean\" not in param_name and \"var\" not in param_name: # skip batch norm params\n",
    "                    params[iter_i][param_name].append(ckpt[\"optimizee\"][param_name].detach().view(-1).cpu())\n",
    "    \n",
    "    ### post-process - calculate noise norm and alpha estimates per iteration\n",
    "    print(\"Post-processing (per iteration)...\")\n",
    "    for iter_i in range(\n",
    "        iters_window[0],\n",
    "        iters_window[1],\n",
    "        ckpt_iter_freq\n",
    "    ):\n",
    "        if iter_i % iter_print_freq == 0:\n",
    "            print(f\"  [POSTPROCESSING-PER-ITER][{iter_i}/{iters_window[1]}]\")\n",
    "        for n in params[iter_i].keys():\n",
    "            # get alpha estimate\n",
    "            params[iter_i][n] = torch.stack(params[iter_i][n]) # (eval_n_tests, param_dim)\n",
    "            mean_param = params[iter_i][n].mean(dim=0) # (eval_n_tests, param_dim) -> (param_dim,)\n",
    "            N = params[iter_i][n].shape[0] * params[iter_i][n].shape[1]\n",
    "            for i in range(1, 1 + int(np.sqrt(N))):\n",
    "                if N % i == 0:\n",
    "                    m = i\n",
    "            alpha = alpha_estimator(m, (params[iter_i][n] - mean_param).view(-1, 1)) # (n_tests,)\n",
    "\n",
    "            noise_norm = (params[iter_i][n] - mean_param).norm(dim=1) # (n_tests,)\n",
    "            parameter_history_metrics[\"params_noise_norm_per_iter\"][iter_i] = torch.mean(noise_norm).item()\n",
    "            parameter_history_metrics[\"params_alpha_per_iter\"][iter_i] = alpha.item()\n",
    "\n",
    "    print(\"Post-processing (total)...\")\n",
    "    for n in params[iters_window[0]].keys():\n",
    "        tmp_params = torch.cat([params[iter_i][n] for iter_i in range(\n",
    "            iters_window[0],\n",
    "            iters_window[1],\n",
    "            ckpt_iter_freq\n",
    "        )], dim=0) # (eval_n_tests * n_iters, param_dim)\n",
    "        tmp_params = tmp_params.view(-1, 1)\n",
    "        tmp_params = tmp_params - torch.mean(tmp_params) # center\n",
    "        alpha = np.median([alpha_estimator(m, tmp_params) for m in get_sampling_ms(tmp_params.shape[0])])\n",
    "        parameter_history_metrics[\"params_alpha_total\"] = alpha\n",
    "    \n",
    "    return parameter_history_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "load_existing = True\n",
    "iters_window = (1, 1000)\n",
    "ckpt_iter_freq = 10 # config[\"meta_testing\"][\"ckpt_iter_freq\"]\n",
    "iter_print_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect for baselines\n",
    "for baseline_name in baselines:\n",
    "    print(f\"Collecting for {baseline_name}...\")\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "\n",
    "    ckpts_dir = os.path.join(baselines[baseline_name][\"baseline_dir\"], \"ckpt\")\n",
    "    save_parameter_history_metrics_path = os.path.join(\n",
    "        baselines[baseline_name][\"baseline_dir\"],\n",
    "        f\"parameter_heavy_tail_alpha_estimates\" +\n",
    "            f\"_{iters_window[0]}-{iters_window[1]}\" +\n",
    "            f\"_{config['meta_testing']['optee_cls'].__name__}_{dict_to_str(config['meta_testing']['optee_config'])}\" +\n",
    "            f\"_{config['meta_testing']['data_cls'].__name__}_{dict_to_str(config['meta_testing']['data_config'])}\" +\n",
    "            f\".pt\"\n",
    "    )\n",
    "\n",
    "    if load_existing and os.path.exists(save_parameter_history_metrics_path):\n",
    "        print(f\"  Loading existing run history from {save_parameter_history_metrics_path}\")\n",
    "        baselines[baseline_name][\"parameter_history_metrics\"] = torch.load(save_parameter_history_metrics_path)\n",
    "    else:\n",
    "        print(f\"  Existing run history at path {save_parameter_history_metrics_path} not found, collecting it...\")\n",
    "        baselines[baseline_name][\"parameter_history_metrics\"] = collect_parameter_heavy_tail_alpha_estimates(\n",
    "            config=config,\n",
    "            ckpts_dir=ckpts_dir,\n",
    "            iters_window=iters_window,\n",
    "            ckpt_iter_freq=ckpt_iter_freq,\n",
    "            iter_print_freq=iter_print_freq,\n",
    "        )\n",
    "        torch.save(baselines[baseline_name][\"parameter_history_metrics\"], save_parameter_history_metrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect for l2os\n",
    "for opter_name in l2os:\n",
    "    print(f\"Collecting for {opter_name}...\")\n",
    "    config = l2os[opter_name][\"config\"]\n",
    "\n",
    "    # ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"])\n",
    "    ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"] + \"_long\")\n",
    "    save_parameter_history_metrics_path = os.path.join(\n",
    "        os.environ[\"CKPT_PATH\"],\n",
    "        config[\"ckpt_base_dir\"],\n",
    "        f\"parameter_heavy_tail_alpha_estimates\" +\n",
    "            f\"_{iters_window[0]}-{iters_window[1]}\" +\n",
    "            f\"_{config['meta_testing']['optee_cls'].__name__}_{dict_to_str(config['meta_testing']['optee_config'])}\" +\n",
    "            f\"_{config['meta_testing']['data_cls'].__name__}_{dict_to_str(config['meta_testing']['data_config'])}\" +\n",
    "            f\".pt\"\n",
    "    )\n",
    "\n",
    "    if load_existing and os.path.exists(save_parameter_history_metrics_path):\n",
    "        print(f\"  Loading existing run history from {save_parameter_history_metrics_path}\")\n",
    "        l2os[opter_name][\"parameter_history_metrics\"] = torch.load(save_parameter_history_metrics_path)\n",
    "    else:\n",
    "        print(f\"  Existing run history not found at path {save_parameter_history_metrics_path}, collecting it...\")\n",
    "        l2os[opter_name][\"parameter_history_metrics\"] = collect_parameter_heavy_tail_alpha_estimates(\n",
    "            config=config,\n",
    "            ckpts_dir=ckpts_dir,\n",
    "            iters_window=iters_window,\n",
    "            ckpt_iter_freq=ckpt_iter_freq,\n",
    "            iter_print_freq=iter_print_freq,\n",
    "        )\n",
    "        torch.save(l2os[opter_name][\"parameter_history_metrics\"], save_parameter_history_metrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l2os[\"baseline_no_reg\"][\"parameter_history_metrics\"][\"params_alpha_total\"])\n",
    "plt.plot(list(l2os[\"baseline_no_reg\"][\"parameter_history_metrics\"][\"params_alpha_per_iter\"].values())[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baselines[\"SGD\"][\"parameter_history_metrics\"][\"params_alpha_total\"])\n",
    "plt.plot(list(baselines[\"SGD\"][\"parameter_history_metrics\"][\"params_alpha_per_iter\"].values())[0:])\n",
    "plt.ylim(0, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the implementation from the authors\n",
    "- [GitHub repository](https://github.com/umutsimsekli/sgd_ht/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = []\n",
    "\n",
    "### collect for baselines\n",
    "for baseline_name in baselines:\n",
    "    print(f\"Collecting for {baseline_name}...\")\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "\n",
    "    ckpts_dir = os.path.join(baselines[baseline_name][\"baseline_dir\"], \"ckpt\")\n",
    "\n",
    "    for iter_i in range(\n",
    "        1500,\n",
    "        2000,\n",
    "        5\n",
    "    ):\n",
    "        if iter_i % 100 == 0:\n",
    "            print(f\"  iter_i={iter_i}\")\n",
    "        ### load checkpoint\n",
    "        ckpt_path = os.path.join(ckpts_dir, f\"run0_{iter_i}.pt\")\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        optee = config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"])\n",
    "        optee.load_state_dict(ckpt[\"optimizee\"])\n",
    "        nets.append(optee.cpu())\n",
    "    \n",
    "### collect for l2os\n",
    "for opter_name in l2os:\n",
    "    print(f\"Collecting for {opter_name}...\")\n",
    "    config = l2os[opter_name][\"config\"]\n",
    "\n",
    "    ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"])\n",
    "\n",
    "    for iter_i in range(\n",
    "        1500,\n",
    "        2000,\n",
    "        5\n",
    "    ):\n",
    "        if iter_i % 100 == 0:\n",
    "            print(f\"  iter_i={iter_i}\")\n",
    "        ### load checkpoint\n",
    "        ckpt_path = os.path.join(ckpts_dir, f\"run0_{iter_i}.pt\")\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        optee = config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"])\n",
    "        optee.load_state_dict(ckpt[\"optimizee\"])\n",
    "        nets.append(optee.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 2\n",
    "num_nets = len(nets)\n",
    "alphas_mc = np.zeros(depth) - 1\n",
    "\n",
    "# Corollary 2.4 in Mohammadi 2014 - for 1d\n",
    "def alpha_estimator_one(m, X):\n",
    "    N = len(X)\n",
    "    n = int(N/m) # must be an integer\n",
    "    \n",
    "    X = X[0:n*m]\n",
    "    \n",
    "    Y = np.sum(X.reshape(n, m),1)\n",
    "    eps = np.spacing(1)\n",
    "\n",
    "    Y_log_norm =  np.log(np.abs(Y) + eps).mean()\n",
    "    X_log_norm =  np.log(np.abs(X) + eps).mean()\n",
    "    diff = (Y_log_norm - X_log_norm) / math.log(m)\n",
    "    return 1 / diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect\n",
    "weights = []\n",
    "for i in range(depth):\n",
    "    weights.append([])\n",
    "\n",
    "# record the layers in different arrays\n",
    "for i in range(num_nets):\n",
    "    tmp_net = nets[i]\n",
    "    ix = 0\n",
    "    for n, p in tmp_net.all_named_parameters():\n",
    "        if \"bias\" in n or \"batch_norm\" in n:\n",
    "            continue\n",
    "        layer = p.detach().numpy()\n",
    "        layer = layer.reshape(-1,1)\n",
    "        weights[ix].append(layer)\n",
    "        ix += 1\n",
    "\n",
    "for i in range(depth):\n",
    "    weights[i] = np.concatenate(weights[i], axis = 1)  \n",
    "\n",
    "for i in range(depth):\n",
    "    tmp_weights = np.mean(weights[i], axis=1)\n",
    "    tmp_weights = tmp_weights.reshape(-1,1)\n",
    "    tmp_weights = tmp_weights - np.mean(tmp_weights)\n",
    "    # tmp_alphas = [alpha_estimator_one(mm, tmp_weights) for mm in (2, 5, 10, 20, 50, 100, 500, 1000)]\n",
    "    tmp_alphas = [alpha_estimator_one(mm, tmp_weights) for mm in (2, 5, 10, 20)]\n",
    "    # tmp_alphas = [alpha_estimator(mm, torch.tensor(tmp_weights).view(-1, 1)) for mm in (2, 5, 10, 20)]\n",
    "    alphas_mc[i] = np.median(tmp_alphas)\n",
    "\n",
    "print(alphas_mc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance of gradients/updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_iter_freq = 10\n",
    "ckpt_iter_freq = 20\n",
    "max_iters = 1000\n",
    "max_test_runs = 3\n",
    "trace_estimate_data_frac = 0.2 # what fraction of the whole dataset to use for trace estimation\n",
    "eigen_estimate_data_frac = 0.2 # what fraction of mini batches of data_mini_batched to use for max eigenvalue estimation\n",
    "\n",
    "data_samples = MNIST(training=True, batch_size=1, preload=True)\n",
    "data_mini_batched = MNIST(training=True, batch_size=128, preload=True)\n",
    "data_full = MNIST(training=True, batch_size=len(data_samples.loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(A, max_iters):\n",
    "    x = torch.randn(A.shape[0], 1).to(A.device)\n",
    "    for _ in range(max_iters):\n",
    "        x = A @ x\n",
    "        x /= torch.norm(x)\n",
    "    return x.T @ A @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cov_trace(optee, data_full, data_samples, trace_estimate_data_frac, opter=None, hidden_states=None, cell_states=None, optee_updates_lr=None):\n",
    "    ### init trace of cov(grad) and cov(updates)\n",
    "    tr_grads, tr_updates = 0, 0\n",
    "    # optee_n_params = sum(\n",
    "    #     [int(np.prod(p.size())) for _, p in optee.all_named_parameters() if p.requires_grad]\n",
    "    # )\n",
    "\n",
    "    ### get true gradient g\n",
    "    loss = optee(data=data_full)\n",
    "    loss.backward()\n",
    "    g = []\n",
    "    update_full = []\n",
    "    offset = 0\n",
    "\n",
    "    ### get updates for baseline opter\n",
    "    if opter is not None and not isinstance(opter, Optimizer):\n",
    "        update_full = torch.cat([\n",
    "            p.detach().view(-1, 1).to(\"cpu\") for p in get_baseline_opter_param_updates(optee, opter, verbose=False).values()\n",
    "        ], dim=0)\n",
    "    \n",
    "    for name, p in optee.all_named_parameters():\n",
    "        if not p.requires_grad or p.grad is None:\n",
    "            continue\n",
    "        param_grad = p.grad.view(-1, 1).detach()\n",
    "        g.append(param_grad)\n",
    "\n",
    "        ### get updates for l2o\n",
    "        if opter is not None and isinstance(opter, Optimizer):\n",
    "            curr_sz = int(np.prod(p.size()))\n",
    "            with torch.no_grad():\n",
    "                param_update, _, _ = opter(\n",
    "                    optee_grads=param_grad,\n",
    "                    hidden=[h[offset : offset + curr_sz] for h in hidden_states],\n",
    "                    cell=[c[offset : offset + curr_sz] for c in cell_states],\n",
    "                    additional_inp=None,\n",
    "                )\n",
    "                update_full.append(optee_updates_lr * param_update.view(-1, 1).detach())\n",
    "            offset += curr_sz\n",
    "        \n",
    "        p.grad = None # clear grad\n",
    "    \n",
    "    g = torch.cat(g, dim=0).to(\"cpu\")\n",
    "    if opter is not None and isinstance(opter, Optimizer):\n",
    "        update_full = torch.cat(update_full, dim=0).to(\"cpu\")\n",
    "\n",
    "    ### get gradients for each sample\n",
    "    num_samples = int(len(data_samples.batches) * trace_estimate_data_frac)\n",
    "    for inp, out in data_samples.batches[:num_samples]:\n",
    "        inp = w(Variable(inp.view(inp.size()[0], 28 * 28)))\n",
    "        out = w(Variable(out))\n",
    "        loss = optee(inp=inp, out=out)\n",
    "        loss.backward()\n",
    "        g_i = []\n",
    "        update_i = []\n",
    "        offset = 0\n",
    "\n",
    "        ### get updates for baseline opter\n",
    "        if opter is not None and not isinstance(opter, Optimizer):\n",
    "            update_i = torch.cat([\n",
    "                p.view(-1, 1).detach().to(\"cpu\") for p in get_baseline_opter_param_updates(optee, opter, verbose=False).values()\n",
    "            ], dim=0)\n",
    "\n",
    "        for name, p in optee.all_named_parameters():\n",
    "            if not p.requires_grad or p.grad is None:\n",
    "                continue\n",
    "            param_grad = p.grad.view(-1, 1).detach()\n",
    "            g_i.append(param_grad)\n",
    "\n",
    "            ### get updates for l2o\n",
    "            if opter is not None and isinstance(opter, Optimizer):\n",
    "                curr_sz = int(np.prod(p.size()))\n",
    "                with torch.no_grad():\n",
    "                    param_update, _, _ = opter(\n",
    "                        optee_grads=param_grad,\n",
    "                        hidden=[h[offset : offset + curr_sz] for h in hidden_states],\n",
    "                        cell=[c[offset : offset + curr_sz] for c in cell_states],\n",
    "                        additional_inp=None,\n",
    "                    )\n",
    "                    update_i.append(optee_updates_lr * param_update.view(-1, 1).detach())\n",
    "                offset += curr_sz\n",
    "\n",
    "            p.grad = None # clear grad\n",
    "\n",
    "        g_i = torch.cat(g_i, dim=0).to(\"cpu\")\n",
    "        tr_grads += torch.norm(g - g_i) ** 2\n",
    "        if opter is not None and isinstance(opter, Optimizer):\n",
    "            update_i = torch.cat(update_i, dim=0).to(\"cpu\")\n",
    "        if opter is not None:\n",
    "            tr_updates += torch.norm(update_full - update_i) ** 2\n",
    "\n",
    "    ### record trace\n",
    "    tr_grads /= num_samples\n",
    "    if opter is not None:\n",
    "        tr_updates /= num_samples\n",
    "        return tr_grads.item(), tr_updates.item()\n",
    "    return tr_grads.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cov_max_eigenval(optee, data_mini_batched, eigen_estimate_data_frac, opter=None, hidden_states=None, cell_states=None, optee_updates_lr=None):\n",
    "    ### get max eigenvalue\n",
    "    L = int(len(data_mini_batched.batches) * eigen_estimate_data_frac)\n",
    "    optee_n_params = sum(\n",
    "        [int(np.prod(p.size())) for _, p in optee.all_named_parameters() if p.requires_grad]\n",
    "    )\n",
    "\n",
    "    grads = []\n",
    "    g = torch.zeros(optee_n_params) # estimate of the true gradient\n",
    "    updates_full = []\n",
    "    update_full = torch.zeros(optee_n_params) # estimate of the true update\n",
    "    for (inp, out) in data_mini_batched.batches[:L]:\n",
    "        inp = w(Variable(inp.view(inp.size()[0], 28 * 28)))\n",
    "        out = w(Variable(out))\n",
    "        loss = optee(inp=inp, out=out)\n",
    "        loss.backward()\n",
    "        g_i = []\n",
    "        update_i = []\n",
    "        offset = 0\n",
    "\n",
    "        ### get updates for baseline opter\n",
    "        if opter is not None and not isinstance(opter, Optimizer):\n",
    "            update_i = torch.cat([\n",
    "                p.view(-1, 1).detach().to(\"cpu\") for p in get_baseline_opter_param_updates(optee, opter, verbose=False).values()\n",
    "            ], dim=0)\n",
    "\n",
    "        for name, p in optee.all_named_parameters():\n",
    "            if not p.requires_grad or p.grad is None:\n",
    "                continue\n",
    "            param_grad = p.grad.view(-1, 1).detach()\n",
    "            g_i.append(param_grad)\n",
    "\n",
    "            ### get updates for l2o\n",
    "            if opter is not None and isinstance(opter, Optimizer):\n",
    "                curr_sz = int(np.prod(p.size()))\n",
    "                with torch.no_grad():\n",
    "                    param_update, _, _ = opter(\n",
    "                        optee_grads=param_grad,\n",
    "                        hidden=[h[offset : offset + curr_sz] for h in hidden_states],\n",
    "                        cell=[c[offset : offset + curr_sz] for c in cell_states],\n",
    "                        additional_inp=None,\n",
    "                    )\n",
    "                    update_i.append(optee_updates_lr * param_update.view(-1, 1).detach())\n",
    "                offset += curr_sz\n",
    "\n",
    "            p.grad = None # clear grad\n",
    "        g_i = torch.cat(g_i, dim=0).to(\"cpu\")\n",
    "        grads.append(g_i)\n",
    "        g += g_i.view(-1)\n",
    "        \n",
    "        if opter is not None:\n",
    "            if isinstance(opter, Optimizer):\n",
    "                update_i = torch.cat(update_i, dim=0).to(\"cpu\")\n",
    "            updates_full.append(update_i)\n",
    "            update_full += update_i.view(-1)\n",
    "\n",
    "    ### get max eigenvalue of the cov(grads)\n",
    "    g /= L\n",
    "    grads = torch.cat(grads, dim=1).T # (L, n_params)\n",
    "    grads = grads - g # (L, n_params)\n",
    "    gram_mat = (grads @ grads.T) / L # (L, L)\n",
    "    max_eigen_grad = power_method(gram_mat, 100)\n",
    "\n",
    "    ### get max eigenvalue of the cov(updates)\n",
    "    if opter is not None:\n",
    "        update_full /= L\n",
    "        updates_full = torch.cat(updates_full, dim=1).T # (L, n_params)\n",
    "        updates_full = updates_full - update_full # (L, n_params)\n",
    "        gram_mat = (updates_full @ updates_full.T) / L # (L, L)\n",
    "        max_eigen_update = power_method(gram_mat, 100)\n",
    "        return max_eigen_grad.item(), max_eigen_update.item()\n",
    "\n",
    "    return max_eigen_grad.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect for L2Os\n",
    "for opter_name in l2os:\n",
    "    print(f\"Collecting for {opter_name}...\")\n",
    "    config = l2os[opter_name][\"config\"]\n",
    "    ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"])\n",
    "    save_path = os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_base_dir\"], \"cov_analysis.pt\")\n",
    "\n",
    "    l2os[opter_name][\"cov_grad_tr\"] = []\n",
    "    l2os[opter_name][\"cov_grad_max_eigen\"] = []\n",
    "    l2os[opter_name][\"cov_update_tr\"] = []\n",
    "    l2os[opter_name][\"cov_update_max_eigen\"] = []\n",
    "    \n",
    "    ### load already collected\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"  Loading previously collected results...\")\n",
    "        cov_analysis = torch.load(save_path)\n",
    "        for k in [\"cov_grad_tr\", \"cov_grad_max_eigen\", \"cov_update_tr\", \"cov_update_max_eigen\"]:\n",
    "            l2os[opter_name][k] = cov_analysis[k]\n",
    "        continue\n",
    "\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        if max_test_runs and test_run_i >= max_test_runs:\n",
    "            break\n",
    "        print(f\"  Test run {test_run_i}...\")\n",
    "\n",
    "        l2os[opter_name][\"cov_grad_tr\"].append([])\n",
    "        l2os[opter_name][\"cov_grad_max_eigen\"].append([])\n",
    "        l2os[opter_name][\"cov_update_tr\"].append([])\n",
    "        l2os[opter_name][\"cov_update_max_eigen\"].append([])\n",
    "\n",
    "        for iter_i in [1, *range(ckpt_iter_freq, max_iters + 1, ckpt_iter_freq)]:\n",
    "            if iter_i % 100 == 0:\n",
    "                print(f\"    iter_i={iter_i}\")\n",
    "\n",
    "            ### load ckpt\n",
    "            ckpt_path = os.path.join(ckpts_dir, f\"run{test_run_i}_{iter_i}.pt\")\n",
    "            ckpt = torch.load(ckpt_path)\n",
    "\n",
    "            ### init optee\n",
    "            optee_ckpt = ckpt[\"optimizee\"]\n",
    "            optee = w(config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"]))\n",
    "            optee.load_state_dict(optee_ckpt)\n",
    "            optee.eval()\n",
    "            # if hasattr(optee, \"batch_norm\"):\n",
    "            #     optee.batch_norm.eval()\n",
    "\n",
    "            ### init opter\n",
    "            opter_ckpt = ckpt[\"optimizer\"]\n",
    "            opter = w(config[\"opter_cls\"](**config[\"opter_config\"]))\n",
    "            opter.load_state_dict(opter_ckpt)\n",
    "            \n",
    "            ### get trace of cov(grad) and cov(updates)\n",
    "            tr_grads, tr_updates = get_cov_trace(\n",
    "                opter=opter,\n",
    "                hidden_states=ckpt[\"hidden_states\"],\n",
    "                cell_states=ckpt[\"cell_states\"],\n",
    "                optee=optee,\n",
    "                data_full=data_full,\n",
    "                data_samples=data_samples,\n",
    "                trace_estimate_data_frac=trace_estimate_data_frac,\n",
    "                optee_updates_lr=config[\"meta_testing\"][\"optee_updates_lr\"],\n",
    "            )\n",
    "            l2os[opter_name][\"cov_grad_tr\"][-1].append(tr_grads)\n",
    "            l2os[opter_name][\"cov_update_tr\"][-1].append(tr_updates)\n",
    "\n",
    "            ### get max eigenvalue of cov(grad) and cov(updates)\n",
    "            max_eigenval_grads, max_eigenval_updates = get_cov_max_eigenval(\n",
    "                opter=opter,\n",
    "                hidden_states=ckpt[\"hidden_states\"],\n",
    "                cell_states=ckpt[\"cell_states\"],\n",
    "                optee=optee,\n",
    "                data_mini_batched=data_mini_batched,\n",
    "                eigen_estimate_data_frac=eigen_estimate_data_frac,\n",
    "                optee_updates_lr=config[\"meta_testing\"][\"optee_updates_lr\"],\n",
    "            )\n",
    "            l2os[opter_name][\"cov_grad_max_eigen\"][-1].append(max_eigenval_grads)\n",
    "            l2os[opter_name][\"cov_update_max_eigen\"][-1].append(max_eigenval_updates)\n",
    "        \n",
    "    ### to np arrays\n",
    "    l2os[opter_name][\"cov_grad_tr\"] = np.array(l2os[opter_name][\"cov_grad_tr\"])\n",
    "    l2os[opter_name][\"cov_grad_max_eigen\"] = np.array(l2os[opter_name][\"cov_grad_max_eigen\"])\n",
    "    l2os[opter_name][\"cov_update_tr\"] = np.array(l2os[opter_name][\"cov_update_tr\"])\n",
    "    l2os[opter_name][\"cov_update_max_eigen\"] = np.array(l2os[opter_name][\"cov_update_max_eigen\"])\n",
    "\n",
    "    ### save\n",
    "    torch.save({\n",
    "        \"cov_grad_tr\": l2os[opter_name][\"cov_grad_tr\"],\n",
    "        \"cov_grad_max_eigen\": l2os[opter_name][\"cov_grad_max_eigen\"],\n",
    "        \"cov_update_tr\": l2os[opter_name][\"cov_update_tr\"],\n",
    "        \"cov_update_max_eigen\": l2os[opter_name][\"cov_update_max_eigen\"],\n",
    "    }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect for baselines\n",
    "for baseline_name in baselines:\n",
    "    print(f\"Collecting for {baseline_name}...\")\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "    ckpts_dir = os.path.join(baselines[baseline_name][\"baseline_dir\"], \"ckpt\")\n",
    "    save_path = os.path.join(baselines[baseline_name][\"baseline_dir\"], \"cov_analysis.pt\")\n",
    "\n",
    "    baselines[baseline_name][\"cov_grad_tr\"] = []\n",
    "    baselines[baseline_name][\"cov_grad_max_eigen\"] = []\n",
    "    baselines[baseline_name][\"cov_update_tr\"] = []\n",
    "    baselines[baseline_name][\"cov_update_max_eigen\"] = []\n",
    "    \n",
    "    ### load already collected\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"  Loading previously collected results...\")\n",
    "        cov_analysis = torch.load(save_path)\n",
    "        for k in [\"cov_grad_tr\", \"cov_grad_max_eigen\", \"cov_update_tr\", \"cov_update_max_eigen\"]:\n",
    "            baselines[baseline_name][k] = cov_analysis[k]\n",
    "        continue\n",
    "\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        if max_test_runs and test_run_i >= max_test_runs:\n",
    "            break\n",
    "        print(f\"  Test run {test_run_i}...\")\n",
    "\n",
    "        baselines[baseline_name][\"cov_grad_tr\"].append([])\n",
    "        baselines[baseline_name][\"cov_grad_max_eigen\"].append([])\n",
    "        baselines[baseline_name][\"cov_update_tr\"].append([])\n",
    "        baselines[baseline_name][\"cov_update_max_eigen\"].append([])\n",
    "\n",
    "        for iter_i in [1, *range(ckpt_iter_freq, max_iters + 1, ckpt_iter_freq)]:\n",
    "            if iter_i % 50 == 0:\n",
    "                print(f\"    iter_i={iter_i}\")\n",
    "\n",
    "            ### load ckpt\n",
    "            ckpt_path = os.path.join(ckpts_dir, f\"run{test_run_i}_{iter_i}.pt\")\n",
    "            ckpt = torch.load(ckpt_path)\n",
    "\n",
    "            ### init optee\n",
    "            optee_ckpt = ckpt[\"optimizee\"]\n",
    "            optee = w(config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"]))\n",
    "            optee.load_state_dict(optee_ckpt)\n",
    "            optee_grads = ckpt[\"optimizee_grads\"]\n",
    "            for k, v in optee.named_parameters():\n",
    "                v.grad = optee_grads[k]\n",
    "            if hasattr(optee, \"batch_norm\"):\n",
    "                optee.batch_norm.eval()\n",
    "            \n",
    "            ### init opter\n",
    "            opter_ckpt = ckpt[\"optimizer\"]\n",
    "            opter = config[\"meta_testing\"][\"baseline_opter_cls\"](optee.parameters(), **config[\"meta_testing\"][\"baseline_opter_config\"])\n",
    "            opter.load_state_dict(opter_ckpt)\n",
    "\n",
    "            ### get trace of cov(grad) and cov(updates)\n",
    "            tr_grads, tr_updates = get_cov_trace(\n",
    "                opter=opter,\n",
    "                hidden_states=None,\n",
    "                cell_states=None,\n",
    "                optee=optee,\n",
    "                data_full=data_full,\n",
    "                data_samples=data_samples,\n",
    "                trace_estimate_data_frac=trace_estimate_data_frac,\n",
    "            )\n",
    "            baselines[baseline_name][\"cov_grad_tr\"][-1].append(tr_grads)\n",
    "            baselines[baseline_name][\"cov_update_tr\"][-1].append(tr_updates)\n",
    "\n",
    "            ### get max eigenvalue of cov(grad) and cov(updates)\n",
    "            max_eigenval_grads, max_eigenval_updates = get_cov_max_eigenval(\n",
    "                opter=opter,\n",
    "                hidden_states=None,\n",
    "                cell_states=None,\n",
    "                optee=optee,\n",
    "                data_mini_batched=data_mini_batched,\n",
    "                # data_mini_batched=data_samples,\n",
    "                eigen_estimate_data_frac=eigen_estimate_data_frac,\n",
    "            )\n",
    "            baselines[baseline_name][\"cov_grad_max_eigen\"][-1].append(max_eigenval_grads)\n",
    "            baselines[baseline_name][\"cov_update_max_eigen\"][-1].append(max_eigenval_updates)\n",
    "\n",
    "    ### to np arrays\n",
    "    baselines[baseline_name][\"cov_grad_tr\"] = np.array(baselines[baseline_name][\"cov_grad_tr\"])\n",
    "    baselines[baseline_name][\"cov_grad_max_eigen\"] = np.array(baselines[baseline_name][\"cov_grad_max_eigen\"])\n",
    "    baselines[baseline_name][\"cov_update_tr\"] = np.array(baselines[baseline_name][\"cov_update_tr\"])\n",
    "    baselines[baseline_name][\"cov_update_max_eigen\"] = np.array(baselines[baseline_name][\"cov_update_max_eigen\"])\n",
    "\n",
    "    ### save\n",
    "    torch.save({\n",
    "        \"cov_grad_tr\": baselines[baseline_name][\"cov_grad_tr\"],\n",
    "        \"cov_grad_max_eigen\": baselines[baseline_name][\"cov_grad_max_eigen\"],\n",
    "        \"cov_update_tr\": baselines[baseline_name][\"cov_update_tr\"],\n",
    "        \"cov_update_max_eigen\": baselines[baseline_name][\"cov_update_max_eigen\"],\n",
    "    }, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot config\n",
    "to_plot = \"cov_update_tr\"\n",
    "plot_iters = max_iters\n",
    "# plot_iters = 200\n",
    "log_scale = True\n",
    "save_to_dir = \"../results/sym_breaking_regularization/MNISTLeakyRelu_meta_training\"\n",
    "save_to_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x_ticks = [1, *range(ckpt_iter_freq, plot_iters + 1, ckpt_iter_freq)]\n",
    "\n",
    "### plot baselines\n",
    "for baseline_name in baselines:\n",
    "    y_mean = np.mean(baselines[baseline_name][to_plot][:,:(plot_iters // ckpt_iter_freq) + 1], axis=0)\n",
    "    sns.lineplot(x=x_ticks, y=y_mean, label=baseline_name, ax=ax, linestyle=\"--\")\n",
    "\n",
    "    ### error bars\n",
    "    y_std = np.std(baselines[baseline_name][to_plot][:,:(plot_iters // ckpt_iter_freq) + 1], axis=0)\n",
    "    ax.fill_between(\n",
    "        x=x_ticks,\n",
    "        y1=y_mean - y_std,\n",
    "        y2=y_mean + y_std,\n",
    "        alpha=0.2\n",
    "    )\n",
    "\n",
    "### plot l2os\n",
    "for opter_name in l2os:\n",
    "    y_mean = np.mean(l2os[opter_name][to_plot][:,:(plot_iters // ckpt_iter_freq) + 1], axis=0)\n",
    "    sns.lineplot(x=x_ticks, y=y_mean, label=opter_name, ax=ax)\n",
    "\n",
    "    ### error bars\n",
    "    y_std = np.std(l2os[opter_name][to_plot][:,:(plot_iters // ckpt_iter_freq) + 1], axis=0)\n",
    "    ax.fill_between(\n",
    "        x=x_ticks,\n",
    "        y1=y_mean - y_std,\n",
    "        y2=y_mean + y_std,\n",
    "        alpha=0.2\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(to_plot)\n",
    "ax.legend(bbox_to_anchor=(0.5, 1.08), loc=\"center\", ncol=3)\n",
    "legend = ax.get_legend()\n",
    "for legend_handle in legend.legendHandles:\n",
    "    legend_handle.set_linewidth(3.0)\n",
    "\n",
    "if log_scale:\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "if save_to_dir is not None:\n",
    "    fig_name = f\"{to_plot}\"\n",
    "    fig_name += f\"_iters_{plot_iters}\"\n",
    "    fig_name += f\"_log\" if log_scale else \"\"\n",
    "    fig.savefig(os.path.join(save_to_dir, f\"{fig_name}.png\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stiffness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config for meta-testing\n",
    "phase = \"meta_testing\"\n",
    "max_iters = 1000\n",
    "max_test_runs = 3\n",
    "ckpt_iter_freq = 1\n",
    "\n",
    "### config for meta-training\n",
    "# phase = \"meta_training\"\n",
    "# max_iters = 200\n",
    "# max_test_runs = 1\n",
    "# epoch = 30\n",
    "# ckpt_iter_freq = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect for L2O\n",
    "for opter_name in l2os:\n",
    "    print(f\"Collecting for {opter_name}...\")\n",
    "    config = l2os[opter_name][\"config\"]\n",
    "    if phase == \"meta_training\":\n",
    "        ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_training\"][\"ckpt_dir\"])\n",
    "        save_path = os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_base_dir\"], f\"stiffness_analysis_meta_training_{epoch}e.pt\")\n",
    "    else:\n",
    "        ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"])\n",
    "        save_path = os.path.join(os.environ[\"CKPT_PATH\"], config[\"ckpt_base_dir\"], \"stiffness_analysis.pt\")\n",
    "\n",
    "    l2os[opter_name][\"param_sai\"] = dict()\n",
    "    l2os[opter_name][\"grad_sai\"] = dict()\n",
    "\n",
    "    ### load already collected\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"  Loading previously collected results...\")\n",
    "        stiffness_analysis = torch.load(save_path)\n",
    "        for k in [\"param_sai\", \"grad_sai\"]:\n",
    "            l2os[opter_name][k] = stiffness_analysis[k]\n",
    "        continue\n",
    "\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        if max_test_runs and test_run_i >= max_test_runs:\n",
    "            break\n",
    "        print(f\"  Test run {test_run_i}...\")\n",
    "\n",
    "        for k in l2os[opter_name][\"param_sai\"].keys():\n",
    "            l2os[opter_name][\"param_sai\"][k].append([])\n",
    "        for k in l2os[opter_name][\"grad_sai\"].keys():\n",
    "            l2os[opter_name][\"grad_sai\"][k].append([])\n",
    "\n",
    "        for iters in zip(\n",
    "            [1, *range(ckpt_iter_freq, max_iters, ckpt_iter_freq)],\n",
    "            range(ckpt_iter_freq, max_iters + ckpt_iter_freq, ckpt_iter_freq)\n",
    "        ):\n",
    "            ### load checkpoints\n",
    "            if phase == \"meta_training\":\n",
    "                ckpt_1_path = os.path.join(ckpts_dir, f\"{epoch}e_{iters[0]}.pt\")\n",
    "                ckpt_2_path = os.path.join(ckpts_dir, f\"{epoch}e_{iters[1]}.pt\")\n",
    "                if not os.path.isfile(ckpt_1_path):\n",
    "                    ### missing as zero sai\n",
    "                    if \"params_combined\" not in l2os[opter_name][\"param_sai\"]:\n",
    "                        l2os[opter_name][\"param_sai\"][\"params_combined\"] = [[]]\n",
    "                    if \"params_combined\" not in l2os[opter_name][\"grad_sai\"]:\n",
    "                        l2os[opter_name][\"grad_sai\"][\"params_combined\"] = [[]]\n",
    "                    for n, k in enumerate(l2os[opter_name][\"param_sai\"].keys()):\n",
    "                        l2os[opter_name][\"param_sai\"][k][-1].append(0)\n",
    "                    for n, k in enumerate(l2os[opter_name][\"grad_sai\"].keys()):\n",
    "                        l2os[opter_name][\"grad_sai\"][k][-1].append(0)\n",
    "                    continue\n",
    "            else:\n",
    "                ckpt_1_path = os.path.join(ckpts_dir, f\"run{test_run_i}_{iters[0]}.pt\")\n",
    "                ckpt_2_path = os.path.join(ckpts_dir, f\"run{test_run_i}_{iters[1]}.pt\")\n",
    "\n",
    "            ckpt_1 = torch.load(ckpt_1_path, map_location=\"cpu\")\n",
    "            ckpt_2 = torch.load(ckpt_2_path, map_location=\"cpu\")\n",
    "            assert ckpt_1[\"optimizee\"].keys() == ckpt_2[\"optimizee\"].keys()\n",
    "            assert ckpt_1[\"optimizee_grads\"].keys() == ckpt_2[\"optimizee_grads\"].keys()\n",
    "            \n",
    "            ### calculate sai\n",
    "            for param_name in ckpt_1[\"optimizee\"].keys():\n",
    "                if ckpt_1[\"optimizee_grads\"][param_name] is None:\n",
    "                    continue\n",
    "                if param_name not in l2os[opter_name][\"param_sai\"]:\n",
    "                    l2os[opter_name][\"param_sai\"][param_name] = [[]]\n",
    "                    if \"params_combined\" in l2os[opter_name][\"param_sai\"] and len(l2os[opter_name][\"param_sai\"][\"params_combined\"]) > 0:\n",
    "                        # add zeros for missing iterations\n",
    "                        l2os[opter_name][\"param_sai\"][param_name][-1].extend(\n",
    "                            [0] * (len(l2os[opter_name][\"param_sai\"][\"params_combined\"][0]) - len(l2os[opter_name][\"param_sai\"][param_name][-1]))\n",
    "                        )\n",
    "                if param_name not in l2os[opter_name][\"grad_sai\"]:\n",
    "                    l2os[opter_name][\"grad_sai\"][param_name] = [[]]\n",
    "                    if \"params_combined\" in l2os[opter_name][\"grad_sai\"] and len(l2os[opter_name][\"grad_sai\"][\"params_combined\"]) > 0:\n",
    "                        # add zeros for missing iterations\n",
    "                        l2os[opter_name][\"grad_sai\"][param_name][-1].extend(\n",
    "                            [0] * (len(l2os[opter_name][\"grad_sai\"][\"params_combined\"][0]) - len(l2os[opter_name][\"grad_sai\"][param_name][-1]))\n",
    "                        )\n",
    "\n",
    "                ### param sai\n",
    "                param_sai = calc_sai(\n",
    "                    vec_t0=ckpt_1[\"optimizee\"][param_name].view(-1),\n",
    "                    vec_t1=ckpt_2[\"optimizee\"][param_name].view(-1),\n",
    "                    time_delta=iters[1] - iters[0],\n",
    "                    normalize=True\n",
    "                )\n",
    "                l2os[opter_name][\"param_sai\"][param_name][-1].append(param_sai.item())\n",
    "\n",
    "                ### grad sai\n",
    "                grad_sai = calc_sai(\n",
    "                    vec_t0=ckpt_1[\"optimizee_grads\"][param_name].view(-1),\n",
    "                    vec_t1=ckpt_2[\"optimizee_grads\"][param_name].view(-1),\n",
    "                    time_delta=iters[1] - iters[0],\n",
    "                    normalize=True\n",
    "                )\n",
    "                l2os[opter_name][\"grad_sai\"][param_name][-1].append(grad_sai.item())\n",
    "\n",
    "            ### params combined\n",
    "            ### param sai\n",
    "            if \"params_combined\" not in l2os[opter_name][\"param_sai\"]:\n",
    "                l2os[opter_name][\"param_sai\"][\"params_combined\"] = [[]]\n",
    "            params_combined_ckpt_1 = torch.cat([ckpt_1[\"optimizee\"][param_name].view(-1) for param_name in ckpt_1[\"optimizee\"].keys() if ckpt_1[\"optimizee_grads\"][param_name] is not None])\n",
    "            params_combined_ckpt_2 = torch.cat([ckpt_2[\"optimizee\"][param_name].view(-1) for param_name in ckpt_2[\"optimizee\"].keys() if ckpt_2[\"optimizee_grads\"][param_name] is not None])\n",
    "            params_combined_sai = calc_sai(\n",
    "                vec_t0=params_combined_ckpt_1,\n",
    "                vec_t1=params_combined_ckpt_2,\n",
    "                time_delta=iters[1] - iters[0],\n",
    "                normalize=True\n",
    "            )\n",
    "            l2os[opter_name][\"param_sai\"][\"params_combined\"][-1].append(params_combined_sai.item())\n",
    "\n",
    "            ### grad sai\n",
    "            if \"params_combined\" not in l2os[opter_name][\"grad_sai\"]:\n",
    "                l2os[opter_name][\"grad_sai\"][\"params_combined\"] = [[]]\n",
    "            grads_combined_ckpt_1 = torch.cat([ckpt_1[\"optimizee_grads\"][param_name].view(-1) for param_name in ckpt_1[\"optimizee_grads\"].keys() if ckpt_1[\"optimizee_grads\"][param_name] is not None])\n",
    "            grads_combined_ckpt_2 = torch.cat([ckpt_2[\"optimizee_grads\"][param_name].view(-1) for param_name in ckpt_2[\"optimizee_grads\"].keys() if ckpt_2[\"optimizee_grads\"][param_name] is not None])\n",
    "            grads_combined_sai = calc_sai(\n",
    "                vec_t0=grads_combined_ckpt_1,\n",
    "                vec_t1=grads_combined_ckpt_2,\n",
    "                time_delta=iters[1] - iters[0],\n",
    "                normalize=True\n",
    "            )\n",
    "            l2os[opter_name][\"grad_sai\"][\"params_combined\"][-1].append(grads_combined_sai.item())\n",
    "\n",
    "    ### to np arrays\n",
    "    for param_name in l2os[opter_name][\"param_sai\"].keys():\n",
    "        l2os[opter_name][\"param_sai\"][param_name] = np.array(l2os[opter_name][\"param_sai\"][param_name])\n",
    "    for param_name in l2os[opter_name][\"grad_sai\"].keys():\n",
    "        l2os[opter_name][\"grad_sai\"][param_name] = np.array(l2os[opter_name][\"grad_sai\"][param_name])\n",
    "\n",
    "    ### save\n",
    "    torch.save({\n",
    "        \"param_sai\": l2os[opter_name][\"param_sai\"],\n",
    "        \"grad_sai\": l2os[opter_name][\"grad_sai\"]\n",
    "    }, save_path)\n",
    "    print(f\"  Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect for baselines\n",
    "for baseline_name in baselines:\n",
    "    print(f\"Collecting for {baseline_name}...\")\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "    ckpts_dir = os.path.join(baselines[baseline_name][\"baseline_dir\"], \"ckpt\")\n",
    "    save_path = os.path.join(baselines[baseline_name][\"baseline_dir\"], \"stiffness_analysis.pt\")\n",
    "\n",
    "    baselines[baseline_name][\"param_sai\"] = dict()\n",
    "    baselines[baseline_name][\"grad_sai\"] = dict()\n",
    "\n",
    "    ### load already collected\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"  Loading previously collected results...\")\n",
    "        stiffness_analysis = torch.load(save_path)\n",
    "        for k in [\"param_sai\", \"grad_sai\"]:\n",
    "            baselines[baseline_name][k] = stiffness_analysis[k]\n",
    "        continue\n",
    "\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        if max_test_runs and test_run_i >= max_test_runs:\n",
    "            break\n",
    "        print(f\"  Test run {test_run_i}...\")\n",
    "        \n",
    "        for k in baselines[baseline_name][\"param_sai\"].keys():\n",
    "            baselines[baseline_name][\"param_sai\"][k].append([])\n",
    "        for k in baselines[baseline_name][\"grad_sai\"].keys():\n",
    "            baselines[baseline_name][\"grad_sai\"][k].append([])\n",
    "\n",
    "        for iters in zip(range(1, max_iters), range(2, max_iters + 1)):\n",
    "            ### load checkpoints\n",
    "            ckpt_1_path = os.path.join(ckpts_dir, f\"run{test_run_i}_{iters[0]}.pt\")\n",
    "            ckpt_2_path = os.path.join(ckpts_dir, f\"run{test_run_i}_{iters[1]}.pt\")\n",
    "\n",
    "            ckpt_1 = torch.load(ckpt_1_path, map_location=\"cpu\")\n",
    "            ckpt_2 = torch.load(ckpt_2_path, map_location=\"cpu\")\n",
    "            assert ckpt_1[\"optimizee\"].keys() == ckpt_2[\"optimizee\"].keys()\n",
    "            assert ckpt_1[\"optimizee_grads\"].keys() == ckpt_2[\"optimizee_grads\"].keys()\n",
    "            \n",
    "            ### calculate sai\n",
    "            for param_name in ckpt_1[\"optimizee\"].keys():\n",
    "                if ckpt_1[\"optimizee_grads\"][param_name] is None or ckpt_2[\"optimizee_grads\"][param_name] is None:\n",
    "                    continue\n",
    "                if param_name not in baselines[baseline_name][\"param_sai\"]:\n",
    "                    baselines[baseline_name][\"param_sai\"][param_name] = [[]]\n",
    "                if param_name not in baselines[baseline_name][\"grad_sai\"]:\n",
    "                    baselines[baseline_name][\"grad_sai\"][param_name] = [[]]\n",
    "                \n",
    "                ### param sai\n",
    "                param_sai = calc_sai(\n",
    "                    vec_t0=ckpt_1[\"optimizee\"][param_name].view(-1),\n",
    "                    vec_t1=ckpt_2[\"optimizee\"][param_name].view(-1),\n",
    "                    time_delta=iters[1] - iters[0],\n",
    "                    normalize=True\n",
    "                )\n",
    "                baselines[baseline_name][\"param_sai\"][param_name][-1].append(param_sai.item())\n",
    "\n",
    "                ### grad sai\n",
    "                grad_sai = calc_sai(\n",
    "                    vec_t0=ckpt_1[\"optimizee_grads\"][param_name].view(-1),\n",
    "                    vec_t1=ckpt_2[\"optimizee_grads\"][param_name].view(-1),\n",
    "                    time_delta=iters[1] - iters[0],\n",
    "                    normalize=True\n",
    "                )\n",
    "                baselines[baseline_name][\"grad_sai\"][param_name][-1].append(grad_sai.item())\n",
    "            \n",
    "            ### params combined\n",
    "            ### param sai\n",
    "            if \"params_combined\" not in baselines[baseline_name][\"param_sai\"]:\n",
    "                baselines[baseline_name][\"param_sai\"][\"params_combined\"] = [[]]\n",
    "            params_combined_ckpt_1 = torch.cat([ckpt_1[\"optimizee\"][param_name].view(-1) \n",
    "                                                for param_name in ckpt_1[\"optimizee\"].keys() if ckpt_1[\"optimizee_grads\"][param_name] is not None], dim=0)\n",
    "            params_combined_ckpt_2 = torch.cat([ckpt_2[\"optimizee\"][param_name].view(-1)\n",
    "                                                for param_name in ckpt_2[\"optimizee\"].keys() if ckpt_2[\"optimizee_grads\"][param_name] is not None], dim=0)\n",
    "            params_combined_sai = calc_sai(\n",
    "                vec_t0=params_combined_ckpt_1,\n",
    "                vec_t1=params_combined_ckpt_2,\n",
    "                time_delta=iters[1] - iters[0],\n",
    "                normalize=True\n",
    "            )\n",
    "            baselines[baseline_name][\"param_sai\"][\"params_combined\"][-1].append(params_combined_sai.item())\n",
    "\n",
    "            ### grad sai\n",
    "            if \"params_combined\" not in baselines[baseline_name][\"grad_sai\"]:\n",
    "                baselines[baseline_name][\"grad_sai\"][\"params_combined\"] = [[]]\n",
    "            grads_combined_ckpt_1 = torch.cat([ckpt_1[\"optimizee_grads\"][param_name].view(-1)\n",
    "                                               for param_name in ckpt_1[\"optimizee_grads\"].keys() if ckpt_1[\"optimizee_grads\"][param_name] is not None], dim=0)\n",
    "            grads_combined_ckpt_2 = torch.cat([ckpt_2[\"optimizee_grads\"][param_name].view(-1)\n",
    "                                               for param_name in ckpt_2[\"optimizee_grads\"].keys() if ckpt_2[\"optimizee_grads\"][param_name] is not None], dim=0)\n",
    "            params_combined_grads_sai = calc_sai(\n",
    "                vec_t0=grads_combined_ckpt_1,\n",
    "                vec_t1=grads_combined_ckpt_2,\n",
    "                time_delta=iters[1] - iters[0],\n",
    "                normalize=True\n",
    "            )\n",
    "            baselines[baseline_name][\"grad_sai\"][\"params_combined\"][-1].append(params_combined_grads_sai.item())\n",
    "\n",
    "    ### to np arrays\n",
    "    for param_name in baselines[baseline_name][\"param_sai\"].keys():\n",
    "        baselines[baseline_name][\"param_sai\"][param_name] = np.array(baselines[baseline_name][\"param_sai\"][param_name])\n",
    "    for param_name in baselines[baseline_name][\"grad_sai\"].keys():\n",
    "        baselines[baseline_name][\"grad_sai\"][param_name] = np.array(baselines[baseline_name][\"grad_sai\"][param_name])\n",
    "    \n",
    "    ### save\n",
    "    torch.save({\n",
    "        \"param_sai\": baselines[baseline_name][\"param_sai\"],\n",
    "        \"grad_sai\": baselines[baseline_name][\"grad_sai\"]\n",
    "    }, save_path)\n",
    "    print(f\"  Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_plot = \"grad_sai\"\n",
    "log_scale = True\n",
    "iters_window = (0, max_iters - 1)\n",
    "# iters_window = (0, 200)\n",
    "x_ticks = range(iters_window[0], iters_window[1], ckpt_iter_freq)\n",
    "save_fig = True\n",
    "\n",
    "fig = plt.figure(figsize=(22, 18))\n",
    "ax_i = 1\n",
    "for param_name in l2os[opter_name][to_plot].keys():\n",
    "    if param_name == \"params_combined\":\n",
    "        ax = fig.add_subplot(3, 2, (ax_i, ax_i + 1))\n",
    "        ax_i += 2\n",
    "    else:\n",
    "        ax = fig.add_subplot(3, 2, ax_i)\n",
    "        ax_i += 1\n",
    "    ax.set_title(param_name, fontsize=18)\n",
    "\n",
    "    ### plot baselines\n",
    "    for baseline_name in baselines:\n",
    "        y_mean = np.mean(baselines[baseline_name][to_plot][param_name][:, iters_window[0] // ckpt_iter_freq:(iters_window[1] // ckpt_iter_freq) + 1], axis=0)\n",
    "        sns.lineplot(\n",
    "            x=x_ticks,\n",
    "            y=y_mean,\n",
    "            label=baseline_name,\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        ### error bars\n",
    "        y_std = np.std(baselines[baseline_name][to_plot][param_name][:, iters_window[0] // ckpt_iter_freq:(iters_window[1] // ckpt_iter_freq) + 1], axis=0)\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=y_mean - y_std,\n",
    "            y2=y_mean + y_std,\n",
    "            alpha=0.2\n",
    "        )\n",
    "    \n",
    "    ### plot L2O\n",
    "    for opter_name in l2os:\n",
    "        y_mean = np.mean(l2os[opter_name][to_plot][param_name][:, iters_window[0] // ckpt_iter_freq:(iters_window[1] // ckpt_iter_freq) + 1], axis=0)\n",
    "        sns.lineplot(\n",
    "            x=x_ticks,\n",
    "            y=y_mean,\n",
    "            label=opter_name,\n",
    "            linewidth=2.5,\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        ### error bars\n",
    "        y_std = np.std(l2os[opter_name][to_plot][param_name][:, iters_window[0] // ckpt_iter_freq:(iters_window[1] // ckpt_iter_freq) + 1], axis=0)\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=y_mean - y_std,\n",
    "            y2=y_mean + y_std,\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Iteration\", fontsize=14)\n",
    "    y_label = to_plot.split(\"_\")\n",
    "    y_label = \" \".join((y_label[0].title(), y_label[1].upper()))\n",
    "    if log_scale:\n",
    "        y_label += \" (log scale)\"\n",
    "        ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(y_label, fontsize=14)\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "fig.tight_layout()\n",
    "if save_fig:\n",
    "    fig_name = f\"{to_plot}_iters_{iters_window[0]}_{iters_window[1]}\"\n",
    "    if phase == \"meta_training\":\n",
    "        fig_name += f\"_{epoch}e\"\n",
    "    if log_scale:\n",
    "        fig_name += \"_log\"\n",
    "    fig.savefig(f\"{fig_name}.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter updates histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(updates, grads, iterations, plot_abs=True, log_scale=True, save_to_dir=None):\n",
    "    assert updates.ndim == 2, \"updates must be a 2D array [iterations, updates]\"\n",
    "    assert len(updates) == len(grads)\n",
    "    assert len(updates) == len(iterations)\n",
    "    assert updates.shape[1] == grads.shape[1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 3 * len(updates)))\n",
    "    n_axes = len(updates)\n",
    "    axes = [fig.add_subplot(n_axes, 1, i + 1) for i in range(n_axes)]\n",
    "    min_val, max_val = np.inf, -np.inf\n",
    "\n",
    "    for i, iter in zip(range(n_axes), iterations):\n",
    "        ax = axes[i]\n",
    "        ax.set_title(f\"Iteration {iter}\")\n",
    "        ax.set_xlabel(\"Value\" if not plot_abs else \"Absolute Value\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "\n",
    "        if not plot_abs:\n",
    "            ax.plot([0, 0], [0, 1e4], color=\"black\", linewidth=0.5, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "        sns.histplot(\n",
    "            updates[i] if not plot_abs else torch.abs(updates[i]),\n",
    "            ax=ax,\n",
    "            # binwidth=0.0075,\n",
    "            # bins=50,s\n",
    "            label=\"update\",\n",
    "            legend=False,\n",
    "            element=\"step\"\n",
    "        )\n",
    "        sns.histplot(\n",
    "            grads[i] if not plot_abs else torch.abs(grads[i]),\n",
    "            ax=ax,\n",
    "            # binwidth=0.0075,\n",
    "            # bins=50,\n",
    "            color=\"red\",\n",
    "            alpha=0.35,\n",
    "            label=\"gradient\",\n",
    "            legend=False,\n",
    "            element=\"step\"\n",
    "        )\n",
    "\n",
    "        ax.legend()\n",
    "        if log_scale:\n",
    "            ax.set_yscale(\"log\")\n",
    "\n",
    "        # get min and max values for all axes\n",
    "        min_val = min(min_val, ax.get_xlim()[0])\n",
    "        max_val = max(max_val, ax.get_xlim()[1])\n",
    "\n",
    "    # set all axes to have the same x limits\n",
    "    for i in range(n_axes):\n",
    "        ax = axes[i]\n",
    "        ax.set_xlim(min_val, max_val)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if save_to_dir is not None:\n",
    "        fig_name = f\"update_hist_all\"\n",
    "        if plot_abs:\n",
    "            fig_name += \"_abs\"\n",
    "        if log_scale:\n",
    "            fig_name += \"_log\"\n",
    "        fig.savefig(os.path.join(save_to_dir, f\"{fig_name}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot histogram of updates - config\n",
    "log_scale = True\n",
    "plot_abs = True\n",
    "optee_updates_lr = 0.1\n",
    "l2o_to_plot = list(l2os.keys())[0]\n",
    "config = l2os[l2o_to_plot][\"config\"]\n",
    "print(f\"Plotting {l2o_to_plot} updates\")\n",
    "save_to_dir = \"../results/sym_breaking_regularization/MNISTNet_meta_training\"\n",
    "# save_to_dir = None\n",
    "\n",
    "### collect for meta-testing\n",
    "ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"])\n",
    "show_iters = [1, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "ckpt_prefix = \"run0_\"\n",
    "\n",
    "### collect for meta-training\n",
    "# ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_training\"][\"ckpt_dir\"])\n",
    "# show_iters = [1, 5, 10, 20, 50, 100, 200]\n",
    "# ckpt_prefix = \"40e_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect and plot histogram of updates\n",
    "all_updates, all_grads, iterations = [], [], []\n",
    "for iteration in show_iters:\n",
    "    ckpt_path = os.path.join(ckpts_dir, f\"{ckpt_prefix}{iteration}.pt\")\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    updates = optee_updates_lr * torch.cat(\n",
    "        [ckpt[\"optimizee_updates\"][param_name].view(-1) for param_name in ckpt[\"optimizee_updates\"].keys() if \"running\" not in param_name]\n",
    "    )\n",
    "    grads = torch.cat(\n",
    "        [ckpt[\"optimizee_grads\"][param_name].view(-1) for param_name in ckpt[\"optimizee_grads\"].keys() if \"running\" not in param_name]\n",
    "    )\n",
    "\n",
    "    all_updates.append(updates.view(1, -1))\n",
    "    all_grads.append(grads.view(1, -1))\n",
    "    iterations.append(iteration)\n",
    "\n",
    "all_updates = torch.cat(all_updates, dim=0)\n",
    "all_grads = torch.cat(all_grads, dim=0)\n",
    "plot_hist(all_updates, all_grads, iterations, plot_abs=plot_abs, log_scale=log_scale, save_to_dir=save_to_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5692ede66a2eeda96ca4e496ad881a063b66ee8e9ec6003b28974c60439bc6fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
