{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import lovely_tensors as lt # can be removed\n",
    "\n",
    "from l2o.others import w, detach_var, rsetattr, rgetattr, count_parameters, print_grads, \\\n",
    "    load_l2o_opter_ckpt, load_baseline_opter_ckpt, load_ckpt, get_baseline_ckpt_dir, dict_to_str\n",
    "from l2o.visualization import get_model_dot\n",
    "from l2o.training import do_fit, fit_normal, fit_optimizer, find_best_lr_normal\n",
    "from l2o.regularization import (\n",
    "    regularize_updates_translation_constraints,\n",
    "    regularize_updates_scale_constraints,\n",
    "    regularize_updates_rescale_constraints,\n",
    "    regularize_updates_constraints,\n",
    "    regularize_translation_conservation_law_breaking,\n",
    "    regularize_rescale_conservation_law_breaking,\n",
    ")\n",
    "from l2o.analysis import (\n",
    "    get_rescale_sym_constraint_deviation,\n",
    "    get_translation_sym_constraint_deviations,\n",
    "    get_scale_sym_constraint_deviation,\n",
    "    get_baseline_opter_param_updates,\n",
    "    collect_rescale_sym_deviations,\n",
    "    collect_translation_sym_deviations,\n",
    "    collect_scale_sym_deviations,\n",
    "    collect_conservation_law_deviations,\n",
    ")\n",
    "from l2o.tail_index_utils import (\n",
    "    alpha_estimator,\n",
    ")\n",
    "from l2o.data import MNIST, CIFAR10\n",
    "from l2o.optimizer import Optimizer\n",
    "from l2o.optimizee import (\n",
    "    MNISTSigmoid,\n",
    "    MNISTReLU,\n",
    "    MNISTNet,\n",
    "    MNISTNet2Layer,\n",
    "    MNISTNetBig,\n",
    "    MNISTRelu,\n",
    "    MNISTLeakyRelu,\n",
    "    MNISTSimoidBatchNorm,\n",
    "    MNISTReluBatchNorm,\n",
    "    MNISTConv,\n",
    "    MNISTReluBig,\n",
    "    MNISTReluBig2Layer,\n",
    "    MNISTMixtureOfActivations,\n",
    "    MNISTNetBig2Layer,\n",
    ")\n",
    "from l2o.meta_module import *\n",
    "from meta_test import meta_test, meta_test_baselines\n",
    "\n",
    "lt.monkey_patch() # can be removed\n",
    "sns.set(color_codes=True)\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### publication figure settings:\n",
    "plt.rc(\"font\", family=\"serif\")\n",
    "\n",
    "plt.rc(\"legend\", fontsize=12)\n",
    "plt.rc(\"xtick\", labelsize=12)\n",
    "plt.rc(\"ytick\", labelsize=12)\n",
    "plt.rc(\"axes\", labelsize=13)\n",
    "plt.rc(\"axes\", titlesize=13)\n",
    "plt.rc(\"axes\", linewidth=0.5)\n",
    "plt.rc(\"axes\", labelpad=10)\n",
    "\n",
    "plt.rc(\"lines\", linewidth=1.)\n",
    "\n",
    "plt.rc(\"figure\", dpi=300)\n",
    "plt.rc(\"figure\", figsize=(6, 4))\n",
    "\n",
    "plt.rc(\"savefig\", dpi=300)\n",
    "plt.rc(\"savefig\", format=\"pdf\")\n",
    "plt.rc(\"savefig\", bbox=\"tight\")\n",
    "plt.rc(\"savefig\", pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "l2os = {\n",
    "    ### MNISTReluBatchNorm\n",
    "    \"L2O - relu batchnorm\": \"26-04-2023_01-22-31_MNISTReluBatchNorm_Optimizer\",\n",
    "    # \"reg_1e-2_scale\": \"03-05-2023_22-42-56_MNISTReluBatchNorm_Optimizer\",\n",
    "    # \"reg_1e-1_scale\": \"02-05-2023_16-51-19_MNISTReluBatchNorm_Optimizer\",\n",
    "    # \"reg_1e0_scale\": \"03-05-2023_22-42-36_MNISTReluBatchNorm_Optimizer\",\n",
    "    \n",
    "    # \"L2O - unroll 10\": \"10-05-2023_13-26-17_MNISTReluBatchNorm_Optimizer\", # already collected\n",
    "    # \"L2O - unroll 20\": \"12-05-2023_00-38-54_MNISTReluBatchNorm_Optimizer\", # lr: 1e-3, already collected\n",
    "    # \"L2O - unroll 30\": \"11-05-2023_01-27-43_MNISTReluBatchNorm_Optimizer\", # already collected\n",
    "    \n",
    "    ### MNISTLeakyRelu\n",
    "    \"L2O - leakyrelu\": \"19-02-2023_18-20-21_MNISTLeakyRelu_Optimizer\",\n",
    "    # \"reg_1e-2_rescale\": \"30-04-2023_13-08-09_MNISTLeakyRelu_Optimizer\",\n",
    "    # \"reg_1e-1_rescale\": \"06-05-2023_01-54-04_MNISTLeakyRelu_Optimizer\",\n",
    "    # \"reg_5e-1_rescale\": \"05-05-2023_01-42-29_MNISTLeakyRelu_Optimizer\",\n",
    "    \n",
    "    # \"L2O - unroll 10\": \"09-05-2023_20-50-27_MNISTLeakyRelu_Optimizer\", # already collected\n",
    "    # \"L2O - unroll 20\": \"12-05-2023_00-35-50_MNISTLeakyRelu_Optimizer\", # lr: 1e-3, already collected\n",
    "    # \"L2O - unroll 30\": \"10-05-2023_11-11-39_MNISTLeakyRelu_Optimizer\", # already collected\n",
    "    \n",
    "    ### MNISTNet\n",
    "    \"L2O - sigmoid\": \"05-03-2023_01-33-57_MNISTNet_Optimizer\",\n",
    "    # \"reg_1e-2_translation\": \"29-04-2023_01-35-00_MNISTNet_Optimizer\",\n",
    "    # \"reg_1e-1_translation\": \"28-04-2023_13-45-29_MNISTNet_Optimizer\",\n",
    "    # \"reg_5e-1_translation\": \"30-04-2023_12-41-11_MNISTNet_Optimizer\",\n",
    "    \n",
    "    # \"L2O - unroll 10\": \"09-05-2023_20-41-10_MNISTNet_Optimizer\", # already collected\n",
    "    # \"L2O - unroll 20\": \"07-05-2023_20-52-18_MNISTNet_Optimizer\", # lr: 1e-3, already collected\n",
    "    # \"L2O - unroll 30\": \"09-05-2023_20-42-08_MNISTNet_Optimizer\", # already collected\n",
    "}\n",
    "\n",
    "baselines = {\n",
    "    ### MNISTReluBatchNorm\n",
    "    \"Adam\": \"Adam_{lr=find_best_lr_normal}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=128}\",\n",
    "    \"SGD\": \"SGD_{lr=find_best_lr_normal_momentum=0.9}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=128}\",\n",
    "\n",
    "    ### MNISTLeakyRelu\n",
    "    # \"Adam\": \"Adam_{lr=find_best_lr_normal}_MNISTLeakyRelu_{}_MNIST_{batch_size=128}\",\n",
    "    # \"SGD\": \"SGD_{lr=find_best_lr_normal_momentum=0.9}_MNISTLeakyRelu_{}_MNIST_{batch_size=128}\",\n",
    "\n",
    "    ### MNISTNet\n",
    "    # \"Adam\": \"Adam_{lr=find_best_lr_normal}_MNISTNet_{}_MNIST_{batch_size=128}\",\n",
    "    # \"SGD\": \"SGD_{lr=find_best_lr_normal_momentum=0.9}_MNISTNet_{}_MNIST_{batch_size=128}\",\n",
    "}\n",
    "\n",
    "_baselines = {\n",
    "    ### MNISTNet\n",
    "    # \"SGD_{lr=0.001}_{batch_size=8}\": \"SGD_{lr=0.001}_MNISTNet_{}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=0.01}_{batch_size=8}\": \"SGD_{lr=0.01}_MNISTNet_{}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=0.1}_{batch_size=8}\": \"SGD_{lr=0.1}_MNISTNet_{}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=1.0}_{batch_size=8}\": \"SGD_{lr=1.0}_MNISTNet_{}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=3.0}_{batch_size=8}\": \"SGD_{lr=3.0}_MNISTNet_{}_MNIST_{batch_size=8}\",\n",
    "\n",
    "    # \"SGD_{lr=0.001}_{batch_size=16}\": \"SGD_{lr=0.001}_MNISTNet_{}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=0.01}_{batch_size=16}\": \"SGD_{lr=0.01}_MNISTNet_{}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=0.1}_{batch_size=16}\": \"SGD_{lr=0.1}_MNISTNet_{}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=1.0}_{batch_size=16}\": \"SGD_{lr=1.0}_MNISTNet_{}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=3.0}_{batch_size=16}\": \"SGD_{lr=3.0}_MNISTNet_{}_MNIST_{batch_size=16}\",\n",
    "\n",
    "    # \"SGD_{lr=0.001}_{batch_size=32}\": \"SGD_{lr=0.001}_MNISTNet_{}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=0.01}_{batch_size=32}\": \"SGD_{lr=0.01}_MNISTNet_{}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=0.1}_{batch_size=32}\": \"SGD_{lr=0.1}_MNISTNet_{}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=1.0}_{batch_size=32}\": \"SGD_{lr=1.0}_MNISTNet_{}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=3.0}_{batch_size=32}\": \"SGD_{lr=3.0}_MNISTNet_{}_MNIST_{batch_size=32}\",\n",
    "\n",
    "    ### MNISTLeakyRelu\n",
    "    # \"SGD_{lr=0.001}_{batch_size=8}\": \"SGD_{lr=0.001}_MNISTLeakyRelu_{}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=0.01}_{batch_size=8}\": \"SGD_{lr=0.01}_MNISTLeakyRelu_{}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=0.1}_{batch_size=8}\": \"SGD_{lr=0.1}_MNISTLeakyRelu_{}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=1.0}_{batch_size=8}\": \"SGD_{lr=1.0}_MNISTLeakyRelu_{}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=3.0}_{batch_size=8}\": \"SGD_{lr=3.0}_MNISTLeakyRelu_{}_MNIST_{batch_size=8}\",\n",
    "\n",
    "    # \"SGD_{lr=0.001}_{batch_size=16}\": \"SGD_{lr=0.001}_MNISTLeakyRelu_{}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=0.01}_{batch_size=16}\": \"SGD_{lr=0.01}_MNISTLeakyRelu_{}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=0.1}_{batch_size=16}\": \"SGD_{lr=0.1}_MNISTLeakyRelu_{}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=1.0}_{batch_size=16}\": \"SGD_{lr=1.0}_MNISTLeakyRelu_{}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=3.0}_{batch_size=16}\": \"SGD_{lr=3.0}_MNISTLeakyRelu_{}_MNIST_{batch_size=16}\",\n",
    "\n",
    "    # \"SGD_{lr=0.001}_{batch_size=32}\": \"SGD_{lr=0.001}_MNISTLeakyRelu_{}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=0.01}_{batch_size=32}\": \"SGD_{lr=0.01}_MNISTLeakyRelu_{}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=0.1}_{batch_size=32}\": \"SGD_{lr=0.1}_MNISTLeakyRelu_{}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=1.0}_{batch_size=32}\": \"SGD_{lr=1.0}_MNISTLeakyRelu_{}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=3.0}_{batch_size=32}\": \"SGD_{lr=3.0}_MNISTLeakyRelu_{}_MNIST_{batch_size=32}\",\n",
    "\n",
    "    ### MNISTReluBatchNorm\n",
    "    # \"SGD_{lr=0.001}_{batch_size=8}\": \"SGD_{lr=0.001}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=0.01}_{batch_size=8}\": \"SGD_{lr=0.01}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=0.1}_{batch_size=8}\": \"SGD_{lr=0.1}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=1.0}_{batch_size=8}\": \"SGD_{lr=1.0}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=8}\",\n",
    "    # \"SGD_{lr=3.0}_{batch_size=8}\": \"SGD_{lr=3.0}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=8}\",\n",
    "\n",
    "    # \"SGD_{lr=0.001}_{batch_size=16}\": \"SGD_{lr=0.001}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=0.01}_{batch_size=16}\": \"SGD_{lr=0.01}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=0.1}_{batch_size=16}\": \"SGD_{lr=0.1}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=1.0}_{batch_size=16}\": \"SGD_{lr=1.0}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=16}\",\n",
    "    # \"SGD_{lr=3.0}_{batch_size=16}\": \"SGD_{lr=3.0}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=16}\",\n",
    "\n",
    "    # \"SGD_{lr=0.001}_{batch_size=32}\": \"SGD_{lr=0.001}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=0.01}_{batch_size=32}\": \"SGD_{lr=0.01}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=0.1}_{batch_size=32}\": \"SGD_{lr=0.1}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=1.0}_{batch_size=32}\": \"SGD_{lr=1.0}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=32}\",\n",
    "    # \"SGD_{lr=3.0}_{batch_size=32}\": \"SGD_{lr=3.0}_MNISTReluBatchNorm_{affine=True_track_running_stats=True}_MNIST_{batch_size=32}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load l2os from disk\n",
    "for l2o_name, l2o_dir in l2os.items():\n",
    "    ### load final l2o checkpoint\n",
    "    ckpt = torch.load(os.path.join(os.environ[\"CKPT_PATH\"], l2o_dir, \"l2o_optimizer.pt\"), map_location=\"cpu\")\n",
    "\n",
    "    ### load all metrics\n",
    "    l2o_metrics = {}\n",
    "    for metrics_file in [f_name for f_name in os.listdir(os.path.join(os.environ[\"CKPT_PATH\"], ckpt[\"config\"][\"ckpt_base_dir\"])) if f_name.startswith(\"metrics_\")]:\n",
    "        metrics_name = metrics_file[8:-4] # remove the \"metrics_\" prefix and \".npy\" suffix\n",
    "        l2o_metrics[metrics_name] = np.load(os.path.join(os.environ[\"CKPT_PATH\"], ckpt[\"config\"][\"ckpt_base_dir\"], metrics_file), allow_pickle=True).item()\n",
    "    l2os[l2o_name] = {\n",
    "        \"ckpt\": ckpt,\n",
    "        \"config\": ckpt[\"config\"],\n",
    "        \"metrics\": l2o_metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load baselines from disk\n",
    "baselines_root_dir = os.path.join(os.environ[\"CKPT_PATH\"], \"baselines\")\n",
    "for baseline_name, baseline_dir in baselines.items():\n",
    "    ### load config\n",
    "    config = torch.load(os.path.join(baselines_root_dir, baseline_dir, \"config.pt\"), map_location=\"cpu\")\n",
    "    if \"baseline_opter_cls\" not in config:\n",
    "        if \"sgd\" in baseline_name.lower():\n",
    "            config[\"meta_testing\"][\"baseline_opter_cls\"] = optim.SGD\n",
    "        elif \"adam\" in baseline_name.lower():\n",
    "            config[\"meta_testing\"][\"baseline_opter_cls\"] = optim.Adam\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    ### load metrics\n",
    "    metrics = np.load(os.path.join(baselines_root_dir, baseline_dir, \"metrics.npy\"), allow_pickle=True).item()\n",
    "    baselines[baseline_name] = {\n",
    "        \"baseline_dir\": os.path.join(baselines_root_dir, baseline_dir),\n",
    "        \"config\": config,\n",
    "        # \"baseline_config\": baseline_config,\n",
    "        \"metrics\": metrics,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(\n",
    "    run_nickname,\n",
    "    show_max_iters,\n",
    "    log_loss=False,\n",
    "    save_fig_to_path=None,\n",
    "    with_err_bars=False,\n",
    "    conv_window=1,\n",
    "):\n",
    "    ### plot comparison\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ### baseline optimizers\n",
    "    for baseline_name, baseline_dict in baselines.items():\n",
    "        opter_metrics = baseline_dict[\"metrics\"]\n",
    "        config = baseline_dict[\"config\"]\n",
    "        if \"test\" in metric:\n",
    "            x = np.arange(config[\"meta_training\"][\"eval_iter_freq\"], show_max_iters + 1, config[\"meta_training\"][\"eval_iter_freq\"])\n",
    "            y = np.mean(opter_metrics[metric][:,:show_max_iters // 10], axis=0)\n",
    "        else:\n",
    "            x = range(opter_metrics[metric][:,:show_max_iters].shape[1])\n",
    "            y = np.mean(opter_metrics[metric][:,:show_max_iters], axis=0)\n",
    "        if conv_window > 1:\n",
    "            y = np.convolve(y, np.ones(conv_window), \"valid\") / conv_window\n",
    "            x = x[conv_window - 1:]\n",
    "        sns.lineplot(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            label=baseline_name,\n",
    "            linestyle=\"--\",\n",
    "            ax=ax,\n",
    "        )\n",
    "        \n",
    "        if with_err_bars:\n",
    "            if \"test\" in metric:\n",
    "                y_std = np.std(opter_metrics[metric][:,:show_max_iters // 10], axis=0)\n",
    "            else:\n",
    "                y_std = np.std(opter_metrics[metric][:,:show_max_iters], axis=0)\n",
    "            if conv_window > 1:\n",
    "                y_std = np.convolve(y_std, np.ones(conv_window), \"valid\") / conv_window\n",
    "                x = x[conv_window - 1:]\n",
    "            ax.fill_between(\n",
    "                x,\n",
    "                y - y_std,\n",
    "                y + y_std,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "            \n",
    "\n",
    "    ### L2O optimizers\n",
    "    for l2o_name, l2o_dict in l2os.items():\n",
    "        metrics = l2o_dict[\"metrics\"][run_nickname]\n",
    "        config = l2o_dict[\"config\"]\n",
    "\n",
    "        if \"test\" in metric:\n",
    "            x = np.arange(config[\"meta_training\"][\"eval_iter_freq\"], show_max_iters + 1, config[\"meta_training\"][\"eval_iter_freq\"])\n",
    "            y = np.mean(metrics[metric][:,:show_max_iters // config[\"meta_training\"][\"eval_iter_freq\"]], axis=0)\n",
    "        else:\n",
    "            x = range(metrics[metric][:,:show_max_iters].shape[1])\n",
    "            y = np.mean(metrics[metric][:,:show_max_iters], axis=0)\n",
    "        if conv_window > 1:\n",
    "            y = np.convolve(y, np.ones(conv_window), \"valid\") / conv_window\n",
    "            x = x[conv_window - 1:]\n",
    "        sns.lineplot(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            label=f\"{l2o_name}\",\n",
    "            # label=fr\"{l2o_name}, $\\beta$={config['meta_training']['reg_mul']}\",\n",
    "            # linewidth=1,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        if with_err_bars:\n",
    "            if \"test\" in metric:\n",
    "                y_std = np.std(metrics[metric][:,:show_max_iters // config[\"meta_training\"][\"eval_iter_freq\"]], axis=0)\n",
    "            else:\n",
    "                y_std = np.std(metrics[metric][:,:show_max_iters], axis=0)\n",
    "            if conv_window > 1:\n",
    "                y_std = np.convolve(y_std, np.ones(conv_window), \"valid\") / conv_window\n",
    "\n",
    "            ax.fill_between(\n",
    "                x,\n",
    "                y - y_std,\n",
    "                y + y_std,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "\n",
    "    ### plot settings\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    if metric == \"train_loss\":\n",
    "        metric_as_label = \"Train Loss\"\n",
    "    elif metric == \"test_loss\":\n",
    "        metric_as_label = \"Test Loss\"\n",
    "    elif metric == \"train_acc\":\n",
    "        metric_as_label = \"Train Accuracy\"\n",
    "    elif metric == \"test_acc\":\n",
    "        metric_as_label = \"Test Accuracy\"\n",
    "    else:\n",
    "        metric_as_label = metric\n",
    "    ax.set_ylabel(metric_as_label)\n",
    "\n",
    "    # set y to log scale\n",
    "    if log_loss and \"loss\" in metric:\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "    if \"acc\" in metric:\n",
    "        ax.set_ylim(0.6, 1.0)\n",
    "    elif log_loss is not True:\n",
    "        ax.set_ylim(0.0, None)\n",
    "\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=3)\n",
    "    legend = ax.get_legend()\n",
    "    for legend_handle in legend.legendHandles:\n",
    "        legend_handle.set_linewidth(3.0)\n",
    "\n",
    "    # x-ticks\n",
    "    x_ticks = ax.get_xticks()\n",
    "    x_ticks = np.linspace(0, show_max_iters, 3)\n",
    "    ax.set_xticks(x_ticks)\n",
    "\n",
    "    ### y-ticks\n",
    "    if log_loss is not True:\n",
    "        y_max = ax.get_ylim()[1]\n",
    "        y_max = np.ceil(y_max / 0.5) * 0.5 # round\n",
    "        y_ticks = np.linspace(0, y_max, 3)\n",
    "        ax.set_yticks(y_ticks)\n",
    "    else:\n",
    "        # y_ticks = ax.get_yticks()\n",
    "        # y_ticks = np.linspace(y_ticks[1], y_ticks[-2], 2)\n",
    "        # ax.set_yticks(y_ticks)\n",
    "        ax.set_yticks([1e0, 1e-1])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ### save the figure\n",
    "    if save_fig_to_path is not None:\n",
    "        fig.savefig(save_fig_to_path, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "show_max_iters = 500\n",
    "log_loss = True\n",
    "metric = \"train_loss\" # [\"train_loss\", \"test_loss\", \"train_acc\", \"test_acc\"]\n",
    "conv_window = 1\n",
    "with_err_bars = True\n",
    "\n",
    "### where to save the figure\n",
    "optee_cls = MNISTReluBatchNorm\n",
    "optee_config = {}\n",
    "optee_config = {\"affine\": True, \"track_running_stats\": True}\n",
    "# optee_config = {\"layer_sizes\": [100,100]}\n",
    "data_cls = MNIST\n",
    "data_config = {\"batch_size\": 128}\n",
    "optee_nickname = f\"{optee_cls.__name__}_{dict_to_str(optee_config)}\"\n",
    "run_nickname = f\"{optee_nickname}_{data_cls.__name__}_{dict_to_str(data_config)}\"\n",
    "\n",
    "fig_dir = \"../results/publication/reg_comparison\"\n",
    "fig_name = f\"{metric}_comparison_{optee_nickname}_{show_max_iters}.pdf\"\n",
    "if log_loss is True:\n",
    "    fig_name = f\"log_{fig_name}\"\n",
    "save_fig_to_path = os.path.join(fig_dir, fig_name)\n",
    "save_fig_to_path = None # don't save\n",
    "\n",
    "print(f\"Final destination: {save_fig_to_path if save_fig_to_path is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(\n",
    "    run_nickname=run_nickname,\n",
    "    show_max_iters=show_max_iters,\n",
    "    log_loss=log_loss,\n",
    "    save_fig_to_path=save_fig_to_path,\n",
    "    with_err_bars=with_err_bars,\n",
    "    conv_window=conv_window,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params, Grads, Updates\n",
    "Plot the norm and mean abs value of parameters, gradients, and parameter updates across training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect\n",
    "### baseline optimizers\n",
    "for baseline_name in baselines.keys():\n",
    "    baselines[baseline_name][\"params_grads\"] = {k: {} for k in [\"param_norms\", \"param_abs_means\", \"grad_norms\", \"grad_abs_means\", \"updates_norms\", \"updates_abs_means\"]}\n",
    "    opter_metrics = baselines[baseline_name][\"metrics\"]\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "\n",
    "    for iter_i in range(config[\"meta_testing\"][\"ckpt_iter_freq\"], config[\"meta_testing\"][\"n_iters\"] + 1, config[\"meta_testing\"][\"ckpt_iter_freq\"]):\n",
    "        ckpt_dir = os.path.join(os.environ[\"CKPT_PATH\"], \"baselines\", baselines[baseline_name][\"baseline_dir\"], \"ckpt\")\n",
    "        optee, opter, optee_grads, loss_history = load_baseline_opter_ckpt(\n",
    "            path=os.path.join(ckpt_dir, f\"run0_{iter_i}.pt\"),\n",
    "            optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "            opter_cls=config[\"meta_testing\"][\"baseline_opter_cls\"],\n",
    "            optee_config=config[\"meta_testing\"][\"optee_config\"],\n",
    "            opter_config=config[\"meta_testing\"][\"baseline_opter_config\"]\n",
    "        )\n",
    "        optee_updates = get_baseline_opter_param_updates(optee, opter)\n",
    "    \n",
    "        for n, p in optee.all_named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            for k in baselines[baseline_name][\"params_grads\"]:\n",
    "                if n not in baselines[baseline_name][\"params_grads\"][k]:\n",
    "                    baselines[baseline_name][\"params_grads\"][k][n] = []\n",
    "            baselines[baseline_name][\"params_grads\"][\"param_norms\"][n].append(p.norm().item())\n",
    "            baselines[baseline_name][\"params_grads\"][\"param_abs_means\"][n].append(p.abs().mean().item())\n",
    "            baselines[baseline_name][\"params_grads\"][\"grad_norms\"][n].append(p.grad.norm().item())\n",
    "            baselines[baseline_name][\"params_grads\"][\"grad_abs_means\"][n].append(p.grad.abs().mean().item())\n",
    "            baselines[baseline_name][\"params_grads\"][\"updates_norms\"][n].append(optee_updates[n].norm().item())\n",
    "            baselines[baseline_name][\"params_grads\"][\"updates_abs_means\"][n].append(optee_updates[n].abs().mean().item())\n",
    "\n",
    "### l2o optimizers\n",
    "for l2o_name in l2os:\n",
    "    config = l2os[l2o_name][\"config\"]\n",
    "    l2os[l2o_name][\"params_grads\"] = {k: {} for k in [\"param_norms\", \"param_abs_means\", \"grad_norms\", \"grad_abs_means\", \"updates_norms\", \"updates_abs_means\"]}\n",
    "    \n",
    "    for iter_i in range(config[\"meta_testing\"][\"ckpt_iter_freq\"], config[\"meta_testing\"][\"n_iters\"] + 1, config[\"meta_testing\"][\"ckpt_iter_freq\"]):\n",
    "        # load L2O optimizer\n",
    "        optee, opter, optee_grads, optee_updates, loss_history = load_l2o_opter_ckpt(\n",
    "            path=os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"], f\"{iter_i}.pt\"),\n",
    "            optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "            opter_cls=config[\"opter_cls\"],\n",
    "            optee_config=config[\"meta_testing\"][\"optee_config\"],\n",
    "            opter_config=config[\"opter_config\"],\n",
    "        )\n",
    "        # scale updates by optee update lr\n",
    "        optee_updates = {n: p * config[\"meta_testing\"][\"optee_updates_lr\"] for n, p in optee_updates.items()}\n",
    "\n",
    "        for n, p in optee.all_named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            for k in l2os[l2o_name][\"params_grads\"]:\n",
    "                if n not in l2os[l2o_name][\"params_grads\"][k]:\n",
    "                    l2os[l2o_name][\"params_grads\"][k][n] = []\n",
    "            l2os[l2o_name][\"params_grads\"][\"param_norms\"][n].append(p.norm().item())\n",
    "            l2os[l2o_name][\"params_grads\"][\"param_abs_means\"][n].append(p.abs().mean().item())\n",
    "            l2os[l2o_name][\"params_grads\"][\"grad_norms\"][n].append(p.grad.norm().item())\n",
    "            l2os[l2o_name][\"params_grads\"][\"grad_abs_means\"][n].append(p.grad.abs().mean().item())\n",
    "            l2os[l2o_name][\"params_grads\"][\"updates_norms\"][n].append(optee_updates[n].norm().item())\n",
    "            l2os[l2o_name][\"params_grads\"][\"updates_abs_means\"][n].append(optee_updates[n].abs().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = \"grad_norms\"\n",
    "max_len = 500\n",
    "\n",
    "fig = plt.figure(figsize=(22, 20))\n",
    "x_ticks = np.arange(5, max_len + 1, 5)\n",
    "\n",
    "def smooth(y, box_pts=10):\n",
    "    box = np.ones(box_pts) / box_pts\n",
    "    y_smooth = np.convolve(y, box, mode=\"same\")\n",
    "    return y_smooth\n",
    "\n",
    "for i, n in enumerate(baselines[\"SGD\"][\"params_grads\"][k]):\n",
    "    ax = fig.add_subplot(3, 2, i + 1)\n",
    "\n",
    "    for baseline_name, baseline_dict in baselines.items():\n",
    "        if n not in baseline_dict[\"params_grads\"][k]:\n",
    "            continue\n",
    "        sns.lineplot(x=x_ticks, y=smooth(baseline_dict[\"params_grads\"][k][n][:max_len]), alpha=0.8, linewidth=1.5, linestyle=\"--\", ax=ax, label=f\"{baseline_name}\")\n",
    "    for l2o_name, l2o_dict in l2os.items():\n",
    "        if n not in l2o_dict[\"params_grads\"][k]:\n",
    "            continue\n",
    "        sns.lineplot(x=x_ticks, y=smooth(l2o_dict[\"params_grads\"][k][n][:max_len]), alpha=0.8, linewidth=1.5, ax=ax, label=f\"{l2o_name}\")\n",
    "\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_title(k.replace(\"_\", \" \").title() + \": \" + n, fontsize=13, fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "### save the figure\n",
    "fig.savefig(os.path.join(f\"params_grads_updates_{k}.png\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conservation Law Breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = \"meta_testing\"\n",
    "max_iters = 500\n",
    "reg_func = regularize_translation_conservation_law_breaking\n",
    "\n",
    "### collect deviations\n",
    "for l2o_name in l2os:\n",
    "    config = l2os[l2o_name][\"config\"]\n",
    "    l2os[l2o_name][reg_func.__name__] = np.array(collect_conservation_law_deviations(\n",
    "        func=reg_func,\n",
    "        opter_cls=Optimizer,\n",
    "        opter_config=config[\"opter_config\"],\n",
    "        optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "        optee_config=config[\"meta_testing\"][\"optee_config\"],\n",
    "        ckpt_iter_freq=config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "        n_iters=config[\"meta_testing\"][\"n_iters\"],\n",
    "        ckpt_path_prefix=os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"], \"\"),\n",
    "        is_l2o=True,\n",
    "        max_iters=max_iters,\n",
    "    ))\n",
    "\n",
    "### Baseline optimizers\n",
    "for baseline_name in baselines:\n",
    "    if \"baseline_opter_cls\" in baselines[baseline_name][\"config\"]:\n",
    "        baseline_opter_cls = baselines[baseline_name][\"config\"][\"baseline_opter_cls\"]\n",
    "    elif \"sgd\" in baseline_name.lower():\n",
    "        baseline_opter_cls = optim.SGD\n",
    "    elif \"adam\" in baseline_name.lower():\n",
    "        baseline_opter_cls = optim.Adam\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "    ### collect deviations\n",
    "    baselines[baseline_name][reg_func.__name__] = np.array(collect_conservation_law_deviations(\n",
    "        func=reg_func,\n",
    "        opter_cls=config[\"meta_testing\"][\"baseline_opter_config\"],\n",
    "        opter_config=config[\"meta_testing\"][\"baseline_opter_config\"],\n",
    "        optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "        optee_config=baselines[baseline_name][\"config\"][\"meta_testing\"][\"optee_config\"],\n",
    "        ckpt_iter_freq=baselines[baseline_name][\"config\"][\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "        n_iters=baselines[baseline_name][\"config\"][\"meta_testing\"][\"n_iters\"],\n",
    "        ckpt_path_prefix=os.path.join(os.environ[\"CKPT_PATH\"], baselines[baseline_name][\"baseline_dir\"], \"ckpt/\"),\n",
    "        is_l2o=False,\n",
    "        max_iters=max_iters,\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking Geometric Constraints on Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = \"meta_testing\"\n",
    "max_iters = None\n",
    "collect_func = collect_translation_sym_deviations\n",
    "\n",
    "### L2O optimizers\n",
    "for l2o_name in l2os:\n",
    "    config = l2os[l2o_name][\"config\"]\n",
    "    l2os[l2o_name][collect_func.__name__ + \"_grads\"] = []\n",
    "    l2os[l2o_name][collect_func.__name__ + \"_updates\"] = []\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        grad_deviations, param_update_deviations = collect_func(\n",
    "            ckpt_iter_freq=config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "            n_iters=config[\"meta_testing\"][\"n_iters\"],\n",
    "            optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "            opter_cls=config[\"opter_cls\"],\n",
    "            optee_config=config[\"meta_testing\"][\"optee_config\"],\n",
    "            opter_config=config[\"opter_config\"],\n",
    "            phase=\"meta_testing\",\n",
    "            ckpt_path_prefix=os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"], f\"run{test_run_i}_\"),\n",
    "            max_iters=max_iters,\n",
    "        )\n",
    "        if np.ndim(grad_deviations) == 2:\n",
    "            ### sum the deviations (weight and bias)\n",
    "            grad_deviations = grad_deviations.sum(-1)\n",
    "            param_update_deviations = param_update_deviations.sum(-1)\n",
    "        l2os[l2o_name][collect_func.__name__ + \"_grads\"].append(grad_deviations)\n",
    "        l2os[l2o_name][collect_func.__name__ + \"_updates\"].append(param_update_deviations)\n",
    "\n",
    "### Baseline optimizers\n",
    "for baseline_name in baselines:\n",
    "    if \"baseline_opter_cls\" in baselines[baseline_name][\"config\"]:\n",
    "        baseline_opter_cls = baselines[baseline_name][\"config\"][\"baseline_opter_cls\"]\n",
    "    elif \"sgd\" in baseline_name.lower():\n",
    "        baseline_opter_cls = optim.SGD\n",
    "    elif \"adam\" in baseline_name.lower():\n",
    "        baseline_opter_cls = optim.Adam\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "    baseline_opter_config = baselines[baseline_name][\"config\"][\"meta_testing\"][\"baseline_opter_config\"]\n",
    "    \n",
    "    ### collect deviations\n",
    "    baselines[baseline_name][collect_func.__name__ + \"_grads\"] = []\n",
    "    baselines[baseline_name][collect_func.__name__ + \"_updates\"] = []\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        grad_deviations, param_update_deviations = collect_func(\n",
    "            ckpt_iter_freq=config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "            n_iters=config[\"meta_testing\"][\"n_iters\"],\n",
    "            optee_cls=config[\"meta_testing\"][\"optee_cls\"],\n",
    "            optee_config=config[\"meta_testing\"][\"optee_config\"],\n",
    "            opter_cls=baseline_opter_cls,\n",
    "            opter_config=baseline_opter_config,\n",
    "            phase=\"meta_testing\",\n",
    "            ckpt_path_prefix=os.path.join(baselines[baseline_name][\"baseline_dir\"], f\"ckpt/run{test_run_i}_\"),\n",
    "            max_iters=max_iters,\n",
    "        )\n",
    "        if np.ndim(grad_deviations) == 2:\n",
    "            ### sum the deviations (weight and bias)\n",
    "            grad_deviations = grad_deviations.sum(-1)\n",
    "            param_update_deviations =  param_update_deviations.sum(-1)\n",
    "        baselines[baseline_name][collect_func.__name__ + \"_grads\"].append(grad_deviations)\n",
    "        baselines[baseline_name][collect_func.__name__ + \"_updates\"].append(param_update_deviations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting - Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deviations(dict_key_to_plot, log=False, abs_values=False, max_iters=None, save_fig_to_path=None):\n",
    "    ### plot comparison\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    x_ticks = np.arange(\n",
    "        0,\n",
    "        min(max_iters + 1, config[\"meta_testing\"][\"n_iters\"] + 1) if max_iters != None else config[\"meta_testing\"][\"n_iters\"] + 1,\n",
    "        config[\"meta_testing\"][\"ckpt_iter_freq\"]\n",
    "    )\n",
    "\n",
    "    ### L2O optimizers\n",
    "    for l2o_name, l2o_dict in l2os.items():\n",
    "        to_plot = np.array(\n",
    "            l2o_dict[dict_key_to_plot] if not abs_values else np.abs(l2o_dict[dict_key_to_plot])\n",
    "        )[:, :len(x_ticks)]\n",
    "        sns.lineplot(\n",
    "            x=x_ticks,\n",
    "            y=to_plot.mean(0),\n",
    "            label=l2o_name,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=to_plot.mean(0) - to_plot.std(0),\n",
    "            y2=to_plot.mean(0) + to_plot.std(0),\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "    ### baseline optimizers\n",
    "    for baseline_name, baseline_dict in baselines.items():\n",
    "        to_plot = np.array(\n",
    "            baseline_dict[dict_key_to_plot] if not abs_values else np.abs(baseline_dict[dict_key_to_plot])\n",
    "        )[:, :len(x_ticks)]\n",
    "        sns.lineplot(\n",
    "            x=x_ticks,\n",
    "            y=to_plot.mean(0),\n",
    "            label=baseline_name,\n",
    "            linestyle=\"--\",\n",
    "            linewidth=1.5,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=to_plot.mean(0) - to_plot.std(0),\n",
    "            y2=to_plot.mean(0) + to_plot.std(0),\n",
    "            alpha=0.2,\n",
    "        )\n",
    "\n",
    "    ### plot settings\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Deviation\" if not abs_values else \"Absolute Deviation\")\n",
    "\n",
    "    # set y to log scale\n",
    "    if log:\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "    # ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), ncol=2)\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    legend = ax.get_legend()\n",
    "    for legend_handle in legend.legendHandles:\n",
    "        legend_handle.set_linewidth(3.0)\n",
    "\n",
    "    # x-ticks\n",
    "    x_ticks = ax.get_xticks()\n",
    "    x_ticks = np.linspace(0, max_iters, 3)\n",
    "    ax.set_xticks(x_ticks)\n",
    "    \n",
    "    # y-ticks\n",
    "    # y_max = ax.get_ylim()[1]\n",
    "    # y_max = np.ceil(y_max / 100) * 100 # round\n",
    "    # y_ticks = np.linspace(0, y_max, 4)\n",
    "    # ax.set_yticks(y_ticks)\n",
    "    # y_ticks = ax.get_yticks()\n",
    "    # ax.set_yticks(y_ticks[::3])\n",
    "    # ax.set_ylim(0, 90)\n",
    "    # ax.set_yticks([0, 45, 90])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    ### save the figure\n",
    "    if save_fig_to_path is not None:\n",
    "        fig.savefig(save_fig_to_path, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_plot = collect_func.__name__ + \"_updates\"\n",
    "# key_to_plot = reg_func.__name__\n",
    "save_fig_to_path = os.path.join(\n",
    "    \"..\",\n",
    "    \"results\",\n",
    "    \"publication\",\n",
    "    \"constraint_deviations\",\n",
    "    \"translation_sym_meta_testing_MNISTNet_{}_200.pdf\"\n",
    ")\n",
    "save_fig_to_path = None\n",
    "\n",
    "plot_deviations(\n",
    "    key_to_plot,\n",
    "    log=False,\n",
    "    abs_values=True,\n",
    "    max_iters=200,\n",
    "    save_fig_to_path=save_fig_to_path,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavy-tail gradient/update noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(optee, data_loader, l2o_opter=None, hidden_states=None, cell_states=None):\n",
    "    ### calculate gradient (and update) noise over the the given data loader\n",
    "    optee.eval()\n",
    "    tmp_optee_optim = optim.SGD(optee.parameters(), lr=0.0) # just for zeroing out the gradients\n",
    "    grads = []\n",
    "    l2o_updates = []\n",
    "    losses = []\n",
    "    accs = []\n",
    "    n_minibatches = 0\n",
    "    for i, (x, y) in enumerate(data_loader):\n",
    "        n_minibatches += 1\n",
    "        tmp_optee_optim.zero_grad()\n",
    "        \n",
    "        x, y = x.view(-1, 784).cuda(), y.cuda()\n",
    "        loss, acc = optee(inp=x, out=y, return_acc=True)\n",
    "        loss.backward()\n",
    "\n",
    "        ### collect gradients\n",
    "        grads.append(torch.cat([p.grad.detach().view(-1) for n, p in optee.all_named_parameters() if p.requires_grad]).cpu())\n",
    "\n",
    "        ### collect l2o updates\n",
    "        if l2o_opter is not None:\n",
    "            curr_l2o_updates = []\n",
    "            offset = 0\n",
    "            for name, p in optee.all_named_parameters():\n",
    "                if p.requires_grad == False: # batchnorm stats\n",
    "                    continue\n",
    "\n",
    "                cur_sz = int(np.prod(p.size()))\n",
    "                gradients = p.grad.detach().view(cur_sz, 1)\n",
    "                updates, _, _ = l2o_opter(\n",
    "                    optee_grads=gradients,\n",
    "                    hidden=[h[offset : offset + cur_sz] for h in hidden_states],\n",
    "                    cell=[c[offset : offset + cur_sz] for c in cell_states],\n",
    "                    additional_inp=None,\n",
    "                )\n",
    "                offset += cur_sz\n",
    "                curr_l2o_updates.append(updates.detach().view(-1))\n",
    "            l2o_updates.append(torch.cat(curr_l2o_updates).cpu())\n",
    "\n",
    "        ### track history\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc.item())\n",
    "\n",
    "    optee_total_params = len(grads[0])\n",
    "\n",
    "    ### gradients\n",
    "    grads = torch.stack(grads, dim=0) # (n_minibatches, optee_total_params)\n",
    "    mean_grad = grads.mean(dim=0) # (optee_total_params,)\n",
    "    grads_noise_norm = (grads - mean_grad).norm(dim=1) # (n_minibatches,)\n",
    "    # get the tail index alpha\n",
    "    N = optee_total_params * n_minibatches\n",
    "    for i in range(1, 1 + int(np.sqrt(N))):\n",
    "        if N % i == 0:\n",
    "            m = i\n",
    "    grads_alpha = alpha_estimator(m, (grads - mean_grad).view(-1, 1))\n",
    "\n",
    "    ### l2o updates\n",
    "    l2o_updates_noise_norm, l2o_updates_alpha = None, None\n",
    "    if l2o_opter is not None:\n",
    "        l2o_updates = torch.stack(l2o_updates, dim=0) # (n_minibatches, optee_total_params)\n",
    "        mean_l2o_updates = l2o_updates.mean(dim=0) # (optee_total_params,)\n",
    "        l2o_updates_noise_norm = (l2o_updates - mean_l2o_updates).norm(dim=1) # (n_minibatches,)\n",
    "        # get the tail index alpha\n",
    "        N = optee_total_params * n_minibatches\n",
    "        for i in range(1, 1 + int(np.sqrt(N))):\n",
    "            if N % i == 0:\n",
    "                m = i\n",
    "        l2o_updates_alpha = alpha_estimator(m, (l2o_updates - mean_l2o_updates).view(-1, 1))\n",
    "\n",
    "    return (\n",
    "        losses,\n",
    "        accs,\n",
    "        grads_noise_norm,\n",
    "        grads_alpha,\n",
    "        l2o_updates_noise_norm,\n",
    "        l2o_updates_alpha,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_grad_update_noise_for_l2o(ckpt_path, run_history, config, train_data, test_data):\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    optee = config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"]).cuda()\n",
    "    optee.load_state_dict(ckpt[\"optimizee\"])\n",
    "\n",
    "    ### init l2o optimizer to collect noise in its updates\n",
    "    opter = config[\"opter_cls\"](**config[\"opter_config\"]).cuda()\n",
    "    opter.load_state_dict(ckpt[\"optimizer\"])\n",
    "\n",
    "    ### collect history\n",
    "    for phase, data_loader in ((\"train\", train_data.loader), (\"test\", test_data.loader)):\n",
    "        losses, accs, grads_noise_norm, grads_alpha, l2o_updates_noise_norm, l2o_updates_alpha = eval_metrics(\n",
    "            optee=optee,\n",
    "            data_loader=data_loader,\n",
    "            l2o_opter=opter,\n",
    "            hidden_states=ckpt[\"hidden_states\"],\n",
    "            cell_states=ckpt[\"cell_states\"]\n",
    "        )\n",
    "        run_history[phase][\"loss\"].append(np.mean(losses))\n",
    "        run_history[phase][\"acc\"].append(np.mean(accs))\n",
    "        run_history[phase][\"grads_noise_norm\"].append(grads_noise_norm)\n",
    "        run_history[phase][\"grads_alpha\"].append(grads_alpha.item())\n",
    "        run_history[phase][\"l2o_updates_noise_norm\"].append(l2o_updates_noise_norm)\n",
    "        run_history[phase][\"l2o_updates_alpha\"].append(l2o_updates_alpha.item())\n",
    "    \n",
    "    return run_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing = True\n",
    "\n",
    "### collect for L2Os\n",
    "for opter_name in l2os:\n",
    "    print(f\"Running {opter_name}...\")\n",
    "    config = l2os[opter_name][\"config\"]\n",
    "    ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"])\n",
    "    save_run_history_path = os.path.join(\n",
    "        os.environ[\"CKPT_PATH\"],\n",
    "        config[\"ckpt_base_dir\"],\n",
    "        f\"grads_updates_noise_heavy_tail_alpha_estimates\" +\n",
    "            f\"_{config['meta_testing']['optee_cls'].__name__}_{dict_to_str(config['meta_testing']['optee_config'])}\" +\n",
    "            f\"_{config['meta_testing']['data_cls'].__name__}_{dict_to_str(config['meta_testing']['data_config'])}\" +\n",
    "            f\"_{config['eval_n_tests']}_tests.pt\"\n",
    "    )\n",
    "\n",
    "    if load_existing and os.path.exists(save_run_history_path):\n",
    "        print(f\"  Loading existing run history from {save_run_history_path}\")\n",
    "        run_history = torch.load(save_run_history_path)\n",
    "        l2os[opter_name][\"run_history\"] = run_history\n",
    "        continue\n",
    "\n",
    "    run_history_all_test_runs = []\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        print(f\"  Test run {test_run_i}...\")\n",
    "        ckpt_prefix = f\"run{test_run_i}_\"\n",
    "    \n",
    "        ### collect\n",
    "        run_history = {k: {k: [] for k in (\"loss\", \"acc\", \"grads_noise_norm\", \"grads_alpha\", \"l2o_updates_noise_norm\", \"l2o_updates_alpha\")}\n",
    "            for k in (\"train\", \"test\")}\n",
    "        train_data = MNIST(training=True, batch_size=128)\n",
    "        test_data = MNIST(training=False, batch_size=128)\n",
    "\n",
    "        ### the very first checkpoint\n",
    "        print(f\"  [{1}/{config['meta_testing']['n_iters']}]\")\n",
    "        ckpt_path = os.path.join(ckpts_dir, f\"{ckpt_prefix}1.pt\")\n",
    "        if os.path.exists(ckpt_path):\n",
    "            run_history = collect_grad_update_noise_for_l2o(\n",
    "                ckpt_path=ckpt_path,\n",
    "                run_history=run_history,\n",
    "                config=config,\n",
    "                train_data=train_data,\n",
    "                test_data=test_data\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  {ckpt_path} does not exist, skipping.\")\n",
    "\n",
    "        ### the rest of the checkpoints\n",
    "        for iter_i in range(\n",
    "            config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "            config[\"meta_testing\"][\"n_iters\"] + 1,\n",
    "            config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "        ):\n",
    "            print(f\"  [{iter_i}/{config['meta_testing']['n_iters']}]\")\n",
    "\n",
    "            ### load checkpoint\n",
    "            ckpt_path = os.path.join(ckpts_dir, f\"{ckpt_prefix}{iter_i}.pt\")\n",
    "            run_history = collect_grad_update_noise_for_l2o(\n",
    "                ckpt_path=ckpt_path,\n",
    "                run_history=run_history,\n",
    "                config=config,\n",
    "                train_data=train_data,\n",
    "                test_data=test_data\n",
    "            )\n",
    "\n",
    "        ### save current test run\n",
    "        run_history_all_test_runs.append(run_history)\n",
    "\n",
    "    ### save\n",
    "    l2os[opter_name][\"run_history\"] = run_history_all_test_runs\n",
    "    torch.save(run_history_all_test_runs, save_run_history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_grad_noise_for_normal(ckpt_path, run_history, config, train_data, test_data):\n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    optee = config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"]).cuda()\n",
    "    optee.load_state_dict(ckpt[\"optimizee\"])\n",
    "\n",
    "    ### collect\n",
    "    for phase, data_loader in ((\"train\", train_data.loader), (\"test\", test_data.loader)):\n",
    "        losses, accs, grads_noise_norm, grads_alpha, _, _ = eval_metrics(\n",
    "            optee=optee,\n",
    "            data_loader=data_loader,\n",
    "            l2o_opter=None,\n",
    "            hidden_states=None,\n",
    "            cell_states=None\n",
    "        )\n",
    "        run_history[phase][\"loss\"].append(np.mean(losses))\n",
    "        run_history[phase][\"acc\"].append(np.mean(accs))\n",
    "        run_history[phase][\"grads_noise_norm\"].append(grads_noise_norm)\n",
    "        run_history[phase][\"grads_alpha\"].append(grads_alpha.item())\n",
    "\n",
    "    return run_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_existing = True\n",
    "\n",
    "### collect for baselines\n",
    "for baseline_name in baselines:\n",
    "    print(f\"Running {baseline_name}...\")\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "    save_run_history_path = os.path.join(\n",
    "        baselines[baseline_name][\"baseline_dir\"],\n",
    "        f\"grad_noise_run_history_{config['eval_n_tests']}_tests.pt\"\n",
    "    )\n",
    "\n",
    "    if load_existing and os.path.exists(save_run_history_path):\n",
    "        print(f\"  Loading existing run history from {save_run_history_path}\")\n",
    "        run_history = torch.load(save_run_history_path)\n",
    "        baselines[baseline_name][\"run_history\"] = run_history\n",
    "        continue\n",
    "    \n",
    "    run_history_all_test_runs = []\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        print(f\"  Test run {test_run_i}...\")\n",
    "        ckpt_prefix = f\"run{test_run_i}_\"\n",
    "        ### collect\n",
    "        run_history = {k: {k: [] for k in (\"loss\", \"acc\", \"grads_noise_norm\", \"grads_alpha\", \"l2o_updates_noise_norm\", \"l2o_updates_alpha\")}\n",
    "            for k in (\"train\", \"test\")}\n",
    "        train_data = MNIST(training=True, batch_size=128)\n",
    "        test_data = MNIST(training=False, batch_size=128)\n",
    "\n",
    "        ### the very first checkpoint\n",
    "        print(f\"  [{1}/{config['meta_testing']['n_iters']}]\")\n",
    "        ckpt_path = os.path.join(baselines[baseline_name][\"baseline_dir\"], \"ckpt\", f\"{ckpt_prefix}1.pt\")\n",
    "        if os.path.exists(ckpt_path):\n",
    "            run_history = collect_grad_noise_for_normal(\n",
    "                ckpt_path=ckpt_path,\n",
    "                run_history=run_history,\n",
    "                config=config,\n",
    "                train_data=train_data,\n",
    "                test_data=test_data\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  {ckpt_path} does not exist, skipping.\")\n",
    "\n",
    "        ### the rest of the checkpoints\n",
    "        for iter_i in range(\n",
    "            config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "            config[\"meta_testing\"][\"n_iters\"] + 1,\n",
    "            config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "        ):\n",
    "            print(f\"  [{iter_i}/{config['meta_testing']['n_iters']}]\")\n",
    "            \n",
    "            ### load checkpoint\n",
    "            ckpt_path = os.path.join(baselines[baseline_name][\"baseline_dir\"], \"ckpt\", f\"{ckpt_prefix}{iter_i}.pt\")\n",
    "            run_history = collect_grad_noise_for_normal(\n",
    "                ckpt_path=ckpt_path,\n",
    "                run_history=run_history,\n",
    "                config=config,\n",
    "                train_data=train_data,\n",
    "                test_data=test_data\n",
    "            )\n",
    "        \n",
    "        ### save current test run\n",
    "        run_history_all_test_runs.append(run_history)\n",
    "\n",
    "    ### save\n",
    "    baselines[baseline_name][\"run_history\"] = run_history_all_test_runs\n",
    "    torch.save(run_history_all_test_runs, save_run_history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preprocess\n",
    "### turn l2os[opter_name][\"run_history\"][<number-of-test>][\"train\"][<metric>] into l2os[opter_name][\"run_history\"][\"train\"][<metric>][<number-of-test>]\n",
    "for opter_name in l2os:\n",
    "    new_train_run_history = dict()\n",
    "    new_test_run_history = dict()\n",
    "    for test_run_i in range(l2os[opter_name][\"config\"][\"eval_n_tests\"]):\n",
    "        for metric_name in l2os[opter_name][\"run_history\"][test_run_i][\"train\"]:\n",
    "            if test_run_i == 0:\n",
    "                new_train_run_history[metric_name] = [l2os[opter_name][\"run_history\"][test_run_i][\"train\"][metric_name]]\n",
    "                new_test_run_history[metric_name] = [l2os[opter_name][\"run_history\"][test_run_i][\"test\"][metric_name]]\n",
    "            else:\n",
    "                new_train_run_history[metric_name].append(l2os[opter_name][\"run_history\"][test_run_i][\"train\"][metric_name])\n",
    "                new_test_run_history[metric_name].append(l2os[opter_name][\"run_history\"][test_run_i][\"test\"][metric_name])\n",
    "    l2os[opter_name][\"run_history\"] = dict()\n",
    "    l2os[opter_name][\"run_history\"][\"train\"] = new_train_run_history\n",
    "    l2os[opter_name][\"run_history\"][\"test\"] = new_test_run_history\n",
    "\n",
    "for baseline_name in baselines:\n",
    "    new_train_run_history = dict()\n",
    "    new_test_run_history = dict()\n",
    "    for test_run_i in range(baselines[baseline_name][\"config\"][\"eval_n_tests\"]):\n",
    "        for metric_name in baselines[baseline_name][\"run_history\"][test_run_i][\"train\"]:\n",
    "            if test_run_i == 0:\n",
    "                new_train_run_history[metric_name] = [baselines[baseline_name][\"run_history\"][test_run_i][\"train\"][metric_name]]\n",
    "                new_test_run_history[metric_name] = [baselines[baseline_name][\"run_history\"][test_run_i][\"test\"][metric_name]]\n",
    "            else:\n",
    "                new_train_run_history[metric_name].append(baselines[baseline_name][\"run_history\"][test_run_i][\"train\"][metric_name])\n",
    "                new_test_run_history[metric_name].append(baselines[baseline_name][\"run_history\"][test_run_i][\"test\"][metric_name])\n",
    "    baselines[baseline_name][\"run_history\"] = dict()\n",
    "    baselines[baseline_name][\"run_history\"][\"train\"] = new_train_run_history\n",
    "    baselines[baseline_name][\"run_history\"][\"test\"] = new_test_run_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 2000\n",
    "plot_l2o_grad_alpha = False\n",
    "plot_l2o_update_alpha = True\n",
    "optee_name = \"MNISTNet\"\n",
    "phase = \"train\"\n",
    "save_fig_to_path = os.path.join(\n",
    "    \"../results/heavy_tail_grad_update_noise\",\n",
    "    f\"update_noise_alpha_estimates_{optee_name}_{phase}.pdf\" if max_iters is None \\\n",
    "        else f\"update_noise_alpha_estimates_{optee_name}_{phase}_{max_iters}_iters.pdf\"\n",
    ")\n",
    "save_fig_to_path = None\n",
    "\n",
    "x_ticks = range(\n",
    "    config[\"meta_testing\"][\"ckpt_iter_freq\"],\n",
    "    config[\"meta_testing\"][\"n_iters\"] + config[\"meta_testing\"][\"ckpt_iter_freq\"] + 1 \\\n",
    "        if max_iters is None or config[\"meta_testing\"][\"n_iters\"] <= max_iters \\\n",
    "        else max_iters + 1,\n",
    "    config[\"meta_testing\"][\"ckpt_iter_freq\"]\n",
    ")\n",
    "\n",
    "### plot alpha estimates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for opter_name in l2os:\n",
    "    if plot_l2o_grad_alpha:\n",
    "        ### alpha estimates for gradients\n",
    "        grads_alpha = np.array(l2os[opter_name][\"run_history\"][phase][\"grads_alpha\"])[:, :len(x_ticks)] # (n_tests, n_iters)\n",
    "        sns.lineplot(x=x_ticks, y=grads_alpha.mean(0), label=f\"{opter_name} - gradients\", ax=ax)\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=grads_alpha.mean(0) - grads_alpha.std(0),\n",
    "            y2=grads_alpha.mean(0) + grads_alpha.std(0),\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "    if plot_l2o_update_alpha:\n",
    "        ### alpha estimates for param updates\n",
    "        updates_alpha = np.array(l2os[opter_name][\"run_history\"][phase][\"l2o_updates_alpha\"])[:, :len(x_ticks)] # (n_tests, n_iters)\n",
    "        sns.lineplot(x=x_ticks, y=updates_alpha.mean(0), label=f\"{opter_name} - updates\", ax=ax)\n",
    "        ax.fill_between(\n",
    "            x=x_ticks,\n",
    "            y1=updates_alpha.mean(0) - updates_alpha.std(0),\n",
    "            y2=updates_alpha.mean(0) + updates_alpha.std(0),\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "for baseline_name in baselines:\n",
    "    grads_alpha = np.array(baselines[baseline_name][\"run_history\"][phase][\"grads_alpha\"])[:, :len(x_ticks)] # (n_tests, n_iters)\n",
    "    sns.lineplot(x=x_ticks, y=grads_alpha.mean(0), label=baseline_name, ax=ax)\n",
    "    ax.fill_between(\n",
    "        x=x_ticks,\n",
    "        y1=grads_alpha.mean(0) - grads_alpha.std(0),\n",
    "        y2=grads_alpha.mean(0) + grads_alpha.std(0),\n",
    "        alpha=0.2\n",
    "    )\n",
    "\n",
    "# ax.set_title(f\"Alpha estimates ({phase})\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Alpha estimate\")\n",
    "\n",
    "\n",
    "### manual\n",
    "# ax.set_ylim(0.5, 1.)\n",
    "# yticks = ax.get_yticks()\n",
    "# ax.set_yticks(yticks[::3])\n",
    "# ax.set_yticks([0.5, 0.65, 0.8, 1])\n",
    "ax.set_xticks(np.arange(0, 1001 if max_iters is None else max_iters + 1, 500))\n",
    "\n",
    "# legend\n",
    "ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.25), ncol=2)\n",
    "legend = ax.get_legend()\n",
    "for legend_handle in legend.legendHandles:\n",
    "    legend_handle.set_linewidth(3.0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save fig\n",
    "if save_fig_to_path:\n",
    "    fig.savefig(save_fig_to_path, bbox_inches=\"tight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating results from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, input_dim=28*28 , width=50, depth=3, num_classes=10):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.input_dim = input_dim \n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        layers = self.get_layers()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.width, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            *layers,\n",
    "            nn.Linear(self.width, self.num_classes, bias=False),\n",
    "        )\n",
    "\n",
    "    def get_layers(self):\n",
    "        layers = []\n",
    "        for i in range(self.depth - 2):\n",
    "            layers.append(nn.Linear(self.width, self.width, bias=False))\n",
    "            layers.append(nn.ReLU())\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), self.input_dim)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    from torchvision import datasets, transforms\n",
    "    data_class = 'MNIST'\n",
    "    num_classes = 10\n",
    "    stats = {\n",
    "        'mean': [0.1307], \n",
    "        'std': [0.3081]\n",
    "        }\n",
    "\n",
    "    # input transformation w/o preprocessing for now\n",
    "\n",
    "    trans = [\n",
    "        transforms.ToTensor(),\n",
    "        lambda t: t.type(torch.get_default_dtype()),\n",
    "        transforms.Normalize(**stats)\n",
    "        ]\n",
    "        \n",
    "    # get tr and te data with the same normalization\n",
    "    tr_data = getattr(datasets, data_class)(\n",
    "        root=os.environ['DATA_PATH'], \n",
    "        train=True, \n",
    "        download=False,\n",
    "        transform=transforms.Compose(trans)\n",
    "        )\n",
    "\n",
    "    te_data = getattr(datasets, data_class)(\n",
    "        root=os.environ['DATA_PATH'], \n",
    "        train=False, \n",
    "        download=False,\n",
    "        transform=transforms.Compose(trans)\n",
    "        )\n",
    "\n",
    "    # get tr_loader for train/eval and te_loader for eval\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=tr_data,\n",
    "        batch_size=100, \n",
    "        shuffle=False,\n",
    "        )\n",
    "\n",
    "    train_loader_eval = torch.utils.data.DataLoader(\n",
    "        dataset=tr_data,\n",
    "        batch_size=100, \n",
    "        shuffle=False,\n",
    "        )\n",
    "\n",
    "    test_loader_eval = torch.utils.data.DataLoader(\n",
    "        dataset=te_data,\n",
    "        batch_size=100, \n",
    "        shuffle=False,\n",
    "        )\n",
    "\n",
    "    return train_loader, test_loader_eval, train_loader_eval, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run separately\n",
    "run_history = {k: [] for k in (\"train_loss\", \"train_acc\", \"noise_norm\", \"alpha\")}\n",
    "# train_data = MNIST(training=True, batch_size=100)\n",
    "# test_data = MNIST(training=False, batch_size=100)\n",
    "train_loader, test_loader_eval, train_loader_eval, num_classes = get_data()\n",
    "\n",
    "# optee = MNISTRelu().cuda()\n",
    "optee = FullyConnected(width=20, depth=1).cuda()\n",
    "optee_optim = optim.SGD(optee.parameters(), lr=0.1)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def eval():\n",
    "    optee.eval()\n",
    "    grads = []\n",
    "    losses = []\n",
    "    accs = []\n",
    "    n_minibatches = 0\n",
    "    # for data in (train_data,):\n",
    "    for i, (x, y) in enumerate(train_loader_eval):\n",
    "        n_minibatches += 1\n",
    "        \n",
    "        x, y = x.view(-1, 784).cuda(), y.cuda()\n",
    "        # loss, acc = optee(x, out=y, return_acc=True)\n",
    "        y_hat = optee(x)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "\n",
    "        ### collect gradients\n",
    "        # grads.append(torch.cat([p.grad.detach().view(-1) for n, p in optee.all_named_parameters() if p.requires_grad]).cpu())\n",
    "        grads.append(torch.cat([p.grad.detach().view(-1) for n, p in optee.named_parameters() if p.requires_grad]).cpu())\n",
    "\n",
    "        ### track history\n",
    "        losses.append(loss.item())\n",
    "        acc = (y_hat.argmax(dim=1) == y).float().mean()\n",
    "        accs.append(acc.item())\n",
    "        \n",
    "        optee_optim.zero_grad()\n",
    "\n",
    "    optee_total_params = len(grads[0])\n",
    "    grads = torch.stack(grads, dim=0) # (n_minibatches, optee_total_params)\n",
    "    mean_grad = grads.mean(dim=0) # (optee_total_params,)\n",
    "    noise_norm = (grads - mean_grad).norm(dim=1) # (n_minibatches,)\n",
    "\n",
    "    ### get the tail index alpha\n",
    "    N = optee_total_params * n_minibatches\n",
    "    for i in range(1, 1 + int(np.sqrt(N))):\n",
    "        if N % i == 0:\n",
    "            m = i\n",
    "    alpha = alpha_estimator(m, (grads - mean_grad).view(-1, 1))\n",
    "\n",
    "    ### collect history\n",
    "    run_history[\"train_loss\"].append(np.mean(losses))\n",
    "    run_history[\"train_acc\"].append(np.mean(accs))\n",
    "    run_history[\"noise_norm\"].append(noise_norm)\n",
    "    run_history[\"alpha\"].append(alpha.item())\n",
    "\n",
    "def cyclic_loader(loader):\n",
    "    while True:\n",
    "        for x in loader:\n",
    "            yield x\n",
    "\n",
    "cyclic_train_loader = cyclic_loader(train_loader)\n",
    "\n",
    "for i, (x, y) in enumerate(cyclic_train_loader):\n",
    "    if i % 100 == 0:\n",
    "        eval()\n",
    "        print(i, run_history[\"train_loss\"][-1], run_history[\"train_acc\"][-1], run_history[\"alpha\"][-1])\n",
    "    \n",
    "    optee.train()\n",
    "    x, y = x.view(-1, 784).cuda(), y.cuda()\n",
    "    # loss, acc = optee(inp=x, out=y, return_acc=True)\n",
    "    y_hat = optee(x)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    loss.backward()\n",
    "    optee_optim.step()\n",
    "    optee_optim.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(run_history[\"train_loss\"])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(run_history[\"train_acc\"])\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(run_history[\"alpha\"])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavy-tail distribution of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling_ms(N):\n",
    "    ms = [2]\n",
    "    select_ms_closest_to_curr_idx = 0\n",
    "    select_ms_closest_to = [5, 10, 20, 50, 100, 500, 1000]\n",
    "    for i in range(3, 1 + int(np.sqrt(N))):\n",
    "        if N % i == 0 \\\n",
    "            and i > ms[-1] \\\n",
    "            and ms[-1] < select_ms_closest_to[select_ms_closest_to_curr_idx] \\\n",
    "            and i >= select_ms_closest_to[select_ms_closest_to_curr_idx]:\n",
    "            ms.append(i)\n",
    "            select_ms_closest_to_curr_idx += 1\n",
    "            if select_ms_closest_to_curr_idx >= len(select_ms_closest_to):\n",
    "                break\n",
    "    return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_parameter_heavy_tail_alpha_estimates(\n",
    "    config,\n",
    "    ckpts_dir,\n",
    "    iters_window,\n",
    "    ckpt_iter_freq,\n",
    "    iter_print_freq=50,\n",
    "):\n",
    "    ### collect\n",
    "    parameter_history_metrics = {k: [] for k in (\"loss\", \"acc\", \"params_alpha_total\")}\n",
    "    for k in (\"params_noise_norm_per_iter\", \"params_alpha_per_iter\"):\n",
    "        parameter_history_metrics[k] = dict()\n",
    "    \n",
    "    _tmp_optee = config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"])\n",
    "    params = {iter_i: {n: [] for n, p in _tmp_optee.named_parameters() if p.requires_grad}\n",
    "        for iter_i in range(iters_window[0], iters_window[1], ckpt_iter_freq)}\n",
    "    del _tmp_optee\n",
    "\n",
    "    for test_run_i in range(config[\"eval_n_tests\"]):\n",
    "        print(f\"  Test run {test_run_i}...\")\n",
    "        for iter_i in range(\n",
    "            iters_window[0],\n",
    "            iters_window[1],\n",
    "            ckpt_iter_freq\n",
    "        ):\n",
    "            if iter_i % iter_print_freq == 0:\n",
    "                print(f\"  [COLLECTING-PARAMS-{test_run_i}][{iter_i}/{iters_window[1]}]\")\n",
    "\n",
    "            ### load checkpoint\n",
    "            ckpt_path = os.path.join(ckpts_dir, f\"run{test_run_i}_{iter_i}.pt\")\n",
    "            ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "            ### load optee and collect parameters\n",
    "            for param_name in ckpt[\"optimizee\"]:\n",
    "                if \"mean\" not in param_name and \"var\" not in param_name: # skip batch norm params\n",
    "                    params[iter_i][param_name].append(ckpt[\"optimizee\"][param_name].detach().view(-1).cpu())\n",
    "    \n",
    "    ### post-process - calculate noise norm and alpha estimates per iteration\n",
    "    print(\"Post-processing (per iteration)...\")\n",
    "    for iter_i in range(\n",
    "        iters_window[0],\n",
    "        iters_window[1],\n",
    "        ckpt_iter_freq\n",
    "    ):\n",
    "        if iter_i % iter_print_freq == 0:\n",
    "            print(f\"  [POSTPROCESSING-PER-ITER][{iter_i}/{iters_window[1]}]\")\n",
    "        for n in params[iter_i].keys():\n",
    "            # get alpha estimate\n",
    "            params[iter_i][n] = torch.stack(params[iter_i][n]) # (eval_n_tests, param_dim)\n",
    "            mean_param = params[iter_i][n].mean(dim=0) # (eval_n_tests, param_dim) -> (param_dim,)\n",
    "            N = params[iter_i][n].shape[0] * params[iter_i][n].shape[1]\n",
    "            for i in range(1, 1 + int(np.sqrt(N))):\n",
    "                if N % i == 0:\n",
    "                    m = i\n",
    "            alpha = alpha_estimator(m, (params[iter_i][n] - mean_param).view(-1, 1)) # (n_tests,)\n",
    "\n",
    "            noise_norm = (params[iter_i][n] - mean_param).norm(dim=1) # (n_tests,)\n",
    "            parameter_history_metrics[\"params_noise_norm_per_iter\"][iter_i] = torch.mean(noise_norm).item()\n",
    "            parameter_history_metrics[\"params_alpha_per_iter\"][iter_i] = alpha.item()\n",
    "\n",
    "    print(\"Post-processing (total)...\")\n",
    "    for n in params[iters_window[0]].keys():\n",
    "        tmp_params = torch.cat([params[iter_i][n] for iter_i in range(\n",
    "            iters_window[0],\n",
    "            iters_window[1],\n",
    "            ckpt_iter_freq\n",
    "        )], dim=0) # (eval_n_tests * n_iters, param_dim)\n",
    "        tmp_params = tmp_params.view(-1, 1)\n",
    "        tmp_params = tmp_params - torch.mean(tmp_params) # center\n",
    "        alpha = np.median([alpha_estimator(m, tmp_params) for m in get_sampling_ms(tmp_params.shape[0])])\n",
    "        parameter_history_metrics[\"params_alpha_total\"] = alpha\n",
    "    \n",
    "    return parameter_history_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "load_existing = True\n",
    "iters_window = (1, 1000)\n",
    "ckpt_iter_freq = 1 # config[\"meta_testing\"][\"ckpt_iter_freq\"]\n",
    "iter_print_freq = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect for baselines\n",
    "for baseline_name in baselines:\n",
    "    print(f\"Collecting for {baseline_name}...\")\n",
    "    config = baselines[baseline_name][\"config\"]\n",
    "\n",
    "    ckpts_dir = os.path.join(baselines[baseline_name][\"baseline_dir\"], \"ckpt\")\n",
    "    save_parameter_history_metrics_path = os.path.join(\n",
    "        baselines[baseline_name][\"baseline_dir\"],\n",
    "        f\"parameter_heavy_tail_alpha_estimates\" +\n",
    "            f\"_{iters_window[0]}-{iters_window[1]}\" +\n",
    "            f\"_{config['meta_testing']['optee_cls'].__name__}_{dict_to_str(config['meta_testing']['optee_config'])}\" +\n",
    "            f\"_{config['meta_testing']['data_cls'].__name__}_{dict_to_str(config['meta_testing']['data_config'])}\" +\n",
    "            f\".pt\"\n",
    "    )\n",
    "\n",
    "    if load_existing and os.path.exists(save_parameter_history_metrics_path):\n",
    "        print(f\"  Loading existing run history from {save_parameter_history_metrics_path}\")\n",
    "        baselines[baseline_name][\"parameter_history_metrics\"] = torch.load(save_parameter_history_metrics_path)\n",
    "    else:\n",
    "        print(f\"  Existing run history at path {save_parameter_history_metrics_path} not found, collecting it...\")\n",
    "        baselines[baseline_name][\"parameter_history_metrics\"] = collect_parameter_heavy_tail_alpha_estimates(\n",
    "            config=config,\n",
    "            ckpts_dir=ckpts_dir,\n",
    "            iters_window=iters_window,\n",
    "            ckpt_iter_freq=ckpt_iter_freq,\n",
    "            iter_print_freq=iter_print_freq,\n",
    "        )\n",
    "        torch.save(baselines[baseline_name][\"parameter_history_metrics\"], save_parameter_history_metrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect for l2os\n",
    "for opter_name in l2os:\n",
    "    print(f\"Collecting for {opter_name}...\")\n",
    "    config = l2os[opter_name][\"config\"]\n",
    "\n",
    "    # ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"])\n",
    "    ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"] + \"_long\")\n",
    "    save_parameter_history_metrics_path = os.path.join(\n",
    "        os.environ[\"CKPT_PATH\"],\n",
    "        config[\"ckpt_base_dir\"],\n",
    "        f\"parameter_heavy_tail_alpha_estimates\" +\n",
    "            f\"_{iters_window[0]}-{iters_window[1]}\" +\n",
    "            f\"_{config['meta_testing']['optee_cls'].__name__}_{dict_to_str(config['meta_testing']['optee_config'])}\" +\n",
    "            f\"_{config['meta_testing']['data_cls'].__name__}_{dict_to_str(config['meta_testing']['data_config'])}\" +\n",
    "            f\".pt\"\n",
    "    )\n",
    "\n",
    "    if load_existing and os.path.exists(save_parameter_history_metrics_path):\n",
    "        print(f\"  Loading existing run history from {save_parameter_history_metrics_path}\")\n",
    "        l2os[opter_name][\"parameter_history_metrics\"] = torch.load(save_parameter_history_metrics_path)\n",
    "    else:\n",
    "        print(f\"  Existing run history not found at path {save_parameter_history_metrics_path}, collecting it...\")\n",
    "        l2os[opter_name][\"parameter_history_metrics\"] = collect_parameter_heavy_tail_alpha_estimates(\n",
    "            config=config,\n",
    "            ckpts_dir=ckpts_dir,\n",
    "            iters_window=iters_window,\n",
    "            ckpt_iter_freq=ckpt_iter_freq,\n",
    "            iter_print_freq=iter_print_freq,\n",
    "        )\n",
    "        torch.save(l2os[opter_name][\"parameter_history_metrics\"], save_parameter_history_metrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l2os[\"baseline_no_reg\"][\"parameter_history_metrics\"][\"params_alpha_total\"])\n",
    "plt.plot(list(l2os[\"baseline_no_reg\"][\"parameter_history_metrics\"][\"params_alpha_per_iter\"].values())[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baselines[\"SGD\"][\"parameter_history_metrics\"][\"params_alpha_total\"])\n",
    "plt.plot(list(baselines[\"SGD\"][\"parameter_history_metrics\"][\"params_alpha_per_iter\"].values())[0:])\n",
    "plt.ylim(0, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the original implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = []\n",
    "\n",
    "# ### collect for baselines\n",
    "# for baseline_name in baselines:\n",
    "#     print(f\"Collecting for {baseline_name}...\")\n",
    "#     config = baselines[baseline_name][\"config\"]\n",
    "\n",
    "#     ckpts_dir = os.path.join(baselines[baseline_name][\"baseline_dir\"], \"ckpt\")\n",
    "\n",
    "#     for iter_i in range(\n",
    "#         1500,\n",
    "#         2000,\n",
    "#         5\n",
    "#     ):\n",
    "#         if iter_i % 100 == 0:\n",
    "#             print(f\"  iter_i={iter_i}\")\n",
    "#         ### load checkpoint\n",
    "#         ckpt_path = os.path.join(ckpts_dir, f\"run0_{iter_i}.pt\")\n",
    "#         ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "#         optee = config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"])\n",
    "#         optee.load_state_dict(ckpt[\"optimizee\"])\n",
    "#         nets.append(optee.cpu())\n",
    "    \n",
    "### collect for l2os\n",
    "for opter_name in l2os:\n",
    "    print(f\"Collecting for {opter_name}...\")\n",
    "    config = l2os[opter_name][\"config\"]\n",
    "\n",
    "    ckpts_dir = os.path.join(os.environ[\"CKPT_PATH\"], config[\"meta_testing\"][\"ckpt_dir\"])\n",
    "\n",
    "    for iter_i in range(\n",
    "        1500,\n",
    "        2000,\n",
    "        5\n",
    "    ):\n",
    "        if iter_i % 100 == 0:\n",
    "            print(f\"  iter_i={iter_i}\")\n",
    "        ### load checkpoint\n",
    "        ckpt_path = os.path.join(ckpts_dir, f\"run0_{iter_i}.pt\")\n",
    "        ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        optee = config[\"meta_testing\"][\"optee_cls\"](**config[\"meta_testing\"][\"optee_config\"])\n",
    "        optee.load_state_dict(ckpt[\"optimizee\"])\n",
    "        nets.append(optee.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 2\n",
    "num_nets = len(nets)\n",
    "alphas_mc = np.zeros(depth) - 1\n",
    "\n",
    "# Corollary 2.4 in Mohammadi 2014 - for 1d\n",
    "def alpha_estimator_one(m, X):\n",
    "    N = len(X)\n",
    "    n = int(N/m) # must be an integer\n",
    "    \n",
    "    X = X[0:n*m]\n",
    "    \n",
    "    Y = np.sum(X.reshape(n, m),1)\n",
    "    eps = np.spacing(1)\n",
    "\n",
    "    Y_log_norm =  np.log(np.abs(Y) + eps).mean()\n",
    "    X_log_norm =  np.log(np.abs(X) + eps).mean()\n",
    "    diff = (Y_log_norm - X_log_norm) / math.log(m)\n",
    "    return 1 / diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect\n",
    "weights = []\n",
    "for i in range(depth):\n",
    "    weights.append([])\n",
    "\n",
    "# record the layers in different arrays\n",
    "for i in range(num_nets):\n",
    "    tmp_net = nets[i]\n",
    "    ix = 0\n",
    "    for n, p in tmp_net.all_named_parameters():\n",
    "        if \"bias\" in n or \"batch_norm\" in n:\n",
    "            continue\n",
    "        layer = p.detach().numpy()\n",
    "        layer = layer.reshape(-1,1)\n",
    "        weights[ix].append(layer)\n",
    "        ix += 1\n",
    "\n",
    "for i in range(depth):\n",
    "    weights[i] = np.concatenate(weights[i], axis = 1)  \n",
    "\n",
    "for i in range(depth):\n",
    "    tmp_weights = np.mean(weights[i], axis=1)\n",
    "    tmp_weights = tmp_weights.reshape(-1,1)\n",
    "    tmp_weights = tmp_weights - np.mean(tmp_weights)\n",
    "    # tmp_alphas = [alpha_estimator_one(mm, tmp_weights) for mm in (2, 5, 10, 20, 50, 100, 500, 1000)]\n",
    "    tmp_alphas = [alpha_estimator_one(mm, tmp_weights) for mm in (2, 5, 10, 20)]\n",
    "    # tmp_alphas = [alpha_estimator(mm, torch.tensor(tmp_weights).view(-1, 1)) for mm in (2, 5, 10, 20)]\n",
    "    alphas_mc[i] = np.median(tmp_alphas)\n",
    "\n",
    "print(alphas_mc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5692ede66a2eeda96ca4e496ad881a063b66ee8e9ec6003b28974c60439bc6fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
